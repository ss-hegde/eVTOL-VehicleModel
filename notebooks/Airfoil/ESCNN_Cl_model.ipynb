{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ESCNN $C_l$ model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ESCNN $C_l$ Model Training and Evaluation\n",
    "\n",
    "This Jupyter Notebook documents the workflow for training and evaluating an ESCNN (Element Spatial Convolutional Neural Network) model to predict the lift coefficient ($C_l$) of airfoils. The notebook includes the following steps:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "    - Loading and preprocessing airfoil coordinate and polar data.\n",
    "    - Downsampling airfoil coordinates to a fixed number of points.\n",
    "    - Organizing the data into a suitable format for training the neural network.\n",
    "\n",
    "2. **Model Definition**:\n",
    "    - Defining the ESCNN model architecture using PyTorch.\n",
    "\n",
    "3. **Training the Model**:\n",
    "    - Splitting the data into training and validation sets.\n",
    "    - Training the model using the training dataset.\n",
    "    - Monitoring the training and validation loss.\n",
    "\n",
    "4. **Evaluation**:\n",
    "    - Evaluating the trained model on a separate testing dataset.\n",
    "    - Calculating evaluation metrics such as Relative L2 Norm Error and R² score.\n",
    "    - Optionally plotting the results and saving the trained model and evaluation results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TRAINED_MODEL = False\n",
    "PLOT_RESULTS = False\n",
    "SAVE_RESULTS = False\n",
    "\n",
    "project_path = '/mnt/e/eVTOL_model/eVTOL-VehicleModel/'\n",
    "data_path = project_path + 'data/airfoil_data/'\n",
    "model_path = project_path + 'trained_models/models/airfoil/'\n",
    "save_path = project_path + 'result/airfoil_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling using average pooling\n",
    "\"\"\"\n",
    "Downsample the input array to 35 elements using interpolation.\n",
    "\"\"\"\n",
    "\n",
    "def downsample_to_35(input_array):\n",
    "    input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
    "    \n",
    "    # Reshape the input to be 1D (if it's not already)\n",
    "    if input_tensor.dim() == 1:\n",
    "        input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, original_length)\n",
    "    elif input_tensor.dim() == 2:\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Shape (1, original_channels, original_length)\n",
    "    \n",
    "    # Perform interpolation to downsample to 35 elements\n",
    "    downsampled_tensor = F.interpolate(input_tensor, size=35, mode='linear', align_corners=True)\n",
    "    \n",
    "    # Remove the unnecessary dimensions to return a 1D tensor\n",
    "    downsampled_array = downsampled_tensor.squeeze().numpy()\n",
    "    \n",
    "    return downsampled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(root, keyword=None):\n",
    "\n",
    "    # Initialization of arrays\n",
    "    # Coordinates\n",
    "    x_i = [] # Initial coordinates - before downsizing\n",
    "    y_i = []\n",
    "\n",
    "    # Polars\n",
    "    alphas = []\n",
    "    Cls = []\n",
    "    Cds = []\n",
    "    Cms = []\n",
    "\n",
    "    if keyword:\n",
    "        # lists of files in each dir\n",
    "        coord_files = [f for f in os.listdir(root) if f == (keyword+'_coordinates.dat')]\n",
    "        polar_files = [f for f in os.listdir(root) if f == ('xf-'+keyword+'-il-1000000.csv')]\n",
    "    else:\n",
    "        # lists of files in each dir\n",
    "        coord_files = [f for f in os.listdir(root) if f.endswith('_coordinates.dat')]\n",
    "        polar_files = [f for f in os.listdir(root) if f.endswith('.csv')]\n",
    "\n",
    "    # Extract base names from coordinate files\n",
    "    coord_bases = {re.sub(r'\\_coordinates.dat$', '', f) for f in coord_files}\n",
    "    polar_bases = {}\n",
    "    for polar_file in polar_files:\n",
    "        match = re.match(r'xf-(.*)-il-1000000\\.csv$', polar_file)\n",
    "        if match:\n",
    "            base_name = match.group(1)\n",
    "            polar_bases[base_name] = polar_file\n",
    "    # print(polar_bases)\n",
    "    for base_name in coord_bases:\n",
    "        if base_name in polar_bases:\n",
    "            coord_file = f\"{base_name}_coordinates.dat\"\n",
    "            polar_file = polar_bases[base_name]\n",
    "\n",
    "            coordinate_data = np.loadtxt(root+coord_file)\n",
    "            # polar_data = np.loadtxt(root+polar_file, skiprows=12)\n",
    "            polar_data = pd.read_csv(root+polar_file, skiprows=10)\n",
    "            polar_data = polar_data[(polar_data['Alpha'] >= -2) & (polar_data['Alpha'] <= 10)]\n",
    "            # print(len(polar_data))\n",
    "\n",
    "            # Coordinates\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            # Polars\n",
    "            alpha = polar_data['Alpha'].values\n",
    "            Cl = polar_data['Cl'].values\n",
    "            Cd = polar_data['Cd'].values\n",
    "            Cm = polar_data['Cm'].values\n",
    "\n",
    "            # print(alpha)\n",
    "\n",
    "            for i in range(0, len(coordinate_data)):\n",
    "                np.array(x.append(float(coordinate_data[i][0]))) \n",
    "                np.array(y.append(float(coordinate_data[i][1])))\n",
    "                \n",
    "            if len(x) >= 35:    # Only consider the files with more than 35 coordinates\n",
    "                x_i.append(x)\n",
    "                y_i.append(y)\n",
    "\n",
    "                alphas.append(alpha)\n",
    "                # Cls.append(Cl)\n",
    "                # Cds.append(Cd)\n",
    "\n",
    "                for num_val in range(len(Cl)):\n",
    "                    Cls.append(Cl[num_val])\n",
    "                    Cds.append(Cd[num_val])\n",
    "                    Cms.append(Cm[num_val])\n",
    "\n",
    "    return x_i, y_i, Cls, Cds, Cms, alphas\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_DOWNSAMPLED = False\n",
    "if PLOT_DOWNSAMPLED:\n",
    "    root = data_path + \"/training_database/\"\n",
    "\n",
    "\n",
    "    coord_file = \"naca6412_coordinates.dat\"\n",
    "    polar_file = \"xf-naca6412-il-1000000.csv\"\n",
    "\n",
    "    coordinate_data = np.loadtxt(root+coord_file)\n",
    "    # polar_data = np.loadtxt(root+polar_file, skiprows=12)\n",
    "    polar_data = pd.read_csv(root+polar_file, skiprows=10)\n",
    "\n",
    "    x=[]\n",
    "    y=[]\n",
    "\n",
    "    for i in range(0, len(coordinate_data)):\n",
    "        np.array(x.append(float(coordinate_data[i][0]))) \n",
    "        np.array(y.append(float(coordinate_data[i][1])))\n",
    "\n",
    "    x_downsampled = downsample_to_35(x)\n",
    "    y_downsampled = downsample_to_35(y)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(10, 6), dpi=300)  # High-resolution plot\n",
    "    plt.plot(x, y, color='black', linewidth=1.5, label='Original Airfoil')\n",
    "    plt.plot(x_downsampled, y_downsampled, 'o--', color='red', markersize=4, label='Downsampled Airfoil')\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(r'Chordwise Position, $x/c$', fontsize=10)\n",
    "    plt.ylabel(r'Normalized Thickness, $y/c$', fontsize=10)\n",
    "    plt.title('Comparison of Original and Downsampled NACA 6412 Airfoil', fontsize=12)\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    # Additional formatting\n",
    "    plt.gca().set_aspect('equal', adjustable='box')  # Maintain correct aspect ratio\n",
    "    plt.xlim((-0.1, 1.1))\n",
    "    plt.ylim((-0.05, 0.2))\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_train = data_path + \"/training_database/\"\n",
    "x_i, y_i, Cls, Cds, Cms, alphas = prep_data(root_train)\n",
    "Cls = np.array(Cls, dtype=float)\n",
    "Cds = np.array(Cds, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to 35 elements\n",
    "x_f = [] # Final coordinates - after downsizing\n",
    "y_f = []\n",
    "for num_airfoil in range(0, len(x_i)):\n",
    "    downsampled_x = downsample_to_35(x_i[num_airfoil])\n",
    "    downsampled_y = downsample_to_35(y_i[num_airfoil])\n",
    "\n",
    "    x_f.append(downsampled_x)\n",
    "    y_f.append(downsampled_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the data in the form of elements\n",
    "* $E = [E_1, E_2, ....., E_n]$ \n",
    "* $E_1 = [x_1, y_1, x_2, y_2, \\alpha]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arange the input data in columns [x, y, alpha, Re, M]\n",
    "\n",
    "def organize_data(x_f, y_f, alphas):\n",
    "\n",
    "    Elements = []\n",
    "\n",
    "    # Loop through the polars\n",
    "    for n_file in range(len(x_f)):\n",
    "        x_temp = x_f[n_file]\n",
    "        y_temp = y_f[n_file]\n",
    "        alpha_temp = alphas[n_file]\n",
    "        \n",
    "        for j in range(len(alpha_temp)):\n",
    "            batch = []\n",
    "            # Loop through the coodrinates\n",
    "            for i in range(len(x_temp)-1):\n",
    "                element = np.array([x_temp[i], y_temp[i], x_temp[i+1], y_temp[i+1], alpha_temp[j]])\n",
    "                # Elements.append(element)\n",
    "                batch.append(element)\n",
    "            batch = np.array(batch)\n",
    "            batch = batch.flatten()\n",
    "            Elements.append(batch)\n",
    "\n",
    "    Elements = np.array(Elements)\n",
    "\n",
    "    return Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elements = organize_data(x_f, y_f, alphas)\n",
    "print(Elements.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESCNN, self).__init__()\n",
    "        \n",
    "        # Conv1: Assume 1D Convolution\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=200, kernel_size=5, stride=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        #conv2\n",
    "        self.conv2 = nn.Conv1d(in_channels=200, out_channels=200, kernel_size=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Conv3\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=100, kernel_size=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Conv4\n",
    "        self.conv4 = nn.Conv1d(in_channels=100, out_channels=1, kernel_size=5, padding=2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        # Final fully connected layer to output scalar\n",
    "        self.fc1 = nn.Linear(in_features=34, out_features=34)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=34, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input if necessary, ensure it's in the shape (batch_size, channels, elements)\n",
    "        x = x.view(-1, 1, 170)  # Reshape to (batch_size, channel=1, elements=170)\n",
    "        \n",
    "        x = self.conv1(x)  \n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)  \n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)  \n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.conv4(x)  \n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)  \n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = ESCNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orgaize the data for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data = Elements\n",
    "output_data = Cls\n",
    "\n",
    "input_data_train, input_data_val, output_data_train, output_data_val = train_test_split(input_data, output_data, test_size=0.25, random_state=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "# And move the data to cuda\n",
    "\n",
    "input_data_train = torch.from_numpy(input_data_train).float().to(device)\n",
    "input_data_val = torch.from_numpy(input_data_val).float().to(device)\n",
    "\n",
    "output_data_train = torch.from_numpy(output_data_train).float().to(device)\n",
    "output_data_val = torch.from_numpy(output_data_val).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "\n",
    "dataset_train = TensorDataset(input_data_train, output_data_train)\n",
    "dataset_val = TensorDataset(input_data_val, output_data_val)\n",
    "\n",
    "trainDataLoader = DataLoader(dataset_train, batch_size=256)\n",
    "valDataLoader = DataLoader(dataset_val, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "learning_rate = 1e-5\n",
    "num_convLayers = 4\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 1500\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"val_loss\": []\n",
    "}\n",
    "\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for e in range(0, num_epochs):\n",
    "\n",
    "    # Model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # initialize total training and validation losses\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "\n",
    "    # Loop over training dataset\n",
    "    for(ip,op) in trainDataLoader:\n",
    "        # Forward pass\n",
    "        pred = model(ip)\n",
    "        pred = pred.squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred, op)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the loss to the total training loss so far and\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Model to validation mode\n",
    "        model.eval()\n",
    "\n",
    "        for (ip, op) in valDataLoader:\n",
    "\n",
    "            pred = model(ip)\n",
    "            pred = pred.squeeze(1)\n",
    "            totalValLoss += criterion(pred, op)\n",
    "\n",
    "    \t\n",
    "    # Calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / len(trainDataLoader.dataset)\n",
    "    avgValLoss = totalValLoss / len(valDataLoader.dataset)\n",
    "\n",
    "    # Update the scheduler based on the validation loss\n",
    "    scheduler.step(avgValLoss)\n",
    "\n",
    "    # Updating training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # print the model training and validation information\n",
    "    print(\"Epoch: {}/{}\".format(e+1, num_epochs))\n",
    "    print(\"Train loss: {:.8f}\".format(avgTrainLoss))\n",
    "    print(\"Val loss: {:.8f}\\n\".format(avgValLoss))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"Finished Training\")\n",
    "print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime-startTime))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)  # High-resolution plot\n",
    "\n",
    "plt.plot(np.log10(H[\"train_loss\"]), color='black', linewidth=1.5, label=\"Training loss\")\n",
    "plt.plot(np.log10(H[\"val_loss\"]), color='red', linewidth=1.5, label=\"Validation loss\")\n",
    "plt.title(\"Training Loss and Validation Loss\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"Log(MSE Loss)\", fontsize=10)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "# plt.savefig(\"airfoil_model_cl_loss.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model if needed\n",
    "\n",
    "if SAVE_TRAINED_MODEL:\n",
    "    save_path = model_path + '/{}_model_Cl_ESCNN_lr{}_e{}_rbf{}_convL{}.pth'.format(date, learning_rate, num_epochs, num_convLayers)\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "root_test = data_path + \"/testing_database/\"\n",
    "\n",
    "\n",
    "coord_files = [f for f in os.listdir(root_test) if f.endswith('_coordinates.dat')]\n",
    "coord_bases = {re.sub(r'\\_coordinates.dat$', '', f) for f in coord_files}\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Absolute Percentage Error (MAPE)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def relative_l2_norm(y_true, y_pred):\n",
    "    \"\"\"Compute Relative L2 Norm Error (ε)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    numerator = np.linalg.norm(y_pred[mask] - y_true[mask], ord=2)  # ||pred - true||_2\n",
    "    denominator = np.linalg.norm(y_true[mask], ord=2)  # ||true||_2\n",
    "    return (numerator / denominator) \n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "l2_norm_cl_list= []\n",
    "r2_cl_list = []\n",
    "airfoil_names = []\n",
    "\n",
    "\n",
    "for keyword in coord_bases:\n",
    "    x_t, y_t, Cls_t, Cms_t, Cds_t, alphas_t = prep_data(root_test, keyword)\n",
    "    Cls_t = np.array(Cls_t, dtype=float)\n",
    "    Cds_t = np.array(Cds_t, dtype=float)\n",
    "    # Cms_t = np.array(Cms_t, dtype=float)\n",
    "    \n",
    "    x_f_t = [] # Final coordinates - after downsizing\n",
    "    y_f_t = []\n",
    "    for num_airfoil in range(0, len(x_t)):\n",
    "        downsampled_x = downsample_to_35(x_t[num_airfoil])\n",
    "        downsampled_y = downsample_to_35(y_t[num_airfoil])\n",
    "\n",
    "        x_f_t.append(downsampled_x)\n",
    "        y_f_t.append(downsampled_y)\n",
    "        \n",
    "    Elements_t = organize_data(x_f_t, y_f_t, alphas_t)\n",
    "    # print(\"Elements: \",Elements_t.shape)\n",
    "    # print(\"Cls: \",Cls_t.shape)\n",
    "\n",
    "    if Elements_t.shape != (0,):\n",
    "        input_data_test = Elements_t\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_test = torch.tensor(input_data_test, dtype=torch.float32)\n",
    "\n",
    "        # Move data to GPU\n",
    "        input_test = input_test.to(device)\n",
    "\n",
    "        # Evaluate the model on test dataset\n",
    "        with torch.no_grad():\n",
    "            Cl_eval = model.forward(input_test)\n",
    "            \n",
    "        Cl_eval = Cl_eval.cpu().detach().numpy()  # Convert tensor to numpy array\n",
    "\n",
    "        # Calculate the evaluation metrics\n",
    "        l2_norm_cl = relative_l2_norm(Cls_t, Cl_eval)\n",
    "        r2_cl = r2_score(Cls_t, Cl_eval)\n",
    "\n",
    "        l2_norm_cl_list.append(l2_norm_cl)\n",
    "        r2_cl_list.append(r2_cl)\n",
    "        airfoil_names.append(keyword)\n",
    "\n",
    "        print(\"l2_norm Cl: {:.2f}%\".format(l2_norm_cl))\n",
    "        print(\"R^2 Cl: {:.4f}\".format(r2_cl))\n",
    "\n",
    "        if PLOT_RESULTS:\n",
    "            plt.figure(figsize=(10, 6), dpi=300)\n",
    "            plt.plot(alphas_t[0], Cl_eval, '--', color='black', linewidth=1.5, label='ESCNN Model')\n",
    "            plt.plot(alphas_t[0],Cls_t, color='red', linewidth=1.5, label='XFOIL')\n",
    "\n",
    "            plt.legend(loc='upper right', fontsize=10)\n",
    "            plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "            plt.title(r'$C_l$ vs $\\alpha$ for {} airfoil'.format(keyword), fontsize=12)\n",
    "            plt.xlabel(r'AOA [$\\alpha$]', fontsize=10)\n",
    "            plt.ylabel(r'$C_l$', fontsize=10)\n",
    "            # plt.gca().set_aspect('equal', adjustable='box')  # Maintain correct aspect ratio\n",
    "            # plt.savefig(\"cl_comparison_{}.pdf\".format(keyword), format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"Mean l2_norm Cl: {:.2f}%\".format(np.mean(l2_norm_cl_list)))\n",
    "print(\"Mean R^2 Cl: {:.4f}\".format(np.mean(r2_cl_list)))\n",
    "\n",
    "mean_l2_norm = np.mean(l2_norm_cl_list)\n",
    "mean_r2 = np.mean(r2_cl_list)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Airfoil\": airfoil_names,\n",
    "    \"Relative L2 Norm Cl (%)\": l2_norm_cl_list,\n",
    "    \"R² Cd\": r2_cl_list\n",
    "})\n",
    "\n",
    "# Append mean values to the DataFrame\n",
    "mean_row = pd.DataFrame({\n",
    "    \"Airfoil\": [\"Mean\"], \n",
    "    \"Relative L2 Norm Cd (%)\": [mean_l2_norm], \n",
    "    \"R² Cd\": [mean_r2]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    results_df = pd.concat([results_df, mean_row], ignore_index=True)\n",
    "    # Define CSV file path\n",
    "    csv_filename = os.path.join(save_path, \"Cl_evaluation_results.csv\")\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Results (including mean values) saved to {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
