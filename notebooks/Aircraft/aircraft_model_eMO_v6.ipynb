{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft Model: LSTM and GNN for Simplified Edge Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of a Long Short-Term Memory (LSTM) network combined with a Graph Neural Network (GNN) to model and predict the aerodynamic behavior of an eVTOL an aircraft. The LSTM is used to capture temporal dependencies in the data, while the GNN is employed to handle the spatial relationships between different components of the aircraft. The simplified edge connections in the GNN help in reducing computational complexity while maintaining the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/mnt/e/eVTOL_model/eVTOL-VehicleModel/'\n",
    "model_save_path = project_path + 'trained_models/'\n",
    "result_out_path = project_path + 'result/aircraft_model/'\n",
    "\n",
    "\n",
    "VISUAILIZE_GRAPH = False\n",
    "TRAIN_MODEL = False\n",
    "SAVE_TRAINED_MODEL = False\n",
    "\n",
    "PLOT_RESULT = True\n",
    "SAVE_PLOT = False\n",
    "SAVE_RESULT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Select the device to use - preferably GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/eVTOL_model/eVTOL-VehicleModel/\n"
     ]
    }
   ],
   "source": [
    "# Add the path to the utility functions\n",
    "sys.path.append(project_path + '/src/')     # Add \"src\" to the system path\n",
    "\n",
    "\n",
    "# Import necessary utility functions\n",
    "from utility_functions import downsample_to_35\n",
    "from utility_functions import organize_data\n",
    "\n",
    "# Import all the models\n",
    "from af_escnn_cl import ESCNN_Cl\n",
    "from af_escnn_cd import ESCNN_Cd\n",
    "\n",
    "from af_rbf_cl import RBFLayer_cl, RBFNet_cl\n",
    "from af_rbf_cd import RBFLayer_cd, RBFNet_cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Airfoil Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15533/4063869800.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  af_model_ESCNN_Cl.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-18_model_Cl_ESCNN_lr1e-05_e1500_rbf170_convL4.pth'))\n",
      "/tmp/ipykernel_15533/4063869800.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  af_model_ESCNN_Cd.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-18_model_Cd_ESCNN_lr5e-05_e250_convL3.pth'))\n",
      "/tmp/ipykernel_15533/4063869800.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  kmeans_center_cl = torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cl_ESCNN_RBF_lr0.06_epoch200_rbfUnits4_RBFcenters.pth')\n",
      "/tmp/ipykernel_15533/4063869800.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  kmeans_center_cd = torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cd_ESCNN_RBF_lr0.025_epoch100_rbfUnits4_RBFcenters.pth')\n",
      "/tmp/ipykernel_15533/4063869800.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  airfoil_cl.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cl_ESCNN_RBF_lr0.06_epoch200_rbfUnits4.pth'))\n",
      "/tmp/ipykernel_15533/4063869800.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  airfoil_cd.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cd_ESCNN_RBF_lr0.025_epoch100_rbfUnits4.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "airfoilModel_cd(\n",
       "  (rbf): RBFLayer_cd()\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize airfoil models\n",
    "root_airfoilModelsTrained = project_path + '/trained_models/models/airfoil/'\n",
    "\n",
    "# Load the model weights\n",
    "af_model_ESCNN_Cl = ESCNN_Cl()\n",
    "af_model_ESCNN_Cl.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-18_model_Cl_ESCNN_lr1e-05_e1500_rbf170_convL4.pth'))\n",
    "af_model_ESCNN_Cl = af_model_ESCNN_Cl.to(device)\n",
    "af_model_ESCNN_Cl.eval()\n",
    "\n",
    "# Load the model weights\n",
    "af_model_ESCNN_Cd = ESCNN_Cd()\n",
    "af_model_ESCNN_Cd.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-18_model_Cd_ESCNN_lr5e-05_e250_convL3.pth'))\n",
    "af_model_ESCNN_Cd = af_model_ESCNN_Cd.to(device)\n",
    "af_model_ESCNN_Cd.eval()\n",
    "\n",
    "input_size = 140\n",
    "output_size = 4\n",
    "num_rbf_units = 4\n",
    "\n",
    "kmeans_center_cl = torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cl_ESCNN_RBF_lr0.06_epoch200_rbfUnits4_RBFcenters.pth')\n",
    "kmeans_center_cd = torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cd_ESCNN_RBF_lr0.025_epoch100_rbfUnits4_RBFcenters.pth')\n",
    "\n",
    "\n",
    "class airfoilModel_cl(RBFNet_cl):\n",
    "    def __init__(self):\n",
    "        super(airfoilModel_cl, self).__init__(input_size, num_rbf_units, output_size, kmeans_center_cl)\n",
    "\n",
    "\n",
    "class airfoilModel_cd(RBFNet_cd):\n",
    "    def __init__(self):\n",
    "        super(airfoilModel_cd, self).__init__(input_size, num_rbf_units, output_size, kmeans_center_cd)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "airfoil_cl = airfoilModel_cl()\n",
    "airfoil_cl.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cl_ESCNN_RBF_lr0.06_epoch200_rbfUnits4.pth'))\n",
    "\n",
    "airfoil_cd = airfoilModel_cd()\n",
    "airfoil_cd.load_state_dict(torch.load(root_airfoilModelsTrained + '2024-11-19_airfoil_model_Cd_ESCNN_RBF_lr0.025_epoch100_rbfUnits4.pth'))\n",
    "\n",
    "airfoil_cl = airfoil_cl.to(device)\n",
    "airfoil_cd = airfoil_cd.to(device)\n",
    "\n",
    "airfoil_cl.eval()\n",
    "airfoil_cd.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_wing_dataset module loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/eVTOL_model/eVTOL-VehicleModel//src/create_wing_dataset.py:376: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (Canard dataset): torch.Size([42, 277, 10])\n",
      "Output shape (Canard dataset): torch.Size([42, 277, 2])\n",
      "Input shape (Wing dataset): torch.Size([42, 277, 10])\n",
      "Output shape (Wing dataset): torch.Size([42, 277, 2])\n"
     ]
    }
   ],
   "source": [
    "from create_wing_dataset import WingDataset, subdir_condition_wing   # Make sure to reload the create wing dataset module manually after changes\n",
    "\n",
    "\n",
    "root_dir_wing = project_path + '/FLOWUnsteady_simulations/aircraft_data/training_data'\n",
    "\n",
    "# Canard dataset\n",
    "dataset_canard = WingDataset(root_dir_wing, \n",
    "                                af_model_ESCNN_Cl=af_model_ESCNN_Cl, \n",
    "                                af_model_ESCNN_Cd=af_model_ESCNN_Cd,\n",
    "                                airfoil_cl=airfoil_cl, \n",
    "                                airfoil_cd=airfoil_cd, \n",
    "                                device=device,\n",
    "                                wing_name = 'Canard',     # select 'wing_main' or 'Canard'  \n",
    "                                subdir_condition=subdir_condition_wing)\n",
    "\n",
    "inputs_canard, outputs_canard = dataset_canard[0:]\n",
    "\n",
    "input_tensor_canard = inputs_canard\n",
    "input_tensor_canard = inputs_canard.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (Canard dataset):\", input_tensor_canard.shape)\n",
    "\n",
    "output_tensor_canard = outputs_canard.squeeze(1)\n",
    "print(\"Output shape (Canard dataset):\",output_tensor_canard.shape) \n",
    "\n",
    "# Wing dataset\n",
    "dataset_wing = WingDataset(root_dir_wing, \n",
    "                            af_model_ESCNN_Cl=af_model_ESCNN_Cl, \n",
    "                            af_model_ESCNN_Cd=af_model_ESCNN_Cd,\n",
    "                            airfoil_cl=airfoil_cl, \n",
    "                            airfoil_cd=airfoil_cd, \n",
    "                            device=device,\n",
    "                            wing_name = 'wing_main',     # select 'wing_main' or 'Canard'  \n",
    "                            subdir_condition=subdir_condition_wing)\n",
    "\n",
    "inputs_wing, outputs_wing = dataset_wing[0:]\n",
    "\n",
    "input_tensor_wing = inputs_wing\n",
    "input_tensor_wing = inputs_wing.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (Wing dataset):\", input_tensor_wing.shape)\n",
    "\n",
    "output_tensor_wing = outputs_wing.squeeze(1)\n",
    "print(\"Output shape (Wing dataset):\",output_tensor_wing.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Rotor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (rotor - L1): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - L1): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - L2): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - L2): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - L3): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - L3): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - L4): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - L4): torch.Size([42, 280, 2])\n"
     ]
    }
   ],
   "source": [
    "from create_rotor_dataset import PropellerDataset, subdir_condition_rotor\n",
    "\n",
    "# Root directory where simulation subdirectories are stored\n",
    "\n",
    "root_dir_rotor = project_path + '/FLOWUnsteady_simulations/aircraft_data/training_data'\n",
    "\n",
    "# dataset - Rotor L1\n",
    "dataset_rotor_L1 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'L1',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rL1, outputs_rL1 = dataset_rotor_L1[0:]\n",
    "\n",
    "\n",
    "input_tensor_rL1 = inputs_rL1.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - L1):\", input_tensor_rL1.shape) \n",
    "\n",
    "output_tensor_rL1 = outputs_rL1.squeeze(1)\n",
    "print(\"Output shape (rotor - L1):\",output_tensor_rL1.shape) \n",
    "\n",
    "\n",
    "# dataset - Rotor L2\n",
    "dataset_rotor_L2 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'L2',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rL2, outputs_rL2 = dataset_rotor_L2[0:]\n",
    "\n",
    "\n",
    "input_tensor_rL2 = inputs_rL2.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - L2):\", input_tensor_rL2.shape) \n",
    "\n",
    "output_tensor_rL2 = outputs_rL2.squeeze(1)\n",
    "print(\"Output shape (rotor - L2):\",output_tensor_rL2.shape) \n",
    "\n",
    "# dataset - Rotor L3\n",
    "dataset_rotor_L3 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'L3',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rL3, outputs_rL3 = dataset_rotor_L3[0:]\n",
    "\n",
    "\n",
    "input_tensor_rL3 = inputs_rL3.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - L3):\", input_tensor_rL3.shape) \n",
    "\n",
    "output_tensor_rL3 = outputs_rL3.squeeze(1)\n",
    "print(\"Output shape (rotor - L3):\",output_tensor_rL3.shape) \n",
    "\n",
    "# dataset - Rotor L4\n",
    "dataset_rotor_L4 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'L4',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rL4, outputs_rL4 = dataset_rotor_L4[0:]\n",
    "\n",
    "\n",
    "input_tensor_rL4 = inputs_rL4.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - L4):\", input_tensor_rL4.shape) \n",
    "\n",
    "output_tensor_rL4 = outputs_rL4.squeeze(1)\n",
    "print(\"Output shape (rotor - L4):\",output_tensor_rL4.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (rotor - R1): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - R1): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - R2): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - R2): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - R3): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - R3): torch.Size([42, 280, 2])\n",
      "Input shape (rotor - R4): torch.Size([42, 280, 10])\n",
      "Output shape (rotor - R4): torch.Size([42, 280, 2])\n"
     ]
    }
   ],
   "source": [
    "# dataset - Rotor R1\n",
    "dataset_rotor_R1 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'R1',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rR1, outputs_rR1 = dataset_rotor_R1[0:]\n",
    "\n",
    "\n",
    "input_tensor_rR1 = inputs_rR1.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - R1):\", input_tensor_rR1.shape) \n",
    "\n",
    "output_tensor_rR1 = outputs_rR1.squeeze(1)\n",
    "print(\"Output shape (rotor - R1):\",output_tensor_rR1.shape) \n",
    "\n",
    "\n",
    "# dataset - Rotor R2\n",
    "dataset_rotor_R2 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'R2',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rR2, outputs_rR2 = dataset_rotor_R2[0:]\n",
    "\n",
    "\n",
    "input_tensor_rR2 = inputs_rR2.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - R2):\", input_tensor_rR2.shape) \n",
    "\n",
    "output_tensor_rR2 = outputs_rR2.squeeze(1)\n",
    "print(\"Output shape (rotor - R2):\",output_tensor_rR2.shape) \n",
    "\n",
    "# dataset - Rotor R3\n",
    "dataset_rotor_R3 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'R3',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rR3, outputs_rR3 = dataset_rotor_R3[0:]\n",
    "\n",
    "\n",
    "input_tensor_rR3 = inputs_rR3.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - R3):\", input_tensor_rR3.shape) \n",
    "\n",
    "output_tensor_rR3 = outputs_rR3.squeeze(1)\n",
    "print(\"Output shape (rotor - R3):\",output_tensor_rR3.shape) \n",
    "\n",
    "# dataset - Rotor R4\n",
    "dataset_rotor_R4 = PropellerDataset(root_dir_rotor,\n",
    "                           rotor_notation = 'R4',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                           subdir_condition=subdir_condition_rotor)\n",
    "inputs_rR4, outputs_rR4 = dataset_rotor_R4[0:]\n",
    "\n",
    "\n",
    "input_tensor_rR4 = inputs_rR4.squeeze(1)  # Reshaping\n",
    "print(\"Input shape (rotor - R4):\", input_tensor_rR4.shape) \n",
    "\n",
    "output_tensor_rR4 = outputs_rR4.squeeze(1)\n",
    "print(\"Output shape (rotor - R4):\",output_tensor_rR4.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wing Model and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15533/3446642864.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wing_model_static.load_state_dict(torch.load(root_wingModelsTrained+'2024-11-19_LSTM_eMO_wingModel_static_lr0.002_e1200_nL3_numNN50.pth'))\n",
      "/mnt/e/eVTOL_model/eVTOL-VehicleModel/vehicle_env/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from wing_static import LSTMNet_static\n",
    "\n",
    "# Static Model\n",
    "input_size_wing_stat = 10           # Number of input features\n",
    "hidden_size_wing_stat = 50          # Hidden LSTM cells\n",
    "output_size_wing_stat = 2           # Number of output features\n",
    "num_layers_wing_stat = 3            # Number of LSTM layers\n",
    "\n",
    "class WingModel_static(LSTMNet_static):\n",
    "    def __init__(self):\n",
    "        super(WingModel_static, self).__init__(input_size_wing_stat, hidden_size_wing_stat, output_size_wing_stat, num_layers_wing_stat)\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "root_wingModelsTrained = project_path + '/trained_models/models/wing/'\n",
    "root_wingScalersTrained = project_path + '/trained_models/scalers/wing/'\n",
    "\n",
    "wing_model_static = WingModel_static()\n",
    "wing_model_static.load_state_dict(torch.load(root_wingModelsTrained+'2024-11-19_LSTM_eMO_wingModel_static_lr0.002_e1200_nL3_numNN50.pth'))\n",
    "wing_model_static = wing_model_static.to(device)\n",
    "wing_model_static.eval()\n",
    "\n",
    "# Load the scaler\n",
    "input_scaler_wing_stat = joblib.load(root_wingScalersTrained+'2024-11-19_LSTM_eMO_wingModel_static_ipScaler_lr0.002_e1200_nL3_numNN50.pkl')\n",
    "output_scaler_wing_stat = joblib.load(root_wingScalersTrained+'2024-11-19_LSTM_eMO_wingModel_static_opScaler_lr0.002_e1200_nL3_numNN50.pkl')\n",
    "\n",
    "input_scaler_wing_temp = MinMaxScaler()\n",
    "output_scaler_wing_temp = MinMaxScaler()\n",
    "\n",
    "input_scaler_canard_temp = MinMaxScaler()\n",
    "output_scaler_canard_temp = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Rotor Model and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15533/4040284321.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rotor1_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor1_lr0.0002_e2500_nL4_numNN100.pth'))\n",
      "/tmp/ipykernel_15533/4040284321.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rotor2_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor2_lr0.0002_e2500_nL4_numNN100.pth'))\n",
      "/tmp/ipykernel_15533/4040284321.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rotor3_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor3_lr0.0002_e2500_nL4_numNN100.pth'))\n",
      "/tmp/ipykernel_15533/4040284321.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rotor4_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor4_lr0.0002_e2500_nL4_numNN100.pth'))\n"
     ]
    }
   ],
   "source": [
    "from rotor_model import LSTMNet_rotor\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 100\n",
    "output_size = 2\n",
    "num_layers = 4\n",
    "\n",
    "class PropModel(LSTMNet_rotor):\n",
    "    def __init__(self):\n",
    "        super(PropModel, self).__init__(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "root_rotorModelsTrained = project_path + '/trained_models/models/rotor/'\n",
    "root_rotorScalersTrained = project_path + '/trained_models/scalers/rotor/'\n",
    "\n",
    "# Initialize the model\n",
    "# prop_model = PropModel()\n",
    "# # /mnt/e/eVTOL_model/eVTOL-VehicleModel/trained_models/models/rotor/2025-01-30_modified_H26FpropModel_retrained_lr0.0002_e1500_nL4_numNN100.pth\n",
    "# # prop_model.load_state_dict(torch.load(root_rotorModelsTrained+'2024-10-04_propModel_lr0.005_e1500_nL2_numNN50.pth'))\n",
    "# prop_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-01-30_modified_H26FpropModel_retrained_lr0.0002_e1500_nL4_numNN100.pth'))\n",
    "# prop_model = prop_model.to(device)\n",
    "# prop_model.eval()\n",
    "\n",
    "# Load the scaler\n",
    "# input_scaler_rotor = joblib.load(root_rotorScalersTrained+'2024-10-04_ipScaler_lr0.005_e1500_nL2_numNN50.pkl')\n",
    "# output_scaler_rotor = joblib.load(root_rotorScalersTrained+'2024-10-04_opScaler_lr0.005_e1500_nL2_numNN50.pkl')\n",
    "\n",
    "# input_scaler_rotor = joblib.load(root_rotorScalersTrained+'2025-01-30_modified_H26F_ipScaler_lr0.001_e2500_nL4_numNN100.pkl')\n",
    "# output_scaler_rotor = joblib.load(root_rotorScalersTrained+'2025-01-30_modified_H26F_opScaler_lr0.001_e2500_nL4_numNN100.pkl')\n",
    "\n",
    "rotor1_model = PropModel()\n",
    "rotor2_model = PropModel()\n",
    "rotor3_model = PropModel()\n",
    "rotor4_model = PropModel()\n",
    "\n",
    "rotor1_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor1_lr0.0002_e2500_nL4_numNN100.pth'))\n",
    "rotor2_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor2_lr0.0002_e2500_nL4_numNN100.pth'))\n",
    "rotor3_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor3_lr0.0002_e2500_nL4_numNN100.pth'))\n",
    "rotor4_model.load_state_dict(torch.load(root_rotorModelsTrained+'2025-02-05_H26FpropModel_rotor4_lr0.0002_e2500_nL4_numNN100.pth'))\n",
    "\n",
    "\n",
    "rotor1_model = rotor1_model.to(device)\n",
    "rotor2_model = rotor2_model.to(device)\n",
    "rotor3_model = rotor3_model.to(device)\n",
    "rotor4_model = rotor4_model.to(device)\n",
    "\n",
    "# 2025-02-05_H26F_rotor4_opScaler_lr0.0002_e2500_nL4_numNN100.pkl\n",
    "input_scaler_rotor1 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor1_ipScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "output_scaler_rotor1 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor1_opScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "\n",
    "input_scaler_rotor2 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor2_ipScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "output_scaler_rotor2 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor2_opScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "\n",
    "input_scaler_rotor3 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor3_ipScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "output_scaler_rotor3 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor3_opScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "\n",
    "input_scaler_rotor4 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor4_ipScaler_lr0.0002_e2500_nL4_numNN100.pkl')\n",
    "output_scaler_rotor4 = joblib.load(root_rotorScalersTrained+'2025-02-05_H26F_rotor4_opScaler_lr0.0002_e2500_nL4_numNN100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align the rotor data to the same length\n",
    "def align_timesteps(data, target_length):\n",
    "    original_length = data.shape[1]\n",
    "    x = np.linspace(0, 1, original_length)\n",
    "    x_new = np.linspace(0, 1, target_length)\n",
    "    interpolator = interp1d(x, data, axis=1, kind='linear')\n",
    "    return interpolator(x_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aircraft dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AircraftDataset(Dataset):\n",
    "    def __init__(self, wing_dataset,\n",
    "                       canard_dataset, \n",
    "                       propeller_L1_dataset, \n",
    "                       propeller_L2_dataset,\n",
    "                       propeller_L3_dataset, \n",
    "                       propeller_L4_dataset, \n",
    "                       propeller_R1_dataset, \n",
    "                       propeller_R2_dataset,\n",
    "                       propeller_R3_dataset, \n",
    "                       propeller_R4_dataset,\n",
    "                       wing_ip_scaler, wing_op_scaler,\n",
    "                       canard_ip_scaler, canard_op_scaler,\n",
    "                       rotor1_ip_scaler, rotor1_op_scaler,\n",
    "                       rotor2_ip_scaler, rotor2_op_scaler,\n",
    "                       rotor3_ip_scaler, rotor3_op_scaler,\n",
    "                       rotor4_ip_scaler, rotor4_op_scaler,\n",
    "                       additional_data=None):\n",
    "        super().__init__()\n",
    "        self.wing_dataset = wing_dataset\n",
    "        self.canard_dataset = canard_dataset\n",
    "        self.propeller_dataset_L1 = propeller_L1_dataset\n",
    "        self.propeller_dataset_L2 = propeller_L2_dataset\n",
    "        self.propeller_dataset_L3 = propeller_L3_dataset\n",
    "        self.propeller_dataset_L4 = propeller_L4_dataset\n",
    "        self.propeller_dataset_R1 = propeller_R1_dataset\n",
    "        self.propeller_dataset_R2 = propeller_R2_dataset\n",
    "        self.propeller_dataset_R3 = propeller_R3_dataset\n",
    "        self.propeller_dataset_R4 = propeller_R4_dataset\n",
    "        \n",
    "        self.additional_data = additional_data or {}\n",
    "        self.wing_ip_scaler = wing_ip_scaler\n",
    "        self.wing_op_scaler = wing_op_scaler\n",
    "        self.canard_ip_scaler = canard_ip_scaler\n",
    "        self.canard_op_scaler = canard_op_scaler\n",
    "        self.rotor1_ip_scaler = rotor1_ip_scaler\n",
    "        self.rotor1_op_scaler = rotor1_op_scaler\n",
    "        self.rotor2_ip_scaler = rotor2_ip_scaler\n",
    "        self.rotor2_op_scaler = rotor2_op_scaler\n",
    "        self.rotor3_ip_scaler = rotor3_ip_scaler\n",
    "        self.rotor3_op_scaler = rotor3_op_scaler\n",
    "        self.rotor4_ip_scaler = rotor4_ip_scaler\n",
    "        self.rotor4_op_scaler = rotor4_op_scaler\n",
    "\n",
    "        # Constants\n",
    "        k = 0.1301\n",
    "        self.constants = {\n",
    "            # Coordinates of the wing, canars and rotors\n",
    "\n",
    "            \"rotor_L1\": torch.tensor([0.36 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L2\": torch.tensor([2.28 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L3\": torch.tensor([4.2 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L4\": torch.tensor([6.54 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R1\": torch.tensor([0.36 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R2\": torch.tensor([2.28 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R3\": torch.tensor([4.2 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R4\": torch.tensor([6.54 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"canard\": torch.tensor([1.14 * k, 0.0, 0.35 * k], dtype=torch.float32),\n",
    "            \"wing\": torch.tensor([4.505 * k, 0.0, -0.2 * k], dtype=torch.float32),\n",
    "            \n",
    "\n",
    "        }\n",
    "\n",
    "        # Initialize data storage\n",
    "        self.data = {\n",
    "            \"time_varying_inputs\": None,\n",
    "            \"constant_inputs\": None,\n",
    "            \"node_data\": {},  # Store node-specific time-varying data\n",
    "        }\n",
    "        self.targets = None\n",
    "\n",
    "        # Combine datasets\n",
    "        self._combine_data()\n",
    "\n",
    "    def _combine_data(self):\n",
    "        # Load wing and rotor datasets\n",
    "\n",
    "        # Wing dataset\n",
    "        inputs_wing, target_wing = self.wing_dataset[0:]\n",
    "        inputs_wing = inputs_wing.squeeze(1)\n",
    "        target_wing = target_wing.squeeze(1)\n",
    "        # Canard dataset\n",
    "        inputs_canard, target_canard = self.canard_dataset[0:]\n",
    "        inputs_canard = inputs_canard.squeeze(1)\n",
    "        target_canard = target_canard.squeeze(1)\n",
    "        # Rotor - L1 dataset\n",
    "        inputs_rotor_L1, target_rotor_L1 = self.propeller_dataset_L1[0:]\n",
    "        inputs_rotor_L1 = inputs_rotor_L1.squeeze(1)\n",
    "        target_rotor_L1 = target_rotor_L1.squeeze(1)\n",
    "        # Rotor - L2 dataset\n",
    "        inputs_rotor_L2, target_rotor_L2 = self.propeller_dataset_L2[0:]\n",
    "        inputs_rotor_L2 = inputs_rotor_L2.squeeze(1)\n",
    "        target_rotor_L2 = target_rotor_L2.squeeze(1)\n",
    "        # Rotor - L3 dataset\n",
    "        inputs_rotor_L3, target_rotor_L3 = self.propeller_dataset_L3[0:]    \n",
    "        inputs_rotor_L3 = inputs_rotor_L3.squeeze(1)\n",
    "        target_rotor_L3 = target_rotor_L3.squeeze(1)\n",
    "        # Rotor - L4 dataset\n",
    "        inputs_rotor_L4, target_rotor_L4 = self.propeller_dataset_L4[0:]\n",
    "        inputs_rotor_L4 = inputs_rotor_L4.squeeze(1)\n",
    "        target_rotor_L4 = target_rotor_L4.squeeze(1)\n",
    "\n",
    "        # Rotor - R1 dataset\n",
    "        inputs_rotor_R1, target_rotor_R1 = self.propeller_dataset_R1[0:]\n",
    "        inputs_rotor_R1 = inputs_rotor_R1.squeeze(1)\n",
    "        target_rotor_R1 = target_rotor_R1.squeeze(1)\n",
    "\n",
    "        # Rotor - R2 dataset\n",
    "        inputs_rotor_R2, target_rotor_R2 = self.propeller_dataset_R2[0:]\n",
    "        inputs_rotor_R2 = inputs_rotor_R2.squeeze(1)\n",
    "        target_rotor_R2 = target_rotor_R2.squeeze(1)\n",
    "\n",
    "        # Rotor - R3 dataset\n",
    "        inputs_rotor_R3, target_rotor_R3 = self.propeller_dataset_R3[0:]\n",
    "        inputs_rotor_R3 = inputs_rotor_R3.squeeze(1)\n",
    "        target_rotor_R3 = target_rotor_R3.squeeze(1)\n",
    "\n",
    "        # Rotor - R4 dataset\n",
    "        inputs_rotor_R4, target_rotor_R4 = self.propeller_dataset_R4[0:]\n",
    "        inputs_rotor_R4 = inputs_rotor_R4.squeeze(1)\n",
    "        target_rotor_R4 = target_rotor_R4.squeeze(1)\n",
    "\n",
    "        # Normalize the wing and canard inputs\n",
    "        inputs_wing_reshaped = inputs_wing.reshape(-1, 10)\n",
    "        inputs_wing_normalized = self.wing_ip_scaler.fit_transform(inputs_wing_reshaped.reshape(-1, 10)).reshape(inputs_wing.shape)\n",
    "        inputs_wing_normalized = torch.tensor(inputs_wing_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_canard_reshaped = inputs_canard.reshape(-1, 10)\n",
    "        inputs_canard_normalized = self.canard_ip_scaler.fit_transform(inputs_canard_reshaped.reshape(-1, 10)).reshape(inputs_canard.shape)\n",
    "        inputs_canard_normalized = torch.tensor(inputs_canard_normalized, dtype=torch.float32)\n",
    "\n",
    "        # Time-varying features\n",
    "        T = inputs_wing_normalized[:, :, 0]\n",
    "        AOA = inputs_wing_normalized[:, :, 1]\n",
    "        v_inf = inputs_wing_normalized[:, :, 2]\n",
    "\n",
    "\n",
    "        # Resize the rotor inputs\n",
    "        inputs_resized_rotor_L1 = torch.tensor(align_timesteps(inputs_rotor_L1, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L2 = torch.tensor(align_timesteps(inputs_rotor_L2, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L3 = torch.tensor(align_timesteps(inputs_rotor_L3, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L4 = torch.tensor(align_timesteps(inputs_rotor_L4, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R1 = torch.tensor(align_timesteps(inputs_rotor_R1, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R2 = torch.tensor(align_timesteps(inputs_rotor_R2, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R3 = torch.tensor(align_timesteps(inputs_rotor_R3, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R4 = torch.tensor(align_timesteps(inputs_rotor_R4, T.shape[1]), dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the rotor inputs\n",
    "        inputs_rotor_L1_reshaped = inputs_resized_rotor_L1.reshape(-1, 10)\n",
    "        inputs_rotor_L1_normalized = self.rotor1_ip_scaler.transform(inputs_rotor_L1_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L1.shape)\n",
    "        inputs_rotor_L1_normalized = torch.tensor(inputs_rotor_L1_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L2_reshaped = inputs_resized_rotor_L2.reshape(-1, 10)\n",
    "        inputs_rotor_L2_normalized = self.rotor2_ip_scaler.transform(inputs_rotor_L2_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L2.shape)\n",
    "        inputs_rotor_L2_normalized = torch.tensor(inputs_rotor_L2_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L3_reshaped = inputs_resized_rotor_L3.reshape(-1, 10)\n",
    "        inputs_rotor_L3_normalized = self.rotor3_ip_scaler.transform(inputs_rotor_L3_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L3.shape)\n",
    "        inputs_rotor_L3_normalized = torch.tensor(inputs_rotor_L3_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L4_reshaped = inputs_resized_rotor_L4.reshape(-1, 10)\n",
    "        inputs_rotor_L4_normalized = self.rotor4_ip_scaler.transform(inputs_rotor_L4_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L4.shape)\n",
    "        inputs_rotor_L4_normalized = torch.tensor(inputs_rotor_L4_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R1_reshaped = inputs_resized_rotor_R1.reshape(-1, 10)\n",
    "        inputs_rotor_R1_normalized = self.rotor1_ip_scaler.transform(inputs_rotor_R1_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R1.shape)\n",
    "        inputs_rotor_R1_normalized = torch.tensor(inputs_rotor_R1_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R2_reshaped = inputs_resized_rotor_R2.reshape(-1, 10)\n",
    "        inputs_rotor_R2_normalized = self.rotor2_ip_scaler.transform(inputs_rotor_R2_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R2.shape)\n",
    "        inputs_rotor_R2_normalized = torch.tensor(inputs_rotor_R2_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R3_reshaped = inputs_resized_rotor_R3.reshape(-1, 10)\n",
    "        inputs_rotor_R3_normalized = self.rotor3_ip_scaler.transform(inputs_rotor_R3_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R3.shape)\n",
    "        inputs_rotor_R3_normalized = torch.tensor(inputs_rotor_R3_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R4_reshaped = inputs_resized_rotor_R4.reshape(-1, 10)\n",
    "        inputs_rotor_R4_normalized = self.rotor4_ip_scaler.transform(inputs_rotor_R4_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R4.shape)\n",
    "        inputs_rotor_R4_normalized = torch.tensor(inputs_rotor_R4_normalized, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        omega = inputs_rotor_L1_normalized[:, :, 1]\n",
    "        sine_component = inputs_rotor_L1_normalized[:, :, 4]\n",
    "        cos_component = inputs_rotor_L1_normalized[:, :, 5]\n",
    "\n",
    "\n",
    "        # Store time-varying inputs in self.data\n",
    "        self.data[\"time_varying_inputs\"] = torch.stack([T, AOA, v_inf, omega], dim=2)       # [num_simulations, n_timesteps, 6]\n",
    "        self.data[\"node_data\"][\"rotor_L1\"] = inputs_rotor_L1_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L2\"] = inputs_rotor_L2_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L3\"] = inputs_rotor_L3_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L4\"] = inputs_rotor_L4_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R1\"] = inputs_rotor_R1_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R2\"] = inputs_rotor_R2_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R3\"] = inputs_rotor_R3_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R4\"] = inputs_rotor_R4_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"canard\"] = inputs_canard_normalized                # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"wing\"] = inputs_wing_normalized                    # [num_simulations, n_timesteps, 10]\n",
    "        \n",
    "        # Store constant features\n",
    "        self.data[\"constant_inputs\"] = torch.stack(list(self.constants.values()))\n",
    "\n",
    "        # Combine targets\n",
    "        target_rotor_L1 = torch.tensor(align_timesteps(target_rotor_L1, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R1 = torch.tensor(align_timesteps(target_rotor_R1, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L2 = torch.tensor(align_timesteps(target_rotor_L2, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R2 = torch.tensor(align_timesteps(target_rotor_R2, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L3 = torch.tensor(align_timesteps(target_rotor_L3, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R3 = torch.tensor(align_timesteps(target_rotor_R3, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L4 = torch.tensor(align_timesteps(target_rotor_L4, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R4 = torch.tensor(align_timesteps(target_rotor_R4, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "\n",
    "        target_wing_reshaped = target_wing.reshape(-1, 2)\n",
    "        target_wing_normalized = self.wing_op_scaler.fit_transform(target_wing_reshaped.reshape(-1, 2)).reshape(target_wing.shape)\n",
    "        target_wing_normalized = torch.tensor(target_wing_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_canard_reshaped = target_canard.reshape(-1, 2)\n",
    "        target_canard_normalized = self.canard_op_scaler.fit_transform(target_canard_reshaped.reshape(-1, 2)).reshape(target_canard.shape)\n",
    "        target_canard_normalized = torch.tensor(target_canard_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L1_reshaped = target_rotor_L1.reshape(-1, 2)\n",
    "        target_rotor_L1_normalized = self.rotor1_op_scaler.transform(target_rotor_L1_reshaped.reshape(-1, 2)).reshape(target_rotor_L1.shape)\n",
    "        target_rotor_L1_normalized = torch.tensor(target_rotor_L1_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L2_reshaped = target_rotor_L2.reshape(-1, 2)\n",
    "        target_rotor_L2_normalized = self.rotor2_op_scaler.transform(target_rotor_L2_reshaped.reshape(-1, 2)).reshape(target_rotor_L2.shape)\n",
    "        target_rotor_L2_normalized = torch.tensor(target_rotor_L2_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L3_reshaped = target_rotor_L3.reshape(-1, 2)\n",
    "        target_rotor_L3_normalized = self.rotor3_op_scaler.transform(target_rotor_L3_reshaped.reshape(-1, 2)).reshape(target_rotor_L3.shape)\n",
    "        target_rotor_L3_normalized = torch.tensor(target_rotor_L3_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L4_reshaped = target_rotor_L4.reshape(-1, 2)\n",
    "        target_rotor_L4_normalized = self.rotor4_op_scaler.transform(target_rotor_L4_reshaped.reshape(-1, 2)).reshape(target_rotor_L4.shape)\n",
    "        target_rotor_L4_normalized = torch.tensor(target_rotor_L4_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R1_reshaped = target_rotor_R1.reshape(-1, 2)\n",
    "        target_rotor_R1_normalized = self.rotor1_op_scaler.transform(target_rotor_R1_reshaped.reshape(-1, 2)).reshape(target_rotor_R1.shape)\n",
    "        target_rotor_R1_normalized = torch.tensor(target_rotor_R1_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R2_reshaped = target_rotor_R2.reshape(-1, 2)\n",
    "        target_rotor_R2_normalized = self.rotor2_op_scaler.transform(target_rotor_R2_reshaped.reshape(-1, 2)).reshape(target_rotor_R2.shape)\n",
    "        target_rotor_R2_normalized = torch.tensor(target_rotor_R2_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R3_reshaped = target_rotor_R3.reshape(-1, 2)\n",
    "        target_rotor_R3_normalized = self.rotor3_op_scaler.transform(target_rotor_R3_reshaped.reshape(-1, 2)).reshape(target_rotor_R3.shape)\n",
    "        target_rotor_R3_normalized = torch.tensor(target_rotor_R3_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R4_reshaped = target_rotor_R4.reshape(-1, 2)\n",
    "        target_rotor_R4_normalized = self.rotor4_op_scaler.transform(target_rotor_R4_reshaped.reshape(-1, 2)).reshape(target_rotor_R4.shape)\n",
    "        target_rotor_R4_normalized = torch.tensor(target_rotor_R4_normalized, dtype=torch.float32)\n",
    "\n",
    "        self.targets = torch.cat([target_rotor_L1_normalized, target_rotor_L2_normalized,\n",
    "                                   target_rotor_L3_normalized, target_rotor_L4_normalized,\n",
    "                                   target_rotor_R1_normalized, target_rotor_R2_normalized,\n",
    "                                   target_rotor_R3_normalized, target_rotor_R4_normalized,\n",
    "                                   target_canard_normalized, target_wing_normalized], dim=2)\n",
    "\n",
    "                                   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"time_varying_inputs\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {\n",
    "            \"node_data\": {\n",
    "                \"rotor_L1\": self.data[\"node_data\"][\"rotor_L1\"][idx],\n",
    "                \"rotor_L2\": self.data[\"node_data\"][\"rotor_L2\"][idx],\n",
    "                \"rotor_L3\": self.data[\"node_data\"][\"rotor_L3\"][idx],\n",
    "                \"rotor_L4\": self.data[\"node_data\"][\"rotor_L4\"][idx],\n",
    "                \"rotor_R1\": self.data[\"node_data\"][\"rotor_R1\"][idx],\n",
    "                \"rotor_R2\": self.data[\"node_data\"][\"rotor_R2\"][idx],\n",
    "                \"rotor_R3\": self.data[\"node_data\"][\"rotor_R3\"][idx],\n",
    "                \"rotor_R4\": self.data[\"node_data\"][\"rotor_R4\"][idx],\n",
    "                \"canard\": self.data[\"node_data\"][\"canard\"][idx],\n",
    "                \"wing\": self.data[\"node_data\"][\"wing\"][idx]\n",
    "            },\n",
    "            \"constant_inputs\": self.data[\"constant_inputs\"],\n",
    "            \"time_varying_inputs\": self.data[\"time_varying_inputs\"][idx]\n",
    "        }\n",
    "        targets = self.targets[idx]\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AircraftDataset(Dataset):\n",
    "    def __init__(self, wing_dataset,\n",
    "                       canard_dataset, \n",
    "                       propeller_L1_dataset, \n",
    "                       propeller_L2_dataset,\n",
    "                       propeller_L3_dataset, \n",
    "                       propeller_L4_dataset, \n",
    "                       propeller_R1_dataset, \n",
    "                       propeller_R2_dataset,\n",
    "                       propeller_R3_dataset, \n",
    "                       propeller_R4_dataset,\n",
    "                       wing_ip_scaler, wing_op_scaler,\n",
    "                       canard_ip_scaler, canard_op_scaler,\n",
    "                       rotor1_ip_scaler, rotor1_op_scaler,\n",
    "                       rotor2_ip_scaler, rotor2_op_scaler,\n",
    "                       rotor3_ip_scaler, rotor3_op_scaler,\n",
    "                       rotor4_ip_scaler, rotor4_op_scaler,\n",
    "                       additional_data=None):\n",
    "        \"\"\"\n",
    "        Initializes the AircraftDataset.\n",
    "\n",
    "        Args:\n",
    "            wing_dataset (Dataset): Dataset for the wing.\n",
    "            canard_dataset (Dataset): Dataset for the canard.\n",
    "            propeller_L1_dataset (Dataset): Dataset for the left propeller 1.\n",
    "            propeller_L2_dataset (Dataset): Dataset for the left propeller 2.\n",
    "            propeller_L3_dataset (Dataset): Dataset for the left propeller 3.\n",
    "            propeller_L4_dataset (Dataset): Dataset for the left propeller 4.\n",
    "            propeller_R1_dataset (Dataset): Dataset for the right propeller 1.\n",
    "            propeller_R2_dataset (Dataset): Dataset for the right propeller 2.\n",
    "            propeller_R3_dataset (Dataset): Dataset for the right propeller 3.\n",
    "            propeller_R4_dataset (Dataset): Dataset for the right propeller 4.\n",
    "            wing_ip_scaler (scaler): Scaler for wing input data.\n",
    "            wing_op_scaler (scaler): Scaler for wing output data.\n",
    "            canard_ip_scaler (scaler): Scaler for canard input data.\n",
    "            canard_op_scaler (scaler): Scaler for canard output data.\n",
    "            rotor1_ip_scaler (scaler): Scaler for rotor 1 input data.\n",
    "            rotor1_op_scaler (scaler): Scaler for rotor 1 output data.\n",
    "            rotor2_ip_scaler (scaler): Scaler for rotor 2 input data.\n",
    "            rotor2_op_scaler (scaler): Scaler for rotor 2 output data.\n",
    "            rotor3_ip_scaler (scaler): Scaler for rotor 3 input data.\n",
    "            rotor3_op_scaler (scaler): Scaler for rotor 3 output data.\n",
    "            rotor4_ip_scaler (scaler): Scaler for rotor 4 input data.\n",
    "            rotor4_op_scaler (scaler): Scaler for rotor 4 output data.\n",
    "            additional_data (dict, optional): Additional data to be included. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.wing_dataset = wing_dataset\n",
    "        self.canard_dataset = canard_dataset\n",
    "        self.propeller_dataset_L1 = propeller_L1_dataset\n",
    "        self.propeller_dataset_L2 = propeller_L2_dataset\n",
    "        self.propeller_dataset_L3 = propeller_L3_dataset\n",
    "        self.propeller_dataset_L4 = propeller_L4_dataset\n",
    "        self.propeller_dataset_R1 = propeller_R1_dataset\n",
    "        self.propeller_dataset_R2 = propeller_R2_dataset\n",
    "        self.propeller_dataset_R3 = propeller_R3_dataset\n",
    "        self.propeller_dataset_R4 = propeller_R4_dataset\n",
    "        \n",
    "        self.additional_data = additional_data or {}\n",
    "        self.wing_ip_scaler = wing_ip_scaler\n",
    "        self.wing_op_scaler = wing_op_scaler\n",
    "        self.canard_ip_scaler = canard_ip_scaler\n",
    "        self.canard_op_scaler = canard_op_scaler\n",
    "        self.rotor1_ip_scaler = rotor1_ip_scaler\n",
    "        self.rotor1_op_scaler = rotor1_op_scaler\n",
    "        self.rotor2_ip_scaler = rotor2_ip_scaler\n",
    "        self.rotor2_op_scaler = rotor2_op_scaler\n",
    "        self.rotor3_ip_scaler = rotor3_ip_scaler\n",
    "        self.rotor3_op_scaler = rotor3_op_scaler\n",
    "        self.rotor4_ip_scaler = rotor4_ip_scaler\n",
    "        self.rotor4_op_scaler = rotor4_op_scaler\n",
    "\n",
    "        # Constants - Aircraft components represented as cartisian coordinates\n",
    "        k = 0.1301\n",
    "        self.constants = {\n",
    "            # Coordinates of the wing, canars and rotors\n",
    "\n",
    "            \"rotor_L1\": torch.tensor([0.36 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L2\": torch.tensor([2.28 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L3\": torch.tensor([4.2 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_L4\": torch.tensor([6.54 * k, -3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R1\": torch.tensor([0.36 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R2\": torch.tensor([2.28 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R3\": torch.tensor([4.2 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"rotor_R4\": torch.tensor([6.54 * k, 3.842 * k, 0.5 * k], dtype=torch.float32),\n",
    "            \"canard\": torch.tensor([1.14 * k, 0.0, 0.35 * k], dtype=torch.float32),\n",
    "            \"wing\": torch.tensor([4.505 * k, 0.0, -0.2 * k], dtype=torch.float32),\n",
    "            \n",
    "\n",
    "        }\n",
    "\n",
    "        # Initialize data storage\n",
    "        self.data = {\n",
    "            \"time_varying_inputs\": None,\n",
    "            \"constant_inputs\": None,\n",
    "            \"node_data\": {},  # Store node-specific time-varying data\n",
    "        }\n",
    "        self.targets = None\n",
    "\n",
    "        # Combine datasets\n",
    "        self._combine_data()\n",
    "\n",
    "    def _combine_data(self):\n",
    "        # Load wing and rotor datasets\n",
    "\n",
    "        # Wing dataset\n",
    "        inputs_wing, target_wing = self.wing_dataset[0:]\n",
    "        inputs_wing = inputs_wing.squeeze(1)\n",
    "        target_wing = target_wing.squeeze(1)\n",
    "        # Canard dataset\n",
    "        inputs_canard, target_canard = self.canard_dataset[0:]\n",
    "        inputs_canard = inputs_canard.squeeze(1)\n",
    "        target_canard = target_canard.squeeze(1)\n",
    "        # Rotor - L1 dataset\n",
    "        inputs_rotor_L1, target_rotor_L1 = self.propeller_dataset_L1[0:]\n",
    "        inputs_rotor_L1 = inputs_rotor_L1.squeeze(1)\n",
    "        target_rotor_L1 = target_rotor_L1.squeeze(1)\n",
    "        # Rotor - L2 dataset\n",
    "        inputs_rotor_L2, target_rotor_L2 = self.propeller_dataset_L2[0:]\n",
    "        inputs_rotor_L2 = inputs_rotor_L2.squeeze(1)\n",
    "        target_rotor_L2 = target_rotor_L2.squeeze(1)\n",
    "        # Rotor - L3 dataset\n",
    "        inputs_rotor_L3, target_rotor_L3 = self.propeller_dataset_L3[0:]    \n",
    "        inputs_rotor_L3 = inputs_rotor_L3.squeeze(1)\n",
    "        target_rotor_L3 = target_rotor_L3.squeeze(1)\n",
    "        # Rotor - L4 dataset\n",
    "        inputs_rotor_L4, target_rotor_L4 = self.propeller_dataset_L4[0:]\n",
    "        inputs_rotor_L4 = inputs_rotor_L4.squeeze(1)\n",
    "        target_rotor_L4 = target_rotor_L4.squeeze(1)\n",
    "\n",
    "        # Rotor - R1 dataset\n",
    "        inputs_rotor_R1, target_rotor_R1 = self.propeller_dataset_R1[0:]\n",
    "        inputs_rotor_R1 = inputs_rotor_R1.squeeze(1)\n",
    "        target_rotor_R1 = target_rotor_R1.squeeze(1)\n",
    "\n",
    "        # Rotor - R2 dataset\n",
    "        inputs_rotor_R2, target_rotor_R2 = self.propeller_dataset_R2[0:]\n",
    "        inputs_rotor_R2 = inputs_rotor_R2.squeeze(1)\n",
    "        target_rotor_R2 = target_rotor_R2.squeeze(1)\n",
    "\n",
    "        # Rotor - R3 dataset\n",
    "        inputs_rotor_R3, target_rotor_R3 = self.propeller_dataset_R3[0:]\n",
    "        inputs_rotor_R3 = inputs_rotor_R3.squeeze(1)\n",
    "        target_rotor_R3 = target_rotor_R3.squeeze(1)\n",
    "\n",
    "        # Rotor - R4 dataset\n",
    "        inputs_rotor_R4, target_rotor_R4 = self.propeller_dataset_R4[0:]\n",
    "        inputs_rotor_R4 = inputs_rotor_R4.squeeze(1)\n",
    "        target_rotor_R4 = target_rotor_R4.squeeze(1)\n",
    "\n",
    "        # Normalize the wing and canard inputs\n",
    "        inputs_wing_reshaped = inputs_wing.reshape(-1, 10)\n",
    "        inputs_wing_normalized = self.wing_ip_scaler.fit_transform(inputs_wing_reshaped.reshape(-1, 10)).reshape(inputs_wing.shape)\n",
    "        inputs_wing_normalized = torch.tensor(inputs_wing_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_canard_reshaped = inputs_canard.reshape(-1, 10)\n",
    "        inputs_canard_normalized = self.canard_ip_scaler.fit_transform(inputs_canard_reshaped.reshape(-1, 10)).reshape(inputs_canard.shape)\n",
    "        inputs_canard_normalized = torch.tensor(inputs_canard_normalized, dtype=torch.float32)\n",
    "\n",
    "        # Time-varying features\n",
    "        T = inputs_wing_normalized[:, :, 0]\n",
    "        AOA = inputs_wing_normalized[:, :, 1]\n",
    "        v_inf = inputs_wing_normalized[:, :, 2]\n",
    "\n",
    "\n",
    "        # Resize the rotor inputs\n",
    "        inputs_resized_rotor_L1 = torch.tensor(align_timesteps(inputs_rotor_L1, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L2 = torch.tensor(align_timesteps(inputs_rotor_L2, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L3 = torch.tensor(align_timesteps(inputs_rotor_L3, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_L4 = torch.tensor(align_timesteps(inputs_rotor_L4, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R1 = torch.tensor(align_timesteps(inputs_rotor_R1, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R2 = torch.tensor(align_timesteps(inputs_rotor_R2, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R3 = torch.tensor(align_timesteps(inputs_rotor_R3, T.shape[1]), dtype=torch.float32)\n",
    "        inputs_resized_rotor_R4 = torch.tensor(align_timesteps(inputs_rotor_R4, T.shape[1]), dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the rotor inputs\n",
    "        inputs_rotor_L1_reshaped = inputs_resized_rotor_L1.reshape(-1, 10)\n",
    "        inputs_rotor_L1_normalized = self.rotor1_ip_scaler.transform(inputs_rotor_L1_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L1.shape)\n",
    "        inputs_rotor_L1_normalized = torch.tensor(inputs_rotor_L1_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L2_reshaped = inputs_resized_rotor_L2.reshape(-1, 10)\n",
    "        inputs_rotor_L2_normalized = self.rotor2_ip_scaler.transform(inputs_rotor_L2_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L2.shape)\n",
    "        inputs_rotor_L2_normalized = torch.tensor(inputs_rotor_L2_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L3_reshaped = inputs_resized_rotor_L3.reshape(-1, 10)\n",
    "        inputs_rotor_L3_normalized = self.rotor3_ip_scaler.transform(inputs_rotor_L3_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L3.shape)\n",
    "        inputs_rotor_L3_normalized = torch.tensor(inputs_rotor_L3_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_L4_reshaped = inputs_resized_rotor_L4.reshape(-1, 10)\n",
    "        inputs_rotor_L4_normalized = self.rotor4_ip_scaler.transform(inputs_rotor_L4_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_L4.shape)\n",
    "        inputs_rotor_L4_normalized = torch.tensor(inputs_rotor_L4_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R1_reshaped = inputs_resized_rotor_R1.reshape(-1, 10)\n",
    "        inputs_rotor_R1_normalized = self.rotor1_ip_scaler.transform(inputs_rotor_R1_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R1.shape)\n",
    "        inputs_rotor_R1_normalized = torch.tensor(inputs_rotor_R1_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R2_reshaped = inputs_resized_rotor_R2.reshape(-1, 10)\n",
    "        inputs_rotor_R2_normalized = self.rotor2_ip_scaler.transform(inputs_rotor_R2_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R2.shape)\n",
    "        inputs_rotor_R2_normalized = torch.tensor(inputs_rotor_R2_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R3_reshaped = inputs_resized_rotor_R3.reshape(-1, 10)\n",
    "        inputs_rotor_R3_normalized = self.rotor3_ip_scaler.transform(inputs_rotor_R3_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R3.shape)\n",
    "        inputs_rotor_R3_normalized = torch.tensor(inputs_rotor_R3_normalized, dtype=torch.float32)\n",
    "\n",
    "        inputs_rotor_R4_reshaped = inputs_resized_rotor_R4.reshape(-1, 10)\n",
    "        inputs_rotor_R4_normalized = self.rotor4_ip_scaler.transform(inputs_rotor_R4_reshaped.reshape(-1, 10)).reshape(inputs_resized_rotor_R4.shape)\n",
    "        inputs_rotor_R4_normalized = torch.tensor(inputs_rotor_R4_normalized, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        omega = inputs_rotor_L1_normalized[:, :, 1]\n",
    "        sine_component = inputs_rotor_L1_normalized[:, :, 4]\n",
    "        cos_component = inputs_rotor_L1_normalized[:, :, 5]\n",
    "\n",
    "\n",
    "        # Store time-varying inputs in self.data\n",
    "        self.data[\"time_varying_inputs\"] = torch.stack([T, AOA, v_inf, omega], dim=2)       # [num_simulations, n_timesteps, 6]\n",
    "        self.data[\"node_data\"][\"rotor_L1\"] = inputs_rotor_L1_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L2\"] = inputs_rotor_L2_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L3\"] = inputs_rotor_L3_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_L4\"] = inputs_rotor_L4_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R1\"] = inputs_rotor_R1_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R2\"] = inputs_rotor_R2_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R3\"] = inputs_rotor_R3_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"rotor_R4\"] = inputs_rotor_R4_normalized     # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"canard\"] = inputs_canard_normalized                # [num_simulations, n_timesteps, 10]\n",
    "        self.data[\"node_data\"][\"wing\"] = inputs_wing_normalized                    # [num_simulations, n_timesteps, 10]\n",
    "        \n",
    "        # Store constant features\n",
    "        self.data[\"constant_inputs\"] = torch.stack(list(self.constants.values()))\n",
    "\n",
    "        # Combine targets\n",
    "        target_rotor_L1 = torch.tensor(align_timesteps(target_rotor_L1, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R1 = torch.tensor(align_timesteps(target_rotor_R1, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L2 = torch.tensor(align_timesteps(target_rotor_L2, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R2 = torch.tensor(align_timesteps(target_rotor_R2, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L3 = torch.tensor(align_timesteps(target_rotor_L3, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R3 = torch.tensor(align_timesteps(target_rotor_R3, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_L4 = torch.tensor(align_timesteps(target_rotor_L4, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "        target_rotor_R4 = torch.tensor(align_timesteps(target_rotor_R4, T.shape[1]), dtype=torch.float32)       # [num_simulations, n_timesteps, 4]\n",
    "\n",
    "        # Normalize the wing and canard outputs\n",
    "        target_wing_reshaped = target_wing.reshape(-1, 2)\n",
    "        target_wing_normalized = self.wing_op_scaler.fit_transform(target_wing_reshaped.reshape(-1, 2)).reshape(target_wing.shape)\n",
    "        target_wing_normalized = torch.tensor(target_wing_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_canard_reshaped = target_canard.reshape(-1, 2)\n",
    "        target_canard_normalized = self.canard_op_scaler.fit_transform(target_canard_reshaped.reshape(-1, 2)).reshape(target_canard.shape)\n",
    "        target_canard_normalized = torch.tensor(target_canard_normalized, dtype=torch.float32)\n",
    "\n",
    "        # Normalize the rotor outputs\n",
    "        target_rotor_L1_reshaped = target_rotor_L1.reshape(-1, 2)\n",
    "        target_rotor_L1_normalized = self.rotor1_op_scaler.transform(target_rotor_L1_reshaped.reshape(-1, 2)).reshape(target_rotor_L1.shape)\n",
    "        target_rotor_L1_normalized = torch.tensor(target_rotor_L1_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L2_reshaped = target_rotor_L2.reshape(-1, 2)\n",
    "        target_rotor_L2_normalized = self.rotor2_op_scaler.transform(target_rotor_L2_reshaped.reshape(-1, 2)).reshape(target_rotor_L2.shape)\n",
    "        target_rotor_L2_normalized = torch.tensor(target_rotor_L2_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L3_reshaped = target_rotor_L3.reshape(-1, 2)\n",
    "        target_rotor_L3_normalized = self.rotor3_op_scaler.transform(target_rotor_L3_reshaped.reshape(-1, 2)).reshape(target_rotor_L3.shape)\n",
    "        target_rotor_L3_normalized = torch.tensor(target_rotor_L3_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_L4_reshaped = target_rotor_L4.reshape(-1, 2)\n",
    "        target_rotor_L4_normalized = self.rotor4_op_scaler.transform(target_rotor_L4_reshaped.reshape(-1, 2)).reshape(target_rotor_L4.shape)\n",
    "        target_rotor_L4_normalized = torch.tensor(target_rotor_L4_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R1_reshaped = target_rotor_R1.reshape(-1, 2)\n",
    "        target_rotor_R1_normalized = self.rotor1_op_scaler.transform(target_rotor_R1_reshaped.reshape(-1, 2)).reshape(target_rotor_R1.shape)\n",
    "        target_rotor_R1_normalized = torch.tensor(target_rotor_R1_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R2_reshaped = target_rotor_R2.reshape(-1, 2)\n",
    "        target_rotor_R2_normalized = self.rotor2_op_scaler.transform(target_rotor_R2_reshaped.reshape(-1, 2)).reshape(target_rotor_R2.shape)\n",
    "        target_rotor_R2_normalized = torch.tensor(target_rotor_R2_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R3_reshaped = target_rotor_R3.reshape(-1, 2)\n",
    "        target_rotor_R3_normalized = self.rotor3_op_scaler.transform(target_rotor_R3_reshaped.reshape(-1, 2)).reshape(target_rotor_R3.shape)\n",
    "        target_rotor_R3_normalized = torch.tensor(target_rotor_R3_normalized, dtype=torch.float32)\n",
    "\n",
    "        target_rotor_R4_reshaped = target_rotor_R4.reshape(-1, 2)\n",
    "        target_rotor_R4_normalized = self.rotor4_op_scaler.transform(target_rotor_R4_reshaped.reshape(-1, 2)).reshape(target_rotor_R4.shape)\n",
    "        target_rotor_R4_normalized = torch.tensor(target_rotor_R4_normalized, dtype=torch.float32)\n",
    "\n",
    "        # Combine the targets into a single tensor \n",
    "        self.targets = torch.cat([target_rotor_L1_normalized, target_rotor_L2_normalized,\n",
    "                                   target_rotor_L3_normalized, target_rotor_L4_normalized,\n",
    "                                   target_rotor_R1_normalized, target_rotor_R2_normalized,\n",
    "                                   target_rotor_R3_normalized, target_rotor_R4_normalized,\n",
    "                                   target_canard_normalized, target_wing_normalized], dim=2)\n",
    "\n",
    "                                   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"time_varying_inputs\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {\n",
    "            \"node_data\": {\n",
    "                \"rotor_L1\": self.data[\"node_data\"][\"rotor_L1\"][idx],\n",
    "                \"rotor_L2\": self.data[\"node_data\"][\"rotor_L2\"][idx],\n",
    "                \"rotor_L3\": self.data[\"node_data\"][\"rotor_L3\"][idx],\n",
    "                \"rotor_L4\": self.data[\"node_data\"][\"rotor_L4\"][idx],\n",
    "                \"rotor_R1\": self.data[\"node_data\"][\"rotor_R1\"][idx],\n",
    "                \"rotor_R2\": self.data[\"node_data\"][\"rotor_R2\"][idx],\n",
    "                \"rotor_R3\": self.data[\"node_data\"][\"rotor_R3\"][idx],\n",
    "                \"rotor_R4\": self.data[\"node_data\"][\"rotor_R4\"][idx],\n",
    "                \"canard\": self.data[\"node_data\"][\"canard\"][idx],\n",
    "                \"wing\": self.data[\"node_data\"][\"wing\"][idx]\n",
    "            },\n",
    "            \"constant_inputs\": self.data[\"constant_inputs\"],\n",
    "            \"time_varying_inputs\": self.data[\"time_varying_inputs\"][idx]\n",
    "        }\n",
    "        targets = self.targets[idx]\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_graph(inputs, targets):\n",
    "    \"\"\"\n",
    "    Create a graph structure from dataset inputs and targets.\n",
    "\n",
    "    Args:\n",
    "        inputs (dict): Includes node data and constant inputs.\n",
    "        targets (Tensor): The target outputs.\n",
    "\n",
    "    Returns:\n",
    "        Data: PyTorch Geometric Data object representing the graph.\n",
    "    \"\"\"\n",
    "    # Extract node features\n",
    "    node_features = torch.stack([\n",
    "        inputs[\"node_data\"][\"rotor_L1\"],        # Node 0: Rotor 1 features\n",
    "        inputs[\"node_data\"][\"rotor_L2\"],        # Node 1: Rotor 2 features\n",
    "        inputs[\"node_data\"][\"rotor_L3\"],        # Node 2: Rotor 3 features\n",
    "        inputs[\"node_data\"][\"rotor_L4\"],        # Node 3: Rotor 4 features\n",
    "        inputs[\"node_data\"][\"rotor_R1\"],        # Node 4: Rotor 5 features\n",
    "        inputs[\"node_data\"][\"rotor_R2\"],        # Node 5: Rotor 6 features\n",
    "        inputs[\"node_data\"][\"rotor_R3\"],        # Node 6: Rotor 7 features\n",
    "        inputs[\"node_data\"][\"rotor_R4\"],        # Node 7: Rotor 8 features\n",
    "        inputs[\"node_data\"][\"canard\"],          # Node 8: Canard features\n",
    "        inputs[\"node_data\"][\"wing\"],            # Node 9: Wing features\n",
    "\n",
    "        \n",
    "    ], dim=0)  # Shape: (num_nodes, timesteps, features)\n",
    "\n",
    "\n",
    "    source = [0, 1, 2, 4, 5, 6, 8]\n",
    "    target = [1, 2, 3, 5, 6, 7, 9]\n",
    "\n",
    "    # Define edge index (connectivity)\n",
    "    edge_index = torch.tensor([source, target], dtype=torch.long)\n",
    "\n",
    "    edge_attr = (inputs[\"constant_inputs\"][torch.tensor(target)] - inputs[\"constant_inputs\"][torch.tensor(source)])\n",
    "\n",
    "    \n",
    "    global_inputs = torch.stack([inputs[\"time_varying_inputs\"]])\n",
    "    global_targets = torch.stack([targets])\n",
    "\n",
    "\n",
    "    # Create graph data object\n",
    "    graph = Data(\n",
    "        x=node_features,                # Node features\n",
    "        edge_index=edge_index,          # Edge connectivity\n",
    "        edge_attr=edge_attr,            # Edge features\n",
    "        global_input = global_inputs,   # Aircraft-level inputs\n",
    "        y=global_targets                # Targets\n",
    "    )\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class to represent the aircraft dataset as a graph dataset\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with a list of inputs and targets.\n",
    "        \n",
    "        Args:\n",
    "            dataset (list): A list of dictionaries, each containing 'inputs' and 'targets'.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, targets = self.dataset[idx]\n",
    "        graph = create_graph(inputs, targets)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "\n",
    "def visualize_graph(graph):\n",
    "    \"\"\"\n",
    "    Visualize a PyTorch Geometric graph using NetworkX.\n",
    "    \n",
    "    Args:\n",
    "        graph (torch_geometric.data.Data): The graph to visualize.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric Data object to NetworkX graph\n",
    "    nx_graph = to_networkx(graph, edge_attrs=[\"edge_attr\"], to_undirected=True)\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(nx_graph)  # Use spring layout for positioning\n",
    "    nx.draw(\n",
    "        nx_graph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=\"skyblue\",\n",
    "        node_size=500,\n",
    "        edge_color=\"gray\",\n",
    "        font_weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # Annotate edge attributes\n",
    "    edge_labels = nx.get_edge_attributes(nx_graph, \"edge_attr\")\n",
    "    edge_labels = {k: tuple(round(x, 2) for x in v) for k, v in edge_labels.items()}  # Round for readability\n",
    "    # edge_labels = {k: round(float(v), 2) for k, v in edge_labels.items()}\n",
    "    nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels, font_size=10)\n",
    "\n",
    "    plt.title(\"Graph Structure\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Composite GNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalCompositeGNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_wing, \n",
    "                 pretrained_canard,\n",
    "                 pretrained_rotor1, pretrained_rotor2,\n",
    "                 pretrained_rotor3, pretrained_rotor4,\n",
    "                 global_input_dim,      # (retained for compatibility and scaling)\n",
    "                 edge_input_dim, \n",
    "                 hidden_dim,            # dimension after the transform (and input to LSTM)\n",
    "                 output_dim,            # final output per node (should be 2)\n",
    "                 lstm_hidden_dim=None,  # hidden size for the LSTM; if None, set equal to hidden_dim\n",
    "                 gat_hidden_dim=32,     # hidden dimension for the first GAT layer\n",
    "                 heads=4,\n",
    "                 dropout=0.3):\n",
    "        super(HierarchicalCompositeGNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim    # expected to be 2\n",
    "        self.num_components = 8         # fixed (wing, canard, rotorL1, rotorR1, rotorL2, rotorR2, rotorL3, rotorR3, rotorL4, rotorR4)\n",
    "        \n",
    "        # Pre-trained models for each component\n",
    "        self.pretrained_wing   = pretrained_wing\n",
    "        self.pretrained_canard = pretrained_canard\n",
    "        self.pretrained_rotorL1 = pretrained_rotor1\n",
    "        self.pretrained_rotorL2 = pretrained_rotor2\n",
    "        self.pretrained_rotorL3 = pretrained_rotor3\n",
    "        self.pretrained_rotorL4 = pretrained_rotor4\n",
    "        # For rotor right, re-use the left rotor models (Symmetric)\n",
    "        self.pretrained_rotorR1 = pretrained_rotor1\n",
    "        self.pretrained_rotorR2 = pretrained_rotor2\n",
    "        self.pretrained_rotorR3 = pretrained_rotor3\n",
    "        self.pretrained_rotorR4 = pretrained_rotor4\n",
    "\n",
    "        # Transform the outputs of the pre-trained models to hidden_dim.\n",
    "        # (Assumes the pre-trained outputs have 2 features.)\n",
    "        self.wing_transform  = nn.Linear(2, hidden_dim)\n",
    "        self.rotor_transform = nn.Linear(2, hidden_dim)\n",
    "        \n",
    "        # Global input transform (if needed)\n",
    "        self.global_inputs_transform = nn.Linear(global_input_dim, hidden_dim)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Lower-level temporal processing: an LSTM operating on each component’s time series.\n",
    "        if lstm_hidden_dim is None:\n",
    "            lstm_hidden_dim = hidden_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "\n",
    "\n",
    "        # We have 10 components: rotorL1, rotorR1, rotorL2, rotorR2, rotorL3, rotorR3, rotorL4, rotorR4, canard, wing\n",
    "        self.num_components = 10\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size=hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "            for _ in range(self.num_components)\n",
    "        ])\n",
    "\n",
    "        # Higher-level spatial GNN.\n",
    "        # The GAT layers operate on the graph of components at each timestep.\n",
    "        self.gat1 = GATConv(lstm_hidden_dim, gat_hidden_dim, heads=heads, concat=True, edge_dim=edge_input_dim)\n",
    "        self.norm1 = nn.LayerNorm(gat_hidden_dim * heads)\n",
    "        self.gat2 = GATConv(gat_hidden_dim * heads, lstm_hidden_dim, heads=2, concat=False, edge_dim=edge_input_dim)\n",
    "        self.norm2 = nn.LayerNorm(lstm_hidden_dim)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, node_inputs, targets, edge_index, edge_attr, global_inputs, batch_size, num_nodes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_inputs: Tensor of shape [B * num_components, timesteps, input_feature_dim]\n",
    "                         (used by the pre-trained models for rotor nodes).\n",
    "            targets: Tensor of shape [B, timesteps, some_feature_dim]\n",
    "                     (used for wing and canard components).\n",
    "            edge_index: Edge connectivity for the higher-level graph (for a graph with num_components nodes).\n",
    "            edge_attr: Edge attributes for the higher-level graph.\n",
    "            global_inputs: (Not used in this model (Exists for scalibility))\n",
    "            batch_size: number of samples in the batch.\n",
    "            num_nodes: total number of nodes per sample (should equal self.num_components).\n",
    "            \n",
    "        Returns:\n",
    "            out: Tensor of shape [B, timesteps, num_components * 2],\n",
    "                 where each sample has timesteps and each timestep contains the concatenated outputs\n",
    "                 from the 10 components (each with 2 outputs).\n",
    "        \"\"\"\n",
    "        # Compute indices for each component so that they can be extracted from the node_inputs tensor.\n",
    "        batch_indices = torch.arange(batch_size).to(node_inputs.device) * num_nodes\n",
    "        wing_indices     = batch_indices + 9       # wing at index 0\n",
    "        canard_indices   = batch_indices + 8       # canard at index 1\n",
    "        \n",
    "        rotorL1_indices  = batch_indices + 0       # rotorL1 at index 2\n",
    "        rotorL2_indices  = batch_indices + 1       # rotorL2 at index 3\n",
    "        rotorL3_indices  = batch_indices + 2       # rotorL3 at index 4\n",
    "        rotorL4_indices  = batch_indices + 3       # rotorL4 at index 5\n",
    "        rotorR1_indices  = batch_indices + 4       # rotorR1 at index 6\n",
    "        rotorR2_indices  = batch_indices + 5       # rotorR2 at index 7\n",
    "        rotorR3_indices  = batch_indices + 6       # rotorR3 at index 8\n",
    "        rotorR4_indices  = batch_indices + 7       # rotorR4 at index 9\n",
    "\n",
    "        \n",
    "        # node_embeddings_wing   = self.wing_transform(targets[:, :, 18:20])  # [B, timesteps, hidden_dim]\n",
    "        # node_embeddings_canard = self.wing_transform(targets[:, :, 16:18])  # [B, timesteps, hidden_dim]\n",
    "        \n",
    "        # Process inputs through pre-trained modules and transform.\n",
    "        node_embeddings_wing = self.wing_transform(self.pretrained_wing(node_inputs[wing_indices]))\n",
    "        node_embeddings_canard = self.wing_transform(self.pretrained_canard(node_inputs[canard_indices]))\n",
    "        node_embeddings_rotorL1 = self.rotor_transform(self.pretrained_rotorL1(node_inputs[rotorL1_indices]))\n",
    "        node_embeddings_rotorL2 = self.rotor_transform(self.pretrained_rotorL2(node_inputs[rotorL2_indices]))\n",
    "        node_embeddings_rotorL3 = self.rotor_transform(self.pretrained_rotorL3(node_inputs[rotorL3_indices]))\n",
    "        node_embeddings_rotorL4 = self.rotor_transform(self.pretrained_rotorL4(node_inputs[rotorL4_indices]))\n",
    "        node_embeddings_rotorR1 = self.rotor_transform(self.pretrained_rotorR1(node_inputs[rotorR1_indices]))\n",
    "        node_embeddings_rotorR2 = self.rotor_transform(self.pretrained_rotorR2(node_inputs[rotorR2_indices]))\n",
    "        node_embeddings_rotorR3 = self.rotor_transform(self.pretrained_rotorR3(node_inputs[rotorR3_indices]))\n",
    "        node_embeddings_rotorR4 = self.rotor_transform(self.pretrained_rotorR4(node_inputs[rotorR4_indices]))\n",
    "\n",
    "        # Stack the 10 component embeddings into one tensor of shape [B, num_components, timesteps, hidden_dim].\n",
    "        components = [\n",
    "            node_embeddings_rotorL1,\n",
    "            node_embeddings_rotorR1,\n",
    "            node_embeddings_rotorL2,\n",
    "            node_embeddings_rotorR2,\n",
    "            node_embeddings_rotorL3,\n",
    "            node_embeddings_rotorR3,\n",
    "            node_embeddings_rotorL4,\n",
    "            node_embeddings_rotorR4,\n",
    "            node_embeddings_canard,  \n",
    "            node_embeddings_wing,    \n",
    "        ]\n",
    "        x = torch.stack(components, dim=1)  # x: [B, num_components, timesteps, hidden_dim]\n",
    "\n",
    "        # ---- Lower-Level: Temporal Processing via LSTM ----\n",
    "        B, N, T, H = x.shape  # (B, 10, timesteps, hidden_dim)\n",
    "\n",
    "        lstm_outputs = []\n",
    "        for i in range(N):\n",
    "            # For each component, process its time series individually.\n",
    "            x_i = x[:, i, :, :]  # shape: [B, timesteps, hidden_dim]\n",
    "            lstm_out_i, _ = self.lstm_layers[i](x_i)  # shape: [B, timesteps, lstm_hidden_dim]\n",
    "            lstm_outputs.append(lstm_out_i)\n",
    "        # Stack the outputs: [B, num_components, timesteps, lstm_hidden_dim]\n",
    "        lstm_out = torch.stack(lstm_outputs, dim=1)\n",
    "\n",
    "        # ---- Higher-Level: Spatial GNN for Each Time Slice ----\n",
    "        # For each timestep, build a graph of the 10 components.\n",
    "        # Permute so that time becomes the second dimension: [B, timesteps, num_components, lstm_hidden_dim]\n",
    "        x_time = lstm_out.permute(0, 2, 1, 3)  # [B, T, N, lstm_hidden_dim]\n",
    "        B_T = B * T\n",
    "        x_time = x_time.contiguous().view(B_T, N, -1)  # [B*T, num_components, lstm_hidden_dim]\n",
    "        # Flatten to feed into GATConv: [B*T * N, lstm_hidden_dim]\n",
    "        x_time_flat = x_time.view(B_T * N, -1)\n",
    "\n",
    "        # The provided edge_index is defined on a graph with N nodes.\n",
    "        # Replicate it for each time slice.\n",
    "        E = edge_index.shape[1]\n",
    "        device = edge_index.device\n",
    "        edge_index_expanded = []\n",
    "        for i in range(B_T):\n",
    "            offset = i * N\n",
    "            edge_index_expanded.append(edge_index + offset)\n",
    "        edge_index_expanded = torch.cat(edge_index_expanded, dim=1).to(device)\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = edge_attr.repeat(B_T, 1)\n",
    "\n",
    "        # Apply the GAT layers.\n",
    "        x_gnn = self.gat1(x_time_flat, edge_index_expanded, edge_attr)\n",
    "        x_gnn = F.relu(x_gnn)\n",
    "        x_gnn = self.dropout(x_gnn)\n",
    "        x_gnn = self.norm1(x_gnn)\n",
    "        x_gnn = self.gat2(x_gnn, edge_index_expanded, edge_attr)\n",
    "        x_gnn = F.relu(x_gnn)\n",
    "        x_gnn = self.norm2(x_gnn)\n",
    "        x_gnn = self.fc(x_gnn)  # [B*T*N, output_dim]\n",
    "\n",
    "        # ---- Reshape to the Final Output ----\n",
    "        # Reshape from [B*T*N, output_dim] to [B, T, N, output_dim]\n",
    "        x_gnn = x_gnn.view(B_T, N, -1)      # [B*T, num_components, output_dim]\n",
    "        x_gnn = x_gnn.view(B, T, N, -1)       # [B, timesteps, num_components, output_dim]\n",
    "        \n",
    "        # Finally, flatten the last two dimensions so that each timestep has shape [num_components * output_dim]\n",
    "        out = x_gnn.view(B, T, N * self.output_dim)  # [B, timesteps, num_components * 2]\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Aircraft data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "dataset_aircraft = AircraftDataset(dataset_wing,\n",
    "                                   dataset_canard,\n",
    "                                   dataset_rotor_L1,\n",
    "                                   dataset_rotor_L2,\n",
    "                                   dataset_rotor_L3,\n",
    "                                   dataset_rotor_L4,\n",
    "                                   dataset_rotor_R1,\n",
    "                                   dataset_rotor_R2,\n",
    "                                   dataset_rotor_R3,\n",
    "                                   dataset_rotor_R4,\n",
    "                                   input_scaler_wing_temp,\n",
    "                                   output_scaler_wing_temp,\n",
    "                                   input_scaler_canard_temp,\n",
    "                                   output_scaler_canard_temp,\n",
    "                                   input_scaler_rotor1, output_scaler_rotor1,\n",
    "                                   input_scaler_rotor2, output_scaler_rotor2,\n",
    "                                   input_scaler_rotor3, output_scaler_rotor3,\n",
    "                                   input_scaler_rotor4, output_scaler_rotor4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[10, 277, 10], edge_index=[2, 7], edge_attr=[7, 3], y=[1, 277, 20], global_input=[1, 277, 4])\n",
      "Targets: torch.Size([277, 20])\n",
      "Targets: torch.Size([10, 277, 2])\n"
     ]
    }
   ],
   "source": [
    "if VISUAILIZE_GRAPH:\n",
    "    inputs_temp, targets_temp = dataset_aircraft[0]\n",
    "    graph = create_graph(inputs_temp, targets_temp)\n",
    "    visualize_graph(graph)\n",
    "    print(graph)\n",
    "\n",
    "    print(\"Targets:\" , targets_temp.shape)\n",
    "    targets_temp = targets_temp.unsqueeze(0).reshape(10, -1, 2)\n",
    "    print(\"Targets:\" , targets_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 2500\n",
    "BATCH_SIZE = 1           # Batch size of 1 is used because of hardware limitations\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph dataset\n",
    "graph_dataset = GraphDataset(dataset_aircraft)\n",
    "\n",
    "# Initialize Pre-Trained Models\n",
    "pretrained_wing = wing_model_static\n",
    "pretrained_canard = wing_model_static\n",
    "pretrained_rotor1 = rotor1_model\n",
    "pretrained_rotor2 = rotor2_model\n",
    "pretrained_rotor3 = rotor3_model\n",
    "pretrained_rotor4 = rotor4_model\n",
    "\n",
    "\n",
    "# Ensure all parameters are non-trainable\n",
    "for param in pretrained_wing.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_canard.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_rotor1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_rotor2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_rotor3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in pretrained_rotor4.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 29\n",
      "Validation Dataset Size: 13\n"
     ]
    }
   ],
   "source": [
    "# Define the split ratio\n",
    "train_ratio = 0.7\n",
    "val_ratio = 1-train_ratio\n",
    "\n",
    "# Calculate split sizes\n",
    "dataset_size = len(graph_dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Perform the split\n",
    "train_dataset_aircraft, val_dataset_aircraft = random_split(graph_dataset, [train_size, val_size])\n",
    "\n",
    "print(\"Train Dataset Size:\", len(train_dataset_aircraft))\n",
    "print(\"Validation Dataset Size:\", len(val_dataset_aircraft))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_aircraft, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_aircraft, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "global_input_dim = dataset_aircraft.data[\"time_varying_inputs\"].shape[-1]\n",
    "edge_input_dim = 3 \n",
    "output_dim = 2      # Outputs per node\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = HierarchicalCompositeGNN(\n",
    "                                    pretrained_wing=pretrained_wing,\n",
    "                                    pretrained_canard=pretrained_canard,\n",
    "                                    pretrained_rotorL1=pretrained_rotor1,\n",
    "                                    pretrained_rotorL2=pretrained_rotor2,\n",
    "                                    pretrained_rotorL3=pretrained_rotor3,\n",
    "                                    pretrained_rotorL4=pretrained_rotor4,\n",
    "                                    global_input_dim=global_input_dim,\n",
    "                                    edge_input_dim=edge_input_dim,\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    output_dim=output_dim,\n",
    "                                    lstm_hidden_dim=100,\n",
    "                                    gat_hidden_dim=hidden_dim,\n",
    "                                    heads=4,\n",
    "                                    dropout=0.3\n",
    "                                ).to(device)\n",
    "\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer_composite = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n",
      "Epoch [1/2500],\n",
      " Training Loss: 0.04556412\n",
      "Evaluation Loss: 0.00182834\n",
      "Epoch [2/2500],\n",
      " Training Loss: 0.00212027\n",
      "Evaluation Loss: 0.00055022\n",
      "Epoch [3/2500],\n",
      " Training Loss: 0.00103763\n",
      "Evaluation Loss: 0.00058366\n",
      "Epoch [4/2500],\n",
      " Training Loss: 0.00048760\n",
      "Evaluation Loss: 0.00028533\n",
      "Epoch [5/2500],\n",
      " Training Loss: 0.00029967\n",
      "Evaluation Loss: 0.00011088\n",
      "Epoch [6/2500],\n",
      " Training Loss: 0.00032797\n",
      "Evaluation Loss: 0.00021426\n",
      "Epoch [7/2500],\n",
      " Training Loss: 0.00025676\n",
      "Evaluation Loss: 0.00021628\n",
      "Epoch [8/2500],\n",
      " Training Loss: 0.00016803\n",
      "Evaluation Loss: 0.00050154\n",
      "Epoch [9/2500],\n",
      " Training Loss: 0.00027423\n",
      "Evaluation Loss: 0.00013888\n",
      "Epoch [10/2500],\n",
      " Training Loss: 0.00011762\n",
      "Evaluation Loss: 0.00015498\n",
      "Epoch [11/2500],\n",
      " Training Loss: 0.00040637\n",
      "Evaluation Loss: 0.00014685\n",
      "Epoch [12/2500],\n",
      " Training Loss: 0.00011889\n",
      "Evaluation Loss: 0.00009709\n",
      "Epoch [13/2500],\n",
      " Training Loss: 0.00032877\n",
      "Evaluation Loss: 0.00030256\n",
      "Epoch [14/2500],\n",
      " Training Loss: 0.00023621\n",
      "Evaluation Loss: 0.00027050\n",
      "Epoch [15/2500],\n",
      " Training Loss: 0.00031475\n",
      "Evaluation Loss: 0.00013442\n",
      "Epoch [16/2500],\n",
      " Training Loss: 0.00030122\n",
      "Evaluation Loss: 0.00018540\n",
      "Epoch [17/2500],\n",
      " Training Loss: 0.00020713\n",
      "Evaluation Loss: 0.00008124\n",
      "Epoch [18/2500],\n",
      " Training Loss: 0.00028876\n",
      "Evaluation Loss: 0.00014923\n",
      "Epoch [19/2500],\n",
      " Training Loss: 0.00024406\n",
      "Evaluation Loss: 0.00009148\n",
      "Epoch [20/2500],\n",
      " Training Loss: 0.00032797\n",
      "Evaluation Loss: 0.00029342\n",
      "Epoch [21/2500],\n",
      " Training Loss: 0.00030921\n",
      "Evaluation Loss: 0.00012554\n",
      "Epoch [22/2500],\n",
      " Training Loss: 0.00104695\n",
      "Evaluation Loss: 0.00073903\n",
      "Epoch [23/2500],\n",
      " Training Loss: 0.00045886\n",
      "Evaluation Loss: 0.00025554\n",
      "Epoch [24/2500],\n",
      " Training Loss: 0.00021041\n",
      "Evaluation Loss: 0.00020268\n",
      "Epoch [25/2500],\n",
      " Training Loss: 0.00031881\n",
      "Evaluation Loss: 0.00022749\n",
      "Epoch [26/2500],\n",
      " Training Loss: 0.00029221\n",
      "Evaluation Loss: 0.00028494\n",
      "Epoch [27/2500],\n",
      " Training Loss: 0.00014046\n",
      "Evaluation Loss: 0.00003446\n",
      "Epoch [28/2500],\n",
      " Training Loss: 0.00009463\n",
      "Evaluation Loss: 0.00004322\n",
      "Epoch [29/2500],\n",
      " Training Loss: 0.00006762\n",
      "Evaluation Loss: 0.00007701\n",
      "Epoch [30/2500],\n",
      " Training Loss: 0.00009993\n",
      "Evaluation Loss: 0.00007838\n",
      "Epoch [31/2500],\n",
      " Training Loss: 0.00007819\n",
      "Evaluation Loss: 0.00004336\n",
      "Epoch [32/2500],\n",
      " Training Loss: 0.00004161\n",
      "Evaluation Loss: 0.00002357\n",
      "Epoch [33/2500],\n",
      " Training Loss: 0.00004474\n",
      "Evaluation Loss: 0.00005539\n",
      "Epoch [34/2500],\n",
      " Training Loss: 0.00004882\n",
      "Evaluation Loss: 0.00003281\n",
      "Epoch [35/2500],\n",
      " Training Loss: 0.00005869\n",
      "Evaluation Loss: 0.00003922\n",
      "Epoch [36/2500],\n",
      " Training Loss: 0.00017629\n",
      "Evaluation Loss: 0.00017778\n",
      "Epoch [37/2500],\n",
      " Training Loss: 0.00061016\n",
      "Evaluation Loss: 0.00056977\n",
      "Epoch [38/2500],\n",
      " Training Loss: 0.00026538\n",
      "Evaluation Loss: 0.00005845\n",
      "Epoch [39/2500],\n",
      " Training Loss: 0.00015751\n",
      "Evaluation Loss: 0.00025248\n",
      "Epoch [40/2500],\n",
      " Training Loss: 0.00015178\n",
      "Evaluation Loss: 0.00027404\n",
      "Epoch [41/2500],\n",
      " Training Loss: 0.00015176\n",
      "Evaluation Loss: 0.00012053\n",
      "Epoch [42/2500],\n",
      " Training Loss: 0.00005096\n",
      "Evaluation Loss: 0.00003421\n",
      "Epoch [43/2500],\n",
      " Training Loss: 0.00005408\n",
      "Evaluation Loss: 0.00002227\n",
      "Epoch [44/2500],\n",
      " Training Loss: 0.00007523\n",
      "Evaluation Loss: 0.00002588\n",
      "Epoch [45/2500],\n",
      " Training Loss: 0.00018234\n",
      "Evaluation Loss: 0.00002174\n",
      "Epoch [46/2500],\n",
      " Training Loss: 0.00016226\n",
      "Evaluation Loss: 0.00018629\n",
      "Epoch [47/2500],\n",
      " Training Loss: 0.00033594\n",
      "Evaluation Loss: 0.00019455\n",
      "Epoch [48/2500],\n",
      " Training Loss: 0.00016138\n",
      "Evaluation Loss: 0.00004617\n",
      "Epoch [49/2500],\n",
      " Training Loss: 0.00010920\n",
      "Evaluation Loss: 0.00021410\n",
      "Epoch [50/2500],\n",
      " Training Loss: 0.00012507\n",
      "Evaluation Loss: 0.00006663\n",
      "Epoch [51/2500],\n",
      " Training Loss: 0.00009566\n",
      "Evaluation Loss: 0.00002896\n",
      "Epoch [52/2500],\n",
      " Training Loss: 0.00004716\n",
      "Evaluation Loss: 0.00010574\n",
      "Epoch [53/2500],\n",
      " Training Loss: 0.00004806\n",
      "Evaluation Loss: 0.00002440\n",
      "Epoch [54/2500],\n",
      " Training Loss: 0.00003446\n",
      "Evaluation Loss: 0.00003873\n",
      "Epoch [55/2500],\n",
      " Training Loss: 0.00004060\n",
      "Evaluation Loss: 0.00003514\n",
      "Epoch [56/2500],\n",
      " Training Loss: 0.00003642\n",
      "Evaluation Loss: 0.00005803\n",
      "Epoch [57/2500],\n",
      " Training Loss: 0.00006455\n",
      "Evaluation Loss: 0.00028698\n",
      "Epoch [58/2500],\n",
      " Training Loss: 0.00009249\n",
      "Evaluation Loss: 0.00010938\n",
      "Epoch [59/2500],\n",
      " Training Loss: 0.00007721\n",
      "Evaluation Loss: 0.00009620\n",
      "Epoch [60/2500],\n",
      " Training Loss: 0.00020478\n",
      "Evaluation Loss: 0.00007560\n",
      "Epoch [61/2500],\n",
      " Training Loss: 0.00008872\n",
      "Evaluation Loss: 0.00011789\n",
      "Epoch [62/2500],\n",
      " Training Loss: 0.00006832\n",
      "Evaluation Loss: 0.00001550\n",
      "Epoch [63/2500],\n",
      " Training Loss: 0.00004754\n",
      "Evaluation Loss: 0.00003681\n",
      "Epoch [64/2500],\n",
      " Training Loss: 0.00004318\n",
      "Evaluation Loss: 0.00002376\n",
      "Epoch [65/2500],\n",
      " Training Loss: 0.00002564\n",
      "Evaluation Loss: 0.00001296\n",
      "Epoch [66/2500],\n",
      " Training Loss: 0.00001673\n",
      "Evaluation Loss: 0.00000809\n",
      "Epoch [67/2500],\n",
      " Training Loss: 0.00002266\n",
      "Evaluation Loss: 0.00000983\n",
      "Epoch [68/2500],\n",
      " Training Loss: 0.00002533\n",
      "Evaluation Loss: 0.00001170\n",
      "Epoch [69/2500],\n",
      " Training Loss: 0.00002095\n",
      "Evaluation Loss: 0.00002637\n",
      "Epoch [70/2500],\n",
      " Training Loss: 0.00003360\n",
      "Evaluation Loss: 0.00003205\n",
      "Epoch [71/2500],\n",
      " Training Loss: 0.00002647\n",
      "Evaluation Loss: 0.00007998\n",
      "Epoch [72/2500],\n",
      " Training Loss: 0.00003946\n",
      "Evaluation Loss: 0.00005267\n",
      "Epoch [73/2500],\n",
      " Training Loss: 0.00015033\n",
      "Evaluation Loss: 0.00007132\n",
      "Epoch [74/2500],\n",
      " Training Loss: 0.00013761\n",
      "Evaluation Loss: 0.00042180\n",
      "Epoch [75/2500],\n",
      " Training Loss: 0.00041277\n",
      "Evaluation Loss: 0.00005717\n",
      "Epoch [76/2500],\n",
      " Training Loss: 0.00031184\n",
      "Evaluation Loss: 0.00130893\n",
      "Epoch [77/2500],\n",
      " Training Loss: 0.00085026\n",
      "Evaluation Loss: 0.00032979\n",
      "Epoch [78/2500],\n",
      " Training Loss: 0.00034058\n",
      "Evaluation Loss: 0.00025195\n",
      "Epoch [79/2500],\n",
      " Training Loss: 0.00009918\n",
      "Evaluation Loss: 0.00003647\n",
      "Epoch [80/2500],\n",
      " Training Loss: 0.00009159\n",
      "Evaluation Loss: 0.00013506\n",
      "Epoch [81/2500],\n",
      " Training Loss: 0.00017086\n",
      "Evaluation Loss: 0.00004415\n",
      "Epoch [82/2500],\n",
      " Training Loss: 0.00010031\n",
      "Evaluation Loss: 0.00008239\n",
      "Epoch [83/2500],\n",
      " Training Loss: 0.00019921\n",
      "Evaluation Loss: 0.00021340\n",
      "Epoch [84/2500],\n",
      " Training Loss: 0.00042680\n",
      "Evaluation Loss: 0.00022969\n",
      "Epoch [85/2500],\n",
      " Training Loss: 0.00043011\n",
      "Evaluation Loss: 0.00039058\n",
      "Epoch [86/2500],\n",
      " Training Loss: 0.00025915\n",
      "Evaluation Loss: 0.00023311\n",
      "Epoch [87/2500],\n",
      " Training Loss: 0.00007617\n",
      "Evaluation Loss: 0.00002936\n",
      "Epoch [88/2500],\n",
      " Training Loss: 0.00005246\n",
      "Evaluation Loss: 0.00003146\n",
      "Epoch [89/2500],\n",
      " Training Loss: 0.00003982\n",
      "Evaluation Loss: 0.00002754\n",
      "Epoch [90/2500],\n",
      " Training Loss: 0.00005334\n",
      "Evaluation Loss: 0.00008346\n",
      "Epoch [91/2500],\n",
      " Training Loss: 0.00013410\n",
      "Evaluation Loss: 0.00015876\n",
      "Epoch [92/2500],\n",
      " Training Loss: 0.00005606\n",
      "Evaluation Loss: 0.00004352\n",
      "Epoch [93/2500],\n",
      " Training Loss: 0.00003776\n",
      "Evaluation Loss: 0.00003566\n",
      "Epoch [94/2500],\n",
      " Training Loss: 0.00002517\n",
      "Evaluation Loss: 0.00001157\n",
      "Epoch [95/2500],\n",
      " Training Loss: 0.00001547\n",
      "Evaluation Loss: 0.00001894\n",
      "Epoch [96/2500],\n",
      " Training Loss: 0.00002696\n",
      "Evaluation Loss: 0.00001718\n",
      "Epoch [97/2500],\n",
      " Training Loss: 0.00001988\n",
      "Evaluation Loss: 0.00001359\n",
      "Epoch [98/2500],\n",
      " Training Loss: 0.00003953\n",
      "Evaluation Loss: 0.00002074\n",
      "Epoch [99/2500],\n",
      " Training Loss: 0.00006301\n",
      "Evaluation Loss: 0.00007301\n",
      "Epoch [100/2500],\n",
      " Training Loss: 0.00015556\n",
      "Evaluation Loss: 0.00014798\n",
      "Epoch [101/2500],\n",
      " Training Loss: 0.00042732\n",
      "Evaluation Loss: 0.00043803\n",
      "Epoch [102/2500],\n",
      " Training Loss: 0.00035201\n",
      "Evaluation Loss: 0.00006945\n",
      "Epoch [103/2500],\n",
      " Training Loss: 0.00013989\n",
      "Evaluation Loss: 0.00015447\n",
      "Epoch [104/2500],\n",
      " Training Loss: 0.00010489\n",
      "Evaluation Loss: 0.00005350\n",
      "Epoch [105/2500],\n",
      " Training Loss: 0.00004487\n",
      "Evaluation Loss: 0.00003448\n",
      "Epoch [106/2500],\n",
      " Training Loss: 0.00005665\n",
      "Evaluation Loss: 0.00004095\n",
      "Epoch [107/2500],\n",
      " Training Loss: 0.00006068\n",
      "Evaluation Loss: 0.00003397\n",
      "Epoch [108/2500],\n",
      " Training Loss: 0.00003719\n",
      "Evaluation Loss: 0.00001597\n",
      "Epoch [109/2500],\n",
      " Training Loss: 0.00005534\n",
      "Evaluation Loss: 0.00016918\n",
      "Epoch [110/2500],\n",
      " Training Loss: 0.00005710\n",
      "Evaluation Loss: 0.00001553\n",
      "Epoch [111/2500],\n",
      " Training Loss: 0.00003908\n",
      "Evaluation Loss: 0.00007622\n",
      "Epoch [112/2500],\n",
      " Training Loss: 0.00005055\n",
      "Evaluation Loss: 0.00007800\n",
      "Epoch [113/2500],\n",
      " Training Loss: 0.00006642\n",
      "Evaluation Loss: 0.00003182\n",
      "Epoch [114/2500],\n",
      " Training Loss: 0.00005933\n",
      "Evaluation Loss: 0.00001364\n",
      "Epoch [115/2500],\n",
      " Training Loss: 0.00002197\n",
      "Evaluation Loss: 0.00002350\n",
      "Epoch [116/2500],\n",
      " Training Loss: 0.00002388\n",
      "Evaluation Loss: 0.00002588\n",
      "Epoch [117/2500],\n",
      " Training Loss: 0.00002336\n",
      "Evaluation Loss: 0.00001637\n",
      "Epoch [118/2500],\n",
      " Training Loss: 0.00002275\n",
      "Evaluation Loss: 0.00002600\n",
      "Epoch [119/2500],\n",
      " Training Loss: 0.00002813\n",
      "Evaluation Loss: 0.00003200\n",
      "Epoch [120/2500],\n",
      " Training Loss: 0.00004474\n",
      "Evaluation Loss: 0.00001092\n",
      "Epoch [121/2500],\n",
      " Training Loss: 0.00003219\n",
      "Evaluation Loss: 0.00002194\n",
      "Epoch [122/2500],\n",
      " Training Loss: 0.00004093\n",
      "Evaluation Loss: 0.00001975\n",
      "Epoch [123/2500],\n",
      " Training Loss: 0.00007973\n",
      "Evaluation Loss: 0.00003738\n",
      "Epoch [124/2500],\n",
      " Training Loss: 0.00005795\n",
      "Evaluation Loss: 0.00003999\n",
      "Epoch [125/2500],\n",
      " Training Loss: 0.00003607\n",
      "Evaluation Loss: 0.00002132\n",
      "Epoch [126/2500],\n",
      " Training Loss: 0.00002611\n",
      "Evaluation Loss: 0.00002547\n",
      "Epoch [127/2500],\n",
      " Training Loss: 0.00002091\n",
      "Evaluation Loss: 0.00001367\n",
      "Epoch [128/2500],\n",
      " Training Loss: 0.00001754\n",
      "Evaluation Loss: 0.00001322\n",
      "Epoch [129/2500],\n",
      " Training Loss: 0.00007446\n",
      "Evaluation Loss: 0.00003491\n",
      "Epoch [130/2500],\n",
      " Training Loss: 0.00013754\n",
      "Evaluation Loss: 0.00013688\n",
      "Epoch [131/2500],\n",
      " Training Loss: 0.00016021\n",
      "Evaluation Loss: 0.00012498\n",
      "Epoch [132/2500],\n",
      " Training Loss: 0.00017579\n",
      "Evaluation Loss: 0.00007970\n",
      "Epoch [133/2500],\n",
      " Training Loss: 0.00013195\n",
      "Evaluation Loss: 0.00009740\n",
      "Epoch [134/2500],\n",
      " Training Loss: 0.00037335\n",
      "Evaluation Loss: 0.00096990\n",
      "Epoch [135/2500],\n",
      " Training Loss: 0.00036505\n",
      "Evaluation Loss: 0.00016068\n",
      "Epoch [136/2500],\n",
      " Training Loss: 0.00041991\n",
      "Evaluation Loss: 0.00029036\n",
      "Epoch [137/2500],\n",
      " Training Loss: 0.00036748\n",
      "Evaluation Loss: 0.00010061\n",
      "Epoch [138/2500],\n",
      " Training Loss: 0.00011986\n",
      "Evaluation Loss: 0.00009675\n",
      "Epoch [139/2500],\n",
      " Training Loss: 0.00017937\n",
      "Evaluation Loss: 0.00013620\n",
      "Epoch [140/2500],\n",
      " Training Loss: 0.00031923\n",
      "Evaluation Loss: 0.00052968\n",
      "Epoch [141/2500],\n",
      " Training Loss: 0.00024073\n",
      "Evaluation Loss: 0.00022015\n",
      "Epoch [142/2500],\n",
      " Training Loss: 0.00016999\n",
      "Evaluation Loss: 0.00012620\n",
      "Epoch [143/2500],\n",
      " Training Loss: 0.00008995\n",
      "Evaluation Loss: 0.00002242\n",
      "Epoch [144/2500],\n",
      " Training Loss: 0.00015117\n",
      "Evaluation Loss: 0.00002574\n",
      "Epoch [145/2500],\n",
      " Training Loss: 0.00005207\n",
      "Evaluation Loss: 0.00002318\n",
      "Epoch [146/2500],\n",
      " Training Loss: 0.00004657\n",
      "Evaluation Loss: 0.00002781\n",
      "Epoch [147/2500],\n",
      " Training Loss: 0.00013850\n",
      "Evaluation Loss: 0.00005792\n",
      "Epoch [148/2500],\n",
      " Training Loss: 0.00003984\n",
      "Evaluation Loss: 0.00005814\n",
      "Epoch [149/2500],\n",
      " Training Loss: 0.00003157\n",
      "Evaluation Loss: 0.00003630\n",
      "Epoch [150/2500],\n",
      " Training Loss: 0.00003306\n",
      "Evaluation Loss: 0.00003071\n",
      "Epoch [151/2500],\n",
      " Training Loss: 0.00002845\n",
      "Evaluation Loss: 0.00002616\n",
      "Epoch [152/2500],\n",
      " Training Loss: 0.00003104\n",
      "Evaluation Loss: 0.00001103\n",
      "Epoch [153/2500],\n",
      " Training Loss: 0.00003047\n",
      "Evaluation Loss: 0.00005086\n",
      "Epoch [154/2500],\n",
      " Training Loss: 0.00014903\n",
      "Evaluation Loss: 0.00013612\n",
      "Epoch [155/2500],\n",
      " Training Loss: 0.00041193\n",
      "Evaluation Loss: 0.00005849\n",
      "Epoch [156/2500],\n",
      " Training Loss: 0.00016279\n",
      "Evaluation Loss: 0.00010280\n",
      "Epoch [157/2500],\n",
      " Training Loss: 0.00020034\n",
      "Evaluation Loss: 0.00004771\n",
      "Epoch [158/2500],\n",
      " Training Loss: 0.00006278\n",
      "Evaluation Loss: 0.00005046\n",
      "Epoch [159/2500],\n",
      " Training Loss: 0.00004310\n",
      "Evaluation Loss: 0.00000914\n",
      "Epoch [160/2500],\n",
      " Training Loss: 0.00003018\n",
      "Evaluation Loss: 0.00002236\n",
      "Epoch [161/2500],\n",
      " Training Loss: 0.00002646\n",
      "Evaluation Loss: 0.00001443\n",
      "Epoch [162/2500],\n",
      " Training Loss: 0.00002743\n",
      "Evaluation Loss: 0.00009174\n",
      "Epoch [163/2500],\n",
      " Training Loss: 0.00003869\n",
      "Evaluation Loss: 0.00002573\n",
      "Epoch [164/2500],\n",
      " Training Loss: 0.00004739\n",
      "Evaluation Loss: 0.00008539\n",
      "Epoch [165/2500],\n",
      " Training Loss: 0.00003407\n",
      "Evaluation Loss: 0.00001391\n",
      "Epoch [166/2500],\n",
      " Training Loss: 0.00008331\n",
      "Evaluation Loss: 0.00006400\n",
      "Epoch [167/2500],\n",
      " Training Loss: 0.00003254\n",
      "Evaluation Loss: 0.00001214\n",
      "Epoch [168/2500],\n",
      " Training Loss: 0.00004029\n",
      "Evaluation Loss: 0.00003231\n",
      "Epoch [169/2500],\n",
      " Training Loss: 0.00024536\n",
      "Evaluation Loss: 0.00007204\n",
      "Epoch [170/2500],\n",
      " Training Loss: 0.00021157\n",
      "Evaluation Loss: 0.00005797\n",
      "Epoch [171/2500],\n",
      " Training Loss: 0.00021899\n",
      "Evaluation Loss: 0.00132212\n",
      "Epoch [172/2500],\n",
      " Training Loss: 0.00034469\n",
      "Evaluation Loss: 0.00010369\n",
      "Epoch [173/2500],\n",
      " Training Loss: 0.00013383\n",
      "Evaluation Loss: 0.00005135\n",
      "Epoch [174/2500],\n",
      " Training Loss: 0.00025148\n",
      "Evaluation Loss: 0.00004628\n",
      "Epoch [175/2500],\n",
      " Training Loss: 0.00005428\n",
      "Evaluation Loss: 0.00008091\n",
      "Epoch [176/2500],\n",
      " Training Loss: 0.00003819\n",
      "Evaluation Loss: 0.00002621\n",
      "Epoch [177/2500],\n",
      " Training Loss: 0.00002025\n",
      "Evaluation Loss: 0.00001272\n",
      "Epoch [178/2500],\n",
      " Training Loss: 0.00002536\n",
      "Evaluation Loss: 0.00002001\n",
      "Epoch [179/2500],\n",
      " Training Loss: 0.00002646\n",
      "Evaluation Loss: 0.00001930\n",
      "Epoch [180/2500],\n",
      " Training Loss: 0.00002827\n",
      "Evaluation Loss: 0.00000936\n",
      "Epoch [181/2500],\n",
      " Training Loss: 0.00002613\n",
      "Evaluation Loss: 0.00001541\n",
      "Epoch [182/2500],\n",
      " Training Loss: 0.00001439\n",
      "Evaluation Loss: 0.00000941\n",
      "Epoch [183/2500],\n",
      " Training Loss: 0.00001878\n",
      "Evaluation Loss: 0.00001683\n",
      "Epoch [184/2500],\n",
      " Training Loss: 0.00002697\n",
      "Evaluation Loss: 0.00001496\n",
      "Epoch [185/2500],\n",
      " Training Loss: 0.00003300\n",
      "Evaluation Loss: 0.00001199\n",
      "Epoch [186/2500],\n",
      " Training Loss: 0.00003301\n",
      "Evaluation Loss: 0.00001799\n",
      "Epoch [187/2500],\n",
      " Training Loss: 0.00002796\n",
      "Evaluation Loss: 0.00002679\n",
      "Epoch [188/2500],\n",
      " Training Loss: 0.00003060\n",
      "Evaluation Loss: 0.00001270\n",
      "Epoch [189/2500],\n",
      " Training Loss: 0.00003611\n",
      "Evaluation Loss: 0.00001303\n",
      "Epoch [190/2500],\n",
      " Training Loss: 0.00003517\n",
      "Evaluation Loss: 0.00003790\n",
      "Epoch [191/2500],\n",
      " Training Loss: 0.00003090\n",
      "Evaluation Loss: 0.00005518\n",
      "Epoch [192/2500],\n",
      " Training Loss: 0.00009891\n",
      "Evaluation Loss: 0.00030219\n",
      "Epoch [193/2500],\n",
      " Training Loss: 0.00019349\n",
      "Evaluation Loss: 0.00047587\n",
      "Epoch [194/2500],\n",
      " Training Loss: 0.00050070\n",
      "Evaluation Loss: 0.00010121\n",
      "Epoch [195/2500],\n",
      " Training Loss: 0.00033032\n",
      "Evaluation Loss: 0.00059393\n",
      "Epoch [196/2500],\n",
      " Training Loss: 0.00029493\n",
      "Evaluation Loss: 0.00005182\n",
      "Epoch [197/2500],\n",
      " Training Loss: 0.00006497\n",
      "Evaluation Loss: 0.00004592\n",
      "Epoch [198/2500],\n",
      " Training Loss: 0.00005215\n",
      "Evaluation Loss: 0.00004974\n",
      "Epoch [199/2500],\n",
      " Training Loss: 0.00011651\n",
      "Evaluation Loss: 0.00009822\n",
      "Epoch [200/2500],\n",
      " Training Loss: 0.00019321\n",
      "Evaluation Loss: 0.00010582\n",
      "Epoch [201/2500],\n",
      " Training Loss: 0.00004105\n",
      "Evaluation Loss: 0.00003597\n",
      "Epoch [202/2500],\n",
      " Training Loss: 0.00005836\n",
      "Evaluation Loss: 0.00004168\n",
      "Epoch [203/2500],\n",
      " Training Loss: 0.00003430\n",
      "Evaluation Loss: 0.00002359\n",
      "Epoch [204/2500],\n",
      " Training Loss: 0.00001998\n",
      "Evaluation Loss: 0.00001032\n",
      "Epoch [205/2500],\n",
      " Training Loss: 0.00001262\n",
      "Evaluation Loss: 0.00000945\n",
      "Epoch [206/2500],\n",
      " Training Loss: 0.00001504\n",
      "Evaluation Loss: 0.00001072\n",
      "Epoch [207/2500],\n",
      " Training Loss: 0.00001401\n",
      "Evaluation Loss: 0.00001722\n",
      "Epoch [208/2500],\n",
      " Training Loss: 0.00001743\n",
      "Evaluation Loss: 0.00001964\n",
      "Epoch [209/2500],\n",
      " Training Loss: 0.00004003\n",
      "Evaluation Loss: 0.00007647\n",
      "Epoch [210/2500],\n",
      " Training Loss: 0.00006185\n",
      "Evaluation Loss: 0.00010139\n",
      "Epoch [211/2500],\n",
      " Training Loss: 0.00004936\n",
      "Evaluation Loss: 0.00005122\n",
      "Epoch [212/2500],\n",
      " Training Loss: 0.00003055\n",
      "Evaluation Loss: 0.00000788\n",
      "Epoch [213/2500],\n",
      " Training Loss: 0.00001401\n",
      "Evaluation Loss: 0.00001461\n",
      "Epoch [214/2500],\n",
      " Training Loss: 0.00001647\n",
      "Evaluation Loss: 0.00001181\n",
      "Epoch [215/2500],\n",
      " Training Loss: 0.00002261\n",
      "Evaluation Loss: 0.00002754\n",
      "Epoch [216/2500],\n",
      " Training Loss: 0.00001913\n",
      "Evaluation Loss: 0.00001299\n",
      "Epoch [217/2500],\n",
      " Training Loss: 0.00001872\n",
      "Evaluation Loss: 0.00007585\n",
      "Epoch [218/2500],\n",
      " Training Loss: 0.00001936\n",
      "Evaluation Loss: 0.00000627\n",
      "Epoch [219/2500],\n",
      " Training Loss: 0.00001190\n",
      "Evaluation Loss: 0.00001686\n",
      "Epoch [220/2500],\n",
      " Training Loss: 0.00001977\n",
      "Evaluation Loss: 0.00001249\n",
      "Epoch [221/2500],\n",
      " Training Loss: 0.00004967\n",
      "Evaluation Loss: 0.00013338\n",
      "Epoch [222/2500],\n",
      " Training Loss: 0.00005406\n",
      "Evaluation Loss: 0.00001280\n",
      "Epoch [223/2500],\n",
      " Training Loss: 0.00006890\n",
      "Evaluation Loss: 0.00012289\n",
      "Epoch [224/2500],\n",
      " Training Loss: 0.00006009\n",
      "Evaluation Loss: 0.00008326\n",
      "Epoch [225/2500],\n",
      " Training Loss: 0.00005795\n",
      "Evaluation Loss: 0.00003193\n",
      "Epoch [226/2500],\n",
      " Training Loss: 0.00004729\n",
      "Evaluation Loss: 0.00006081\n",
      "Epoch [227/2500],\n",
      " Training Loss: 0.00011730\n",
      "Evaluation Loss: 0.00004583\n",
      "Epoch [228/2500],\n",
      " Training Loss: 0.00010520\n",
      "Evaluation Loss: 0.00005349\n",
      "Epoch [229/2500],\n",
      " Training Loss: 0.00005208\n",
      "Evaluation Loss: 0.00005340\n",
      "Epoch [230/2500],\n",
      " Training Loss: 0.00006072\n",
      "Evaluation Loss: 0.00011044\n",
      "Epoch [231/2500],\n",
      " Training Loss: 0.00005872\n",
      "Evaluation Loss: 0.00005345\n",
      "Epoch [232/2500],\n",
      " Training Loss: 0.00002856\n",
      "Evaluation Loss: 0.00001188\n",
      "Epoch [233/2500],\n",
      " Training Loss: 0.00001237\n",
      "Evaluation Loss: 0.00000940\n",
      "Epoch [234/2500],\n",
      " Training Loss: 0.00001380\n",
      "Evaluation Loss: 0.00002382\n",
      "Epoch [235/2500],\n",
      " Training Loss: 0.00014644\n",
      "Evaluation Loss: 0.00037682\n",
      "Epoch [236/2500],\n",
      " Training Loss: 0.00037851\n",
      "Evaluation Loss: 0.00017487\n",
      "Epoch [237/2500],\n",
      " Training Loss: 0.00018571\n",
      "Evaluation Loss: 0.00057479\n",
      "Epoch [238/2500],\n",
      " Training Loss: 0.00023846\n",
      "Evaluation Loss: 0.00011426\n",
      "Epoch [239/2500],\n",
      " Training Loss: 0.00030274\n",
      "Evaluation Loss: 0.00004090\n",
      "Epoch [240/2500],\n",
      " Training Loss: 0.00049465\n",
      "Evaluation Loss: 0.00012681\n",
      "Epoch [241/2500],\n",
      " Training Loss: 0.00023449\n",
      "Evaluation Loss: 0.00004593\n",
      "Epoch [242/2500],\n",
      " Training Loss: 0.00021990\n",
      "Evaluation Loss: 0.00013497\n",
      "Epoch [243/2500],\n",
      " Training Loss: 0.00012304\n",
      "Evaluation Loss: 0.00002137\n",
      "Epoch [244/2500],\n",
      " Training Loss: 0.00004797\n",
      "Evaluation Loss: 0.00003321\n",
      "Epoch [245/2500],\n",
      " Training Loss: 0.00004304\n",
      "Evaluation Loss: 0.00003375\n",
      "Epoch [246/2500],\n",
      " Training Loss: 0.00002981\n",
      "Evaluation Loss: 0.00005902\n",
      "Epoch [247/2500],\n",
      " Training Loss: 0.00003086\n",
      "Evaluation Loss: 0.00004038\n",
      "Epoch [248/2500],\n",
      " Training Loss: 0.00004816\n",
      "Evaluation Loss: 0.00001840\n",
      "Epoch [249/2500],\n",
      " Training Loss: 0.00004017\n",
      "Evaluation Loss: 0.00003039\n",
      "Epoch [250/2500],\n",
      " Training Loss: 0.00002204\n",
      "Evaluation Loss: 0.00001075\n",
      "Epoch [251/2500],\n",
      " Training Loss: 0.00001276\n",
      "Evaluation Loss: 0.00001918\n",
      "Epoch [252/2500],\n",
      " Training Loss: 0.00002026\n",
      "Evaluation Loss: 0.00002507\n",
      "Epoch [253/2500],\n",
      " Training Loss: 0.00002711\n",
      "Evaluation Loss: 0.00001900\n",
      "Epoch [254/2500],\n",
      " Training Loss: 0.00003302\n",
      "Evaluation Loss: 0.00001643\n",
      "Epoch [255/2500],\n",
      " Training Loss: 0.00004992\n",
      "Evaluation Loss: 0.00003578\n",
      "Epoch [256/2500],\n",
      " Training Loss: 0.00002524\n",
      "Evaluation Loss: 0.00001689\n",
      "Epoch [257/2500],\n",
      " Training Loss: 0.00002259\n",
      "Evaluation Loss: 0.00000971\n",
      "Epoch [258/2500],\n",
      " Training Loss: 0.00006116\n",
      "Evaluation Loss: 0.00009947\n",
      "Epoch [259/2500],\n",
      " Training Loss: 0.00008049\n",
      "Evaluation Loss: 0.00003876\n",
      "Epoch [260/2500],\n",
      " Training Loss: 0.00008860\n",
      "Evaluation Loss: 0.00029387\n",
      "Epoch [261/2500],\n",
      " Training Loss: 0.00009023\n",
      "Evaluation Loss: 0.00005634\n",
      "Epoch [262/2500],\n",
      " Training Loss: 0.00005437\n",
      "Evaluation Loss: 0.00022234\n",
      "Epoch [263/2500],\n",
      " Training Loss: 0.00017889\n",
      "Evaluation Loss: 0.00024912\n",
      "Epoch [264/2500],\n",
      " Training Loss: 0.00021186\n",
      "Evaluation Loss: 0.00005112\n",
      "Epoch [265/2500],\n",
      " Training Loss: 0.00009828\n",
      "Evaluation Loss: 0.00005772\n",
      "Epoch [266/2500],\n",
      " Training Loss: 0.00008332\n",
      "Evaluation Loss: 0.00011865\n",
      "Epoch [267/2500],\n",
      " Training Loss: 0.00007308\n",
      "Evaluation Loss: 0.00004317\n",
      "Epoch [268/2500],\n",
      " Training Loss: 0.00006334\n",
      "Evaluation Loss: 0.00009404\n",
      "Epoch [269/2500],\n",
      " Training Loss: 0.00004887\n",
      "Evaluation Loss: 0.00004259\n",
      "Epoch [270/2500],\n",
      " Training Loss: 0.00003990\n",
      "Evaluation Loss: 0.00002364\n",
      "Epoch [271/2500],\n",
      " Training Loss: 0.00003187\n",
      "Evaluation Loss: 0.00006286\n",
      "Epoch [272/2500],\n",
      " Training Loss: 0.00005933\n",
      "Evaluation Loss: 0.00003144\n",
      "Epoch [273/2500],\n",
      " Training Loss: 0.00005331\n",
      "Evaluation Loss: 0.00003805\n",
      "Epoch [274/2500],\n",
      " Training Loss: 0.00005771\n",
      "Evaluation Loss: 0.00003463\n",
      "Epoch [275/2500],\n",
      " Training Loss: 0.00005912\n",
      "Evaluation Loss: 0.00002017\n",
      "Epoch [276/2500],\n",
      " Training Loss: 0.00006633\n",
      "Evaluation Loss: 0.00003843\n",
      "Epoch [277/2500],\n",
      " Training Loss: 0.00007643\n",
      "Evaluation Loss: 0.00016969\n",
      "Epoch [278/2500],\n",
      " Training Loss: 0.00008499\n",
      "Evaluation Loss: 0.00002617\n",
      "Epoch [279/2500],\n",
      " Training Loss: 0.00003441\n",
      "Evaluation Loss: 0.00001202\n",
      "Epoch [280/2500],\n",
      " Training Loss: 0.00002982\n",
      "Evaluation Loss: 0.00001654\n",
      "Epoch [281/2500],\n",
      " Training Loss: 0.00002743\n",
      "Evaluation Loss: 0.00002662\n",
      "Epoch [282/2500],\n",
      " Training Loss: 0.00007907\n",
      "Evaluation Loss: 0.00004929\n",
      "Epoch [283/2500],\n",
      " Training Loss: 0.00039680\n",
      "Evaluation Loss: 0.00021086\n",
      "Epoch [284/2500],\n",
      " Training Loss: 0.00019687\n",
      "Evaluation Loss: 0.00006173\n",
      "Epoch [285/2500],\n",
      " Training Loss: 0.00005654\n",
      "Evaluation Loss: 0.00005579\n",
      "Epoch [286/2500],\n",
      " Training Loss: 0.00006358\n",
      "Evaluation Loss: 0.00002817\n",
      "Epoch [287/2500],\n",
      " Training Loss: 0.00004946\n",
      "Evaluation Loss: 0.00004352\n",
      "Epoch [288/2500],\n",
      " Training Loss: 0.00006645\n",
      "Evaluation Loss: 0.00005908\n",
      "Epoch [289/2500],\n",
      " Training Loss: 0.00003919\n",
      "Evaluation Loss: 0.00002228\n",
      "Epoch [290/2500],\n",
      " Training Loss: 0.00002102\n",
      "Evaluation Loss: 0.00008121\n",
      "Epoch [291/2500],\n",
      " Training Loss: 0.00004121\n",
      "Evaluation Loss: 0.00002692\n",
      "Epoch [292/2500],\n",
      " Training Loss: 0.00002679\n",
      "Evaluation Loss: 0.00002959\n",
      "Epoch [293/2500],\n",
      " Training Loss: 0.00002518\n",
      "Evaluation Loss: 0.00002643\n",
      "Epoch [294/2500],\n",
      " Training Loss: 0.00002939\n",
      "Evaluation Loss: 0.00005770\n",
      "Epoch [295/2500],\n",
      " Training Loss: 0.00009072\n",
      "Evaluation Loss: 0.00003585\n",
      "Epoch [296/2500],\n",
      " Training Loss: 0.00008005\n",
      "Evaluation Loss: 0.00009359\n",
      "Epoch [297/2500],\n",
      " Training Loss: 0.00020944\n",
      "Evaluation Loss: 0.00007890\n",
      "Epoch [298/2500],\n",
      " Training Loss: 0.00036374\n",
      "Evaluation Loss: 0.00015900\n",
      "Epoch [299/2500],\n",
      " Training Loss: 0.00032684\n",
      "Evaluation Loss: 0.00014010\n",
      "Epoch [300/2500],\n",
      " Training Loss: 0.00010490\n",
      "Evaluation Loss: 0.00003311\n",
      "Epoch [301/2500],\n",
      " Training Loss: 0.00006920\n",
      "Evaluation Loss: 0.00003715\n",
      "Epoch [302/2500],\n",
      " Training Loss: 0.00012598\n",
      "Evaluation Loss: 0.00005928\n",
      "Epoch [303/2500],\n",
      " Training Loss: 0.00004861\n",
      "Evaluation Loss: 0.00003233\n",
      "Epoch [304/2500],\n",
      " Training Loss: 0.00005561\n",
      "Evaluation Loss: 0.00002373\n",
      "Epoch [305/2500],\n",
      " Training Loss: 0.00005687\n",
      "Evaluation Loss: 0.00002381\n",
      "Epoch [306/2500],\n",
      " Training Loss: 0.00006547\n",
      "Evaluation Loss: 0.00004252\n",
      "Epoch [307/2500],\n",
      " Training Loss: 0.00004600\n",
      "Evaluation Loss: 0.00003782\n",
      "Epoch [308/2500],\n",
      " Training Loss: 0.00005164\n",
      "Evaluation Loss: 0.00006193\n",
      "Epoch [309/2500],\n",
      " Training Loss: 0.00015503\n",
      "Evaluation Loss: 0.00013551\n",
      "Epoch [310/2500],\n",
      " Training Loss: 0.00014435\n",
      "Evaluation Loss: 0.00032438\n",
      "Epoch [311/2500],\n",
      " Training Loss: 0.00045416\n",
      "Evaluation Loss: 0.00013380\n",
      "Epoch [312/2500],\n",
      " Training Loss: 0.00010058\n",
      "Evaluation Loss: 0.00006530\n",
      "Epoch [313/2500],\n",
      " Training Loss: 0.00006124\n",
      "Evaluation Loss: 0.00003944\n",
      "Epoch [314/2500],\n",
      " Training Loss: 0.00006311\n",
      "Evaluation Loss: 0.00005222\n",
      "Epoch [315/2500],\n",
      " Training Loss: 0.00004122\n",
      "Evaluation Loss: 0.00002022\n",
      "Epoch [316/2500],\n",
      " Training Loss: 0.00002307\n",
      "Evaluation Loss: 0.00001499\n",
      "Epoch [317/2500],\n",
      " Training Loss: 0.00002202\n",
      "Evaluation Loss: 0.00000722\n",
      "Epoch [318/2500],\n",
      " Training Loss: 0.00002564\n",
      "Evaluation Loss: 0.00001250\n",
      "Epoch [319/2500],\n",
      " Training Loss: 0.00001993\n",
      "Evaluation Loss: 0.00001119\n",
      "Epoch [320/2500],\n",
      " Training Loss: 0.00001465\n",
      "Evaluation Loss: 0.00000837\n",
      "Epoch [321/2500],\n",
      " Training Loss: 0.00001138\n",
      "Evaluation Loss: 0.00000937\n",
      "Epoch [322/2500],\n",
      " Training Loss: 0.00000962\n",
      "Evaluation Loss: 0.00000657\n",
      "Epoch [323/2500],\n",
      " Training Loss: 0.00001321\n",
      "Evaluation Loss: 0.00001129\n",
      "Epoch [324/2500],\n",
      " Training Loss: 0.00001655\n",
      "Evaluation Loss: 0.00003269\n",
      "Epoch [325/2500],\n",
      " Training Loss: 0.00014547\n",
      "Evaluation Loss: 0.00007237\n",
      "Epoch [326/2500],\n",
      " Training Loss: 0.00007999\n",
      "Evaluation Loss: 0.00004148\n",
      "Epoch [327/2500],\n",
      " Training Loss: 0.00004550\n",
      "Evaluation Loss: 0.00005796\n",
      "Epoch [328/2500],\n",
      " Training Loss: 0.00005286\n",
      "Evaluation Loss: 0.00004853\n",
      "Epoch [329/2500],\n",
      " Training Loss: 0.00003249\n",
      "Evaluation Loss: 0.00004410\n",
      "Epoch [330/2500],\n",
      " Training Loss: 0.00003807\n",
      "Evaluation Loss: 0.00001927\n",
      "Epoch [331/2500],\n",
      " Training Loss: 0.00001445\n",
      "Evaluation Loss: 0.00001337\n",
      "Epoch [332/2500],\n",
      " Training Loss: 0.00001322\n",
      "Evaluation Loss: 0.00001508\n",
      "Epoch [333/2500],\n",
      " Training Loss: 0.00001365\n",
      "Evaluation Loss: 0.00000567\n",
      "Epoch [334/2500],\n",
      " Training Loss: 0.00001174\n",
      "Evaluation Loss: 0.00001175\n",
      "Epoch [335/2500],\n",
      " Training Loss: 0.00001078\n",
      "Evaluation Loss: 0.00000914\n",
      "Epoch [336/2500],\n",
      " Training Loss: 0.00003170\n",
      "Evaluation Loss: 0.00004285\n",
      "Epoch [337/2500],\n",
      " Training Loss: 0.00003471\n",
      "Evaluation Loss: 0.00002614\n",
      "Epoch [338/2500],\n",
      " Training Loss: 0.00001803\n",
      "Evaluation Loss: 0.00000820\n",
      "Epoch [339/2500],\n",
      " Training Loss: 0.00001851\n",
      "Evaluation Loss: 0.00000671\n",
      "Epoch [340/2500],\n",
      " Training Loss: 0.00001372\n",
      "Evaluation Loss: 0.00001286\n",
      "Epoch [341/2500],\n",
      " Training Loss: 0.00001323\n",
      "Evaluation Loss: 0.00001752\n",
      "Epoch [342/2500],\n",
      " Training Loss: 0.00001402\n",
      "Evaluation Loss: 0.00000875\n",
      "Epoch [343/2500],\n",
      " Training Loss: 0.00005129\n",
      "Evaluation Loss: 0.00004821\n",
      "Epoch [344/2500],\n",
      " Training Loss: 0.00007955\n",
      "Evaluation Loss: 0.00012768\n",
      "Epoch [345/2500],\n",
      " Training Loss: 0.00011075\n",
      "Evaluation Loss: 0.00006653\n",
      "Epoch [346/2500],\n",
      " Training Loss: 0.00012187\n",
      "Evaluation Loss: 0.00012788\n",
      "Epoch [347/2500],\n",
      " Training Loss: 0.00015718\n",
      "Evaluation Loss: 0.00009763\n",
      "Epoch [348/2500],\n",
      " Training Loss: 0.00021365\n",
      "Evaluation Loss: 0.00034511\n",
      "Epoch [349/2500],\n",
      " Training Loss: 0.00023677\n",
      "Evaluation Loss: 0.00013996\n",
      "Epoch [350/2500],\n",
      " Training Loss: 0.00013301\n",
      "Evaluation Loss: 0.00033895\n",
      "Epoch [351/2500],\n",
      " Training Loss: 0.00008294\n",
      "Evaluation Loss: 0.00008268\n",
      "Epoch [352/2500],\n",
      " Training Loss: 0.00003822\n",
      "Evaluation Loss: 0.00001927\n",
      "Epoch [353/2500],\n",
      " Training Loss: 0.00002225\n",
      "Evaluation Loss: 0.00000887\n",
      "Epoch [354/2500],\n",
      " Training Loss: 0.00001507\n",
      "Evaluation Loss: 0.00002136\n",
      "Epoch [355/2500],\n",
      " Training Loss: 0.00005280\n",
      "Evaluation Loss: 0.00006283\n",
      "Epoch [356/2500],\n",
      " Training Loss: 0.00012104\n",
      "Evaluation Loss: 0.00026325\n",
      "Epoch [357/2500],\n",
      " Training Loss: 0.00019267\n",
      "Evaluation Loss: 0.00007062\n",
      "Epoch [358/2500],\n",
      " Training Loss: 0.00007024\n",
      "Evaluation Loss: 0.00003960\n",
      "Epoch [359/2500],\n",
      " Training Loss: 0.00015593\n",
      "Evaluation Loss: 0.00015052\n",
      "Epoch [360/2500],\n",
      " Training Loss: 0.00011725\n",
      "Evaluation Loss: 0.00006978\n",
      "Epoch [361/2500],\n",
      " Training Loss: 0.00004540\n",
      "Evaluation Loss: 0.00003019\n",
      "Epoch [362/2500],\n",
      " Training Loss: 0.00006602\n",
      "Evaluation Loss: 0.00005410\n",
      "Epoch [363/2500],\n",
      " Training Loss: 0.00003821\n",
      "Evaluation Loss: 0.00001442\n",
      "Epoch [364/2500],\n",
      " Training Loss: 0.00007209\n",
      "Evaluation Loss: 0.00006911\n",
      "Epoch [365/2500],\n",
      " Training Loss: 0.00012046\n",
      "Evaluation Loss: 0.00006211\n",
      "Epoch [366/2500],\n",
      " Training Loss: 0.00002685\n",
      "Evaluation Loss: 0.00001181\n",
      "Epoch [367/2500],\n",
      " Training Loss: 0.00002061\n",
      "Evaluation Loss: 0.00000975\n",
      "Epoch [368/2500],\n",
      " Training Loss: 0.00001071\n",
      "Evaluation Loss: 0.00001021\n",
      "Epoch [369/2500],\n",
      " Training Loss: 0.00000882\n",
      "Evaluation Loss: 0.00001708\n",
      "Epoch [370/2500],\n",
      " Training Loss: 0.00002127\n",
      "Evaluation Loss: 0.00000871\n",
      "Epoch [371/2500],\n",
      " Training Loss: 0.00000847\n",
      "Evaluation Loss: 0.00000478\n",
      "Epoch [372/2500],\n",
      " Training Loss: 0.00000653\n",
      "Evaluation Loss: 0.00001213\n",
      "Epoch [373/2500],\n",
      " Training Loss: 0.00003555\n",
      "Evaluation Loss: 0.00003280\n",
      "Epoch [374/2500],\n",
      " Training Loss: 0.00001305\n",
      "Evaluation Loss: 0.00000980\n",
      "Epoch [375/2500],\n",
      " Training Loss: 0.00000705\n",
      "Evaluation Loss: 0.00000638\n",
      "Epoch [376/2500],\n",
      " Training Loss: 0.00000856\n",
      "Evaluation Loss: 0.00000644\n",
      "Epoch [377/2500],\n",
      " Training Loss: 0.00001805\n",
      "Evaluation Loss: 0.00000890\n",
      "Epoch [378/2500],\n",
      " Training Loss: 0.00001397\n",
      "Evaluation Loss: 0.00000606\n",
      "Epoch [379/2500],\n",
      " Training Loss: 0.00001625\n",
      "Evaluation Loss: 0.00000410\n",
      "Epoch [380/2500],\n",
      " Training Loss: 0.00000679\n",
      "Evaluation Loss: 0.00000451\n",
      "Epoch [381/2500],\n",
      " Training Loss: 0.00000969\n",
      "Evaluation Loss: 0.00000517\n",
      "Epoch [382/2500],\n",
      " Training Loss: 0.00001080\n",
      "Evaluation Loss: 0.00000522\n",
      "Epoch [383/2500],\n",
      " Training Loss: 0.00002124\n",
      "Evaluation Loss: 0.00002425\n",
      "Epoch [384/2500],\n",
      " Training Loss: 0.00004255\n",
      "Evaluation Loss: 0.00004104\n",
      "Epoch [385/2500],\n",
      " Training Loss: 0.00006591\n",
      "Evaluation Loss: 0.00004502\n",
      "Epoch [386/2500],\n",
      " Training Loss: 0.00008293\n",
      "Evaluation Loss: 0.00006635\n",
      "Epoch [387/2500],\n",
      " Training Loss: 0.00005129\n",
      "Evaluation Loss: 0.00008651\n",
      "Epoch [388/2500],\n",
      " Training Loss: 0.00011467\n",
      "Evaluation Loss: 0.00024240\n",
      "Epoch [389/2500],\n",
      " Training Loss: 0.00040462\n",
      "Evaluation Loss: 0.00025891\n",
      "Epoch [390/2500],\n",
      " Training Loss: 0.00015329\n",
      "Evaluation Loss: 0.00009631\n",
      "Epoch [391/2500],\n",
      " Training Loss: 0.00013619\n",
      "Evaluation Loss: 0.00008989\n",
      "Epoch [392/2500],\n",
      " Training Loss: 0.00009566\n",
      "Evaluation Loss: 0.00004123\n",
      "Epoch [393/2500],\n",
      " Training Loss: 0.00011244\n",
      "Evaluation Loss: 0.00005273\n",
      "Epoch [394/2500],\n",
      " Training Loss: 0.00005291\n",
      "Evaluation Loss: 0.00003345\n",
      "Epoch [395/2500],\n",
      " Training Loss: 0.00002162\n",
      "Evaluation Loss: 0.00001230\n",
      "Epoch [396/2500],\n",
      " Training Loss: 0.00001470\n",
      "Evaluation Loss: 0.00000910\n",
      "Epoch [397/2500],\n",
      " Training Loss: 0.00000975\n",
      "Evaluation Loss: 0.00000663\n",
      "Epoch [398/2500],\n",
      " Training Loss: 0.00002012\n",
      "Evaluation Loss: 0.00001928\n",
      "Epoch [399/2500],\n",
      " Training Loss: 0.00001637\n",
      "Evaluation Loss: 0.00001019\n",
      "Epoch [400/2500],\n",
      " Training Loss: 0.00001151\n",
      "Evaluation Loss: 0.00000920\n",
      "Epoch [401/2500],\n",
      " Training Loss: 0.00001152\n",
      "Evaluation Loss: 0.00000463\n",
      "Epoch [402/2500],\n",
      " Training Loss: 0.00000735\n",
      "Evaluation Loss: 0.00000711\n",
      "Epoch [403/2500],\n",
      " Training Loss: 0.00000532\n",
      "Evaluation Loss: 0.00000806\n",
      "Epoch [404/2500],\n",
      " Training Loss: 0.00000679\n",
      "Evaluation Loss: 0.00001409\n",
      "Epoch [405/2500],\n",
      " Training Loss: 0.00000857\n",
      "Evaluation Loss: 0.00000853\n",
      "Epoch [406/2500],\n",
      " Training Loss: 0.00001012\n",
      "Evaluation Loss: 0.00001220\n",
      "Epoch [407/2500],\n",
      " Training Loss: 0.00001037\n",
      "Evaluation Loss: 0.00000633\n",
      "Epoch [408/2500],\n",
      " Training Loss: 0.00000872\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [409/2500],\n",
      " Training Loss: 0.00000912\n",
      "Evaluation Loss: 0.00000933\n",
      "Epoch [410/2500],\n",
      " Training Loss: 0.00006504\n",
      "Evaluation Loss: 0.00009825\n",
      "Epoch [411/2500],\n",
      " Training Loss: 0.00009489\n",
      "Evaluation Loss: 0.00012705\n",
      "Epoch [412/2500],\n",
      " Training Loss: 0.00005859\n",
      "Evaluation Loss: 0.00006354\n",
      "Epoch [413/2500],\n",
      " Training Loss: 0.00003565\n",
      "Evaluation Loss: 0.00002503\n",
      "Epoch [414/2500],\n",
      " Training Loss: 0.00006063\n",
      "Evaluation Loss: 0.00003596\n",
      "Epoch [415/2500],\n",
      " Training Loss: 0.00002302\n",
      "Evaluation Loss: 0.00003208\n",
      "Epoch [416/2500],\n",
      " Training Loss: 0.00001148\n",
      "Evaluation Loss: 0.00001215\n",
      "Epoch [417/2500],\n",
      " Training Loss: 0.00001952\n",
      "Evaluation Loss: 0.00001432\n",
      "Epoch [418/2500],\n",
      " Training Loss: 0.00007410\n",
      "Evaluation Loss: 0.00007040\n",
      "Epoch [419/2500],\n",
      " Training Loss: 0.00004496\n",
      "Evaluation Loss: 0.00004637\n",
      "Epoch [420/2500],\n",
      " Training Loss: 0.00021937\n",
      "Evaluation Loss: 0.00016498\n",
      "Epoch [421/2500],\n",
      " Training Loss: 0.00015653\n",
      "Evaluation Loss: 0.00010959\n",
      "Epoch [422/2500],\n",
      " Training Loss: 0.00006743\n",
      "Evaluation Loss: 0.00007149\n",
      "Epoch [423/2500],\n",
      " Training Loss: 0.00026907\n",
      "Evaluation Loss: 0.00039628\n",
      "Epoch [424/2500],\n",
      " Training Loss: 0.00021725\n",
      "Evaluation Loss: 0.00017512\n",
      "Epoch [425/2500],\n",
      " Training Loss: 0.00005478\n",
      "Evaluation Loss: 0.00004595\n",
      "Epoch [426/2500],\n",
      " Training Loss: 0.00005113\n",
      "Evaluation Loss: 0.00005191\n",
      "Epoch [427/2500],\n",
      " Training Loss: 0.00002846\n",
      "Evaluation Loss: 0.00002027\n",
      "Epoch [428/2500],\n",
      " Training Loss: 0.00002142\n",
      "Evaluation Loss: 0.00002713\n",
      "Epoch [429/2500],\n",
      " Training Loss: 0.00002089\n",
      "Evaluation Loss: 0.00001150\n",
      "Epoch [430/2500],\n",
      " Training Loss: 0.00002034\n",
      "Evaluation Loss: 0.00001729\n",
      "Epoch [431/2500],\n",
      " Training Loss: 0.00003020\n",
      "Evaluation Loss: 0.00001143\n",
      "Epoch [432/2500],\n",
      " Training Loss: 0.00002050\n",
      "Evaluation Loss: 0.00002869\n",
      "Epoch [433/2500],\n",
      " Training Loss: 0.00002325\n",
      "Evaluation Loss: 0.00002258\n",
      "Epoch [434/2500],\n",
      " Training Loss: 0.00002333\n",
      "Evaluation Loss: 0.00004332\n",
      "Epoch [435/2500],\n",
      " Training Loss: 0.00005192\n",
      "Evaluation Loss: 0.00002764\n",
      "Epoch [436/2500],\n",
      " Training Loss: 0.00005588\n",
      "Evaluation Loss: 0.00001450\n",
      "Epoch [437/2500],\n",
      " Training Loss: 0.00011620\n",
      "Evaluation Loss: 0.00005098\n",
      "Epoch [438/2500],\n",
      " Training Loss: 0.00005900\n",
      "Evaluation Loss: 0.00007819\n",
      "Epoch [439/2500],\n",
      " Training Loss: 0.00006313\n",
      "Evaluation Loss: 0.00002320\n",
      "Epoch [440/2500],\n",
      " Training Loss: 0.00001960\n",
      "Evaluation Loss: 0.00000804\n",
      "Epoch [441/2500],\n",
      " Training Loss: 0.00000973\n",
      "Evaluation Loss: 0.00000473\n",
      "Epoch [442/2500],\n",
      " Training Loss: 0.00001171\n",
      "Evaluation Loss: 0.00000668\n",
      "Epoch [443/2500],\n",
      " Training Loss: 0.00000645\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [444/2500],\n",
      " Training Loss: 0.00000717\n",
      "Evaluation Loss: 0.00000476\n",
      "Epoch [445/2500],\n",
      " Training Loss: 0.00000590\n",
      "Evaluation Loss: 0.00000384\n",
      "Epoch [446/2500],\n",
      " Training Loss: 0.00000459\n",
      "Evaluation Loss: 0.00000470\n",
      "Epoch [447/2500],\n",
      " Training Loss: 0.00000496\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [448/2500],\n",
      " Training Loss: 0.00000430\n",
      "Evaluation Loss: 0.00000288\n",
      "Epoch [449/2500],\n",
      " Training Loss: 0.00000326\n",
      "Evaluation Loss: 0.00000150\n",
      "Epoch [450/2500],\n",
      " Training Loss: 0.00000375\n",
      "Evaluation Loss: 0.00001818\n",
      "Epoch [451/2500],\n",
      " Training Loss: 0.00001203\n",
      "Evaluation Loss: 0.00000577\n",
      "Epoch [452/2500],\n",
      " Training Loss: 0.00002582\n",
      "Evaluation Loss: 0.00003982\n",
      "Epoch [453/2500],\n",
      " Training Loss: 0.00002211\n",
      "Evaluation Loss: 0.00001014\n",
      "Epoch [454/2500],\n",
      " Training Loss: 0.00001084\n",
      "Evaluation Loss: 0.00000586\n",
      "Epoch [455/2500],\n",
      " Training Loss: 0.00002980\n",
      "Evaluation Loss: 0.00002778\n",
      "Epoch [456/2500],\n",
      " Training Loss: 0.00002693\n",
      "Evaluation Loss: 0.00000835\n",
      "Epoch [457/2500],\n",
      " Training Loss: 0.00001241\n",
      "Evaluation Loss: 0.00001106\n",
      "Epoch [458/2500],\n",
      " Training Loss: 0.00001864\n",
      "Evaluation Loss: 0.00001975\n",
      "Epoch [459/2500],\n",
      " Training Loss: 0.00004972\n",
      "Evaluation Loss: 0.00002267\n",
      "Epoch [460/2500],\n",
      " Training Loss: 0.00002093\n",
      "Evaluation Loss: 0.00000887\n",
      "Epoch [461/2500],\n",
      " Training Loss: 0.00002839\n",
      "Evaluation Loss: 0.00004257\n",
      "Epoch [462/2500],\n",
      " Training Loss: 0.00008082\n",
      "Evaluation Loss: 0.00013366\n",
      "Epoch [463/2500],\n",
      " Training Loss: 0.00015193\n",
      "Evaluation Loss: 0.00007815\n",
      "Epoch [464/2500],\n",
      " Training Loss: 0.00049353\n",
      "Evaluation Loss: 0.00015831\n",
      "Epoch [465/2500],\n",
      " Training Loss: 0.00013494\n",
      "Evaluation Loss: 0.00010503\n",
      "Epoch [466/2500],\n",
      " Training Loss: 0.00011332\n",
      "Evaluation Loss: 0.00003706\n",
      "Epoch [467/2500],\n",
      " Training Loss: 0.00014078\n",
      "Evaluation Loss: 0.00007016\n",
      "Epoch [468/2500],\n",
      " Training Loss: 0.00014960\n",
      "Evaluation Loss: 0.00002777\n",
      "Epoch [469/2500],\n",
      " Training Loss: 0.00003698\n",
      "Evaluation Loss: 0.00001344\n",
      "Epoch [470/2500],\n",
      " Training Loss: 0.00002077\n",
      "Evaluation Loss: 0.00001554\n",
      "Epoch [471/2500],\n",
      " Training Loss: 0.00001240\n",
      "Evaluation Loss: 0.00002738\n",
      "Epoch [472/2500],\n",
      " Training Loss: 0.00001495\n",
      "Evaluation Loss: 0.00001100\n",
      "Epoch [473/2500],\n",
      " Training Loss: 0.00001378\n",
      "Evaluation Loss: 0.00002913\n",
      "Epoch [474/2500],\n",
      " Training Loss: 0.00002215\n",
      "Evaluation Loss: 0.00000996\n",
      "Epoch [475/2500],\n",
      " Training Loss: 0.00005905\n",
      "Evaluation Loss: 0.00007690\n",
      "Epoch [476/2500],\n",
      " Training Loss: 0.00005351\n",
      "Evaluation Loss: 0.00005228\n",
      "Epoch [477/2500],\n",
      " Training Loss: 0.00010550\n",
      "Evaluation Loss: 0.00004577\n",
      "Epoch [478/2500],\n",
      " Training Loss: 0.00021038\n",
      "Evaluation Loss: 0.00013581\n",
      "Epoch [479/2500],\n",
      " Training Loss: 0.00012566\n",
      "Evaluation Loss: 0.00006007\n",
      "Epoch [480/2500],\n",
      " Training Loss: 0.00015824\n",
      "Evaluation Loss: 0.00005852\n",
      "Epoch [481/2500],\n",
      " Training Loss: 0.00009498\n",
      "Evaluation Loss: 0.00003489\n",
      "Epoch [482/2500],\n",
      " Training Loss: 0.00002432\n",
      "Evaluation Loss: 0.00000848\n",
      "Epoch [483/2500],\n",
      " Training Loss: 0.00001237\n",
      "Evaluation Loss: 0.00000967\n",
      "Epoch [484/2500],\n",
      " Training Loss: 0.00000919\n",
      "Evaluation Loss: 0.00000835\n",
      "Epoch [485/2500],\n",
      " Training Loss: 0.00001112\n",
      "Evaluation Loss: 0.00001182\n",
      "Epoch [486/2500],\n",
      " Training Loss: 0.00000885\n",
      "Evaluation Loss: 0.00001251\n",
      "Epoch [487/2500],\n",
      " Training Loss: 0.00000915\n",
      "Evaluation Loss: 0.00000351\n",
      "Epoch [488/2500],\n",
      " Training Loss: 0.00000757\n",
      "Evaluation Loss: 0.00000855\n",
      "Epoch [489/2500],\n",
      " Training Loss: 0.00001649\n",
      "Evaluation Loss: 0.00002182\n",
      "Epoch [490/2500],\n",
      " Training Loss: 0.00000834\n",
      "Evaluation Loss: 0.00000533\n",
      "Epoch [491/2500],\n",
      " Training Loss: 0.00000404\n",
      "Evaluation Loss: 0.00000417\n",
      "Epoch [492/2500],\n",
      " Training Loss: 0.00000332\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [493/2500],\n",
      " Training Loss: 0.00000306\n",
      "Evaluation Loss: 0.00000306\n",
      "Epoch [494/2500],\n",
      " Training Loss: 0.00000416\n",
      "Evaluation Loss: 0.00000482\n",
      "Epoch [495/2500],\n",
      " Training Loss: 0.00000471\n",
      "Evaluation Loss: 0.00000347\n",
      "Epoch [496/2500],\n",
      " Training Loss: 0.00000372\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [497/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000229\n",
      "Epoch [498/2500],\n",
      " Training Loss: 0.00000415\n",
      "Evaluation Loss: 0.00000290\n",
      "Epoch [499/2500],\n",
      " Training Loss: 0.00000274\n",
      "Evaluation Loss: 0.00000307\n",
      "Epoch [500/2500],\n",
      " Training Loss: 0.00000434\n",
      "Evaluation Loss: 0.00000251\n",
      "Epoch [501/2500],\n",
      " Training Loss: 0.00000423\n",
      "Evaluation Loss: 0.00000375\n",
      "Epoch [502/2500],\n",
      " Training Loss: 0.00000716\n",
      "Evaluation Loss: 0.00000397\n",
      "Epoch [503/2500],\n",
      " Training Loss: 0.00000745\n",
      "Evaluation Loss: 0.00000611\n",
      "Epoch [504/2500],\n",
      " Training Loss: 0.00001302\n",
      "Evaluation Loss: 0.00000407\n",
      "Epoch [505/2500],\n",
      " Training Loss: 0.00001343\n",
      "Evaluation Loss: 0.00001366\n",
      "Epoch [506/2500],\n",
      " Training Loss: 0.00002068\n",
      "Evaluation Loss: 0.00000842\n",
      "Epoch [507/2500],\n",
      " Training Loss: 0.00001708\n",
      "Evaluation Loss: 0.00000759\n",
      "Epoch [508/2500],\n",
      " Training Loss: 0.00000600\n",
      "Evaluation Loss: 0.00000301\n",
      "Epoch [509/2500],\n",
      " Training Loss: 0.00000871\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [510/2500],\n",
      " Training Loss: 0.00001144\n",
      "Evaluation Loss: 0.00000573\n",
      "Epoch [511/2500],\n",
      " Training Loss: 0.00000748\n",
      "Evaluation Loss: 0.00000646\n",
      "Epoch [512/2500],\n",
      " Training Loss: 0.00000770\n",
      "Evaluation Loss: 0.00000757\n",
      "Epoch [513/2500],\n",
      " Training Loss: 0.00002162\n",
      "Evaluation Loss: 0.00000747\n",
      "Epoch [514/2500],\n",
      " Training Loss: 0.00000745\n",
      "Evaluation Loss: 0.00001052\n",
      "Epoch [515/2500],\n",
      " Training Loss: 0.00001393\n",
      "Evaluation Loss: 0.00000672\n",
      "Epoch [516/2500],\n",
      " Training Loss: 0.00002183\n",
      "Evaluation Loss: 0.00001162\n",
      "Epoch [517/2500],\n",
      " Training Loss: 0.00006046\n",
      "Evaluation Loss: 0.00002925\n",
      "Epoch [518/2500],\n",
      " Training Loss: 0.00004924\n",
      "Evaluation Loss: 0.00005985\n",
      "Epoch [519/2500],\n",
      " Training Loss: 0.00012454\n",
      "Evaluation Loss: 0.00005285\n",
      "Epoch [520/2500],\n",
      " Training Loss: 0.00016745\n",
      "Evaluation Loss: 0.00006242\n",
      "Epoch [521/2500],\n",
      " Training Loss: 0.00005950\n",
      "Evaluation Loss: 0.00002311\n",
      "Epoch [522/2500],\n",
      " Training Loss: 0.00002476\n",
      "Evaluation Loss: 0.00000951\n",
      "Epoch [523/2500],\n",
      " Training Loss: 0.00004267\n",
      "Evaluation Loss: 0.00002833\n",
      "Epoch [524/2500],\n",
      " Training Loss: 0.00003687\n",
      "Evaluation Loss: 0.00006082\n",
      "Epoch [525/2500],\n",
      " Training Loss: 0.00003709\n",
      "Evaluation Loss: 0.00003108\n",
      "Epoch [526/2500],\n",
      " Training Loss: 0.00004031\n",
      "Evaluation Loss: 0.00007020\n",
      "Epoch [527/2500],\n",
      " Training Loss: 0.00002807\n",
      "Evaluation Loss: 0.00000859\n",
      "Epoch [528/2500],\n",
      " Training Loss: 0.00001588\n",
      "Evaluation Loss: 0.00002233\n",
      "Epoch [529/2500],\n",
      " Training Loss: 0.00002545\n",
      "Evaluation Loss: 0.00001141\n",
      "Epoch [530/2500],\n",
      " Training Loss: 0.00001454\n",
      "Evaluation Loss: 0.00000542\n",
      "Epoch [531/2500],\n",
      " Training Loss: 0.00002544\n",
      "Evaluation Loss: 0.00003884\n",
      "Epoch [532/2500],\n",
      " Training Loss: 0.00003871\n",
      "Evaluation Loss: 0.00002048\n",
      "Epoch [533/2500],\n",
      " Training Loss: 0.00014085\n",
      "Evaluation Loss: 0.00015141\n",
      "Epoch [534/2500],\n",
      " Training Loss: 0.00010511\n",
      "Evaluation Loss: 0.00004634\n",
      "Epoch [535/2500],\n",
      " Training Loss: 0.00007380\n",
      "Evaluation Loss: 0.00002729\n",
      "Epoch [536/2500],\n",
      " Training Loss: 0.00004163\n",
      "Evaluation Loss: 0.00002777\n",
      "Epoch [537/2500],\n",
      " Training Loss: 0.00004938\n",
      "Evaluation Loss: 0.00002882\n",
      "Epoch [538/2500],\n",
      " Training Loss: 0.00008436\n",
      "Evaluation Loss: 0.00012097\n",
      "Epoch [539/2500],\n",
      " Training Loss: 0.00011767\n",
      "Evaluation Loss: 0.00008012\n",
      "Epoch [540/2500],\n",
      " Training Loss: 0.00006782\n",
      "Evaluation Loss: 0.00002272\n",
      "Epoch [541/2500],\n",
      " Training Loss: 0.00008581\n",
      "Evaluation Loss: 0.00003174\n",
      "Epoch [542/2500],\n",
      " Training Loss: 0.00005072\n",
      "Evaluation Loss: 0.00001997\n",
      "Epoch [543/2500],\n",
      " Training Loss: 0.00004984\n",
      "Evaluation Loss: 0.00005802\n",
      "Epoch [544/2500],\n",
      " Training Loss: 0.00004662\n",
      "Evaluation Loss: 0.00001771\n",
      "Epoch [545/2500],\n",
      " Training Loss: 0.00005202\n",
      "Evaluation Loss: 0.00003257\n",
      "Epoch [546/2500],\n",
      " Training Loss: 0.00003606\n",
      "Evaluation Loss: 0.00001592\n",
      "Epoch [547/2500],\n",
      " Training Loss: 0.00001609\n",
      "Evaluation Loss: 0.00000273\n",
      "Epoch [548/2500],\n",
      " Training Loss: 0.00001236\n",
      "Evaluation Loss: 0.00000593\n",
      "Epoch [549/2500],\n",
      " Training Loss: 0.00002424\n",
      "Evaluation Loss: 0.00001608\n",
      "Epoch [550/2500],\n",
      " Training Loss: 0.00004245\n",
      "Evaluation Loss: 0.00003868\n",
      "Epoch [551/2500],\n",
      " Training Loss: 0.00002438\n",
      "Evaluation Loss: 0.00001345\n",
      "Epoch [552/2500],\n",
      " Training Loss: 0.00001277\n",
      "Evaluation Loss: 0.00000422\n",
      "Epoch [553/2500],\n",
      " Training Loss: 0.00000568\n",
      "Evaluation Loss: 0.00000644\n",
      "Epoch [554/2500],\n",
      " Training Loss: 0.00001016\n",
      "Evaluation Loss: 0.00000791\n",
      "Epoch [555/2500],\n",
      " Training Loss: 0.00001481\n",
      "Evaluation Loss: 0.00001332\n",
      "Epoch [556/2500],\n",
      " Training Loss: 0.00001085\n",
      "Evaluation Loss: 0.00000597\n",
      "Epoch [557/2500],\n",
      " Training Loss: 0.00000883\n",
      "Evaluation Loss: 0.00000618\n",
      "Epoch [558/2500],\n",
      " Training Loss: 0.00002087\n",
      "Evaluation Loss: 0.00001111\n",
      "Epoch [559/2500],\n",
      " Training Loss: 0.00002992\n",
      "Evaluation Loss: 0.00000525\n",
      "Epoch [560/2500],\n",
      " Training Loss: 0.00002882\n",
      "Evaluation Loss: 0.00007303\n",
      "Epoch [561/2500],\n",
      " Training Loss: 0.00008983\n",
      "Evaluation Loss: 0.00008542\n",
      "Epoch [562/2500],\n",
      " Training Loss: 0.00006503\n",
      "Evaluation Loss: 0.00004486\n",
      "Epoch [563/2500],\n",
      " Training Loss: 0.00004872\n",
      "Evaluation Loss: 0.00001279\n",
      "Epoch [564/2500],\n",
      " Training Loss: 0.00002417\n",
      "Evaluation Loss: 0.00000922\n",
      "Epoch [565/2500],\n",
      " Training Loss: 0.00001622\n",
      "Evaluation Loss: 0.00001407\n",
      "Epoch [566/2500],\n",
      " Training Loss: 0.00001058\n",
      "Evaluation Loss: 0.00000437\n",
      "Epoch [567/2500],\n",
      " Training Loss: 0.00000627\n",
      "Evaluation Loss: 0.00000586\n",
      "Epoch [568/2500],\n",
      " Training Loss: 0.00000794\n",
      "Evaluation Loss: 0.00000952\n",
      "Epoch [569/2500],\n",
      " Training Loss: 0.00000797\n",
      "Evaluation Loss: 0.00000663\n",
      "Epoch [570/2500],\n",
      " Training Loss: 0.00000587\n",
      "Evaluation Loss: 0.00000428\n",
      "Epoch [571/2500],\n",
      " Training Loss: 0.00001557\n",
      "Evaluation Loss: 0.00002598\n",
      "Epoch [572/2500],\n",
      " Training Loss: 0.00002857\n",
      "Evaluation Loss: 0.00002039\n",
      "Epoch [573/2500],\n",
      " Training Loss: 0.00005747\n",
      "Evaluation Loss: 0.00001883\n",
      "Epoch [574/2500],\n",
      " Training Loss: 0.00003804\n",
      "Evaluation Loss: 0.00003325\n",
      "Epoch [575/2500],\n",
      " Training Loss: 0.00001962\n",
      "Evaluation Loss: 0.00001599\n",
      "Epoch [576/2500],\n",
      " Training Loss: 0.00001596\n",
      "Evaluation Loss: 0.00000938\n",
      "Epoch [577/2500],\n",
      " Training Loss: 0.00001777\n",
      "Evaluation Loss: 0.00000712\n",
      "Epoch [578/2500],\n",
      " Training Loss: 0.00001977\n",
      "Evaluation Loss: 0.00001463\n",
      "Epoch [579/2500],\n",
      " Training Loss: 0.00001382\n",
      "Evaluation Loss: 0.00000968\n",
      "Epoch [580/2500],\n",
      " Training Loss: 0.00001582\n",
      "Evaluation Loss: 0.00002397\n",
      "Epoch [581/2500],\n",
      " Training Loss: 0.00001752\n",
      "Evaluation Loss: 0.00002122\n",
      "Epoch [582/2500],\n",
      " Training Loss: 0.00002159\n",
      "Evaluation Loss: 0.00000891\n",
      "Epoch [583/2500],\n",
      " Training Loss: 0.00001452\n",
      "Evaluation Loss: 0.00002379\n",
      "Epoch [584/2500],\n",
      " Training Loss: 0.00001163\n",
      "Evaluation Loss: 0.00000367\n",
      "Epoch [585/2500],\n",
      " Training Loss: 0.00000558\n",
      "Evaluation Loss: 0.00000377\n",
      "Epoch [586/2500],\n",
      " Training Loss: 0.00000666\n",
      "Evaluation Loss: 0.00000814\n",
      "Epoch [587/2500],\n",
      " Training Loss: 0.00002551\n",
      "Evaluation Loss: 0.00001488\n",
      "Epoch [588/2500],\n",
      " Training Loss: 0.00002200\n",
      "Evaluation Loss: 0.00002320\n",
      "Epoch [589/2500],\n",
      " Training Loss: 0.00017324\n",
      "Evaluation Loss: 0.00018441\n",
      "Epoch [590/2500],\n",
      " Training Loss: 0.00009521\n",
      "Evaluation Loss: 0.00001616\n",
      "Epoch [591/2500],\n",
      " Training Loss: 0.00019666\n",
      "Evaluation Loss: 0.00019665\n",
      "Epoch [592/2500],\n",
      " Training Loss: 0.00015250\n",
      "Evaluation Loss: 0.00001499\n",
      "Epoch [593/2500],\n",
      " Training Loss: 0.00003506\n",
      "Evaluation Loss: 0.00000900\n",
      "Epoch [594/2500],\n",
      " Training Loss: 0.00001764\n",
      "Evaluation Loss: 0.00000806\n",
      "Epoch [595/2500],\n",
      " Training Loss: 0.00000894\n",
      "Evaluation Loss: 0.00000556\n",
      "Epoch [596/2500],\n",
      " Training Loss: 0.00000641\n",
      "Evaluation Loss: 0.00000647\n",
      "Epoch [597/2500],\n",
      " Training Loss: 0.00000692\n",
      "Evaluation Loss: 0.00000484\n",
      "Epoch [598/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000274\n",
      "Epoch [599/2500],\n",
      " Training Loss: 0.00000705\n",
      "Evaluation Loss: 0.00000519\n",
      "Epoch [600/2500],\n",
      " Training Loss: 0.00000913\n",
      "Evaluation Loss: 0.00000760\n",
      "Epoch [601/2500],\n",
      " Training Loss: 0.00001172\n",
      "Evaluation Loss: 0.00000678\n",
      "Epoch [602/2500],\n",
      " Training Loss: 0.00001723\n",
      "Evaluation Loss: 0.00001324\n",
      "Epoch [603/2500],\n",
      " Training Loss: 0.00003093\n",
      "Evaluation Loss: 0.00002569\n",
      "Epoch [604/2500],\n",
      " Training Loss: 0.00005677\n",
      "Evaluation Loss: 0.00003545\n",
      "Epoch [605/2500],\n",
      " Training Loss: 0.00008729\n",
      "Evaluation Loss: 0.00004553\n",
      "Epoch [606/2500],\n",
      " Training Loss: 0.00007690\n",
      "Evaluation Loss: 0.00004613\n",
      "Epoch [607/2500],\n",
      " Training Loss: 0.00009662\n",
      "Evaluation Loss: 0.00001972\n",
      "Epoch [608/2500],\n",
      " Training Loss: 0.00003411\n",
      "Evaluation Loss: 0.00002536\n",
      "Epoch [609/2500],\n",
      " Training Loss: 0.00007050\n",
      "Evaluation Loss: 0.00004050\n",
      "Epoch [610/2500],\n",
      " Training Loss: 0.00004314\n",
      "Evaluation Loss: 0.00002152\n",
      "Epoch [611/2500],\n",
      " Training Loss: 0.00007920\n",
      "Evaluation Loss: 0.00007656\n",
      "Epoch [612/2500],\n",
      " Training Loss: 0.00007482\n",
      "Evaluation Loss: 0.00007766\n",
      "Epoch [613/2500],\n",
      " Training Loss: 0.00003753\n",
      "Evaluation Loss: 0.00004065\n",
      "Epoch [614/2500],\n",
      " Training Loss: 0.00000938\n",
      "Evaluation Loss: 0.00000341\n",
      "Epoch [615/2500],\n",
      " Training Loss: 0.00000345\n",
      "Evaluation Loss: 0.00000280\n",
      "Epoch [616/2500],\n",
      " Training Loss: 0.00000348\n",
      "Evaluation Loss: 0.00000265\n",
      "Epoch [617/2500],\n",
      " Training Loss: 0.00000326\n",
      "Evaluation Loss: 0.00000346\n",
      "Epoch [618/2500],\n",
      " Training Loss: 0.00000260\n",
      "Evaluation Loss: 0.00000231\n",
      "Epoch [619/2500],\n",
      " Training Loss: 0.00000283\n",
      "Evaluation Loss: 0.00000240\n",
      "Epoch [620/2500],\n",
      " Training Loss: 0.00000575\n",
      "Evaluation Loss: 0.00000398\n",
      "Epoch [621/2500],\n",
      " Training Loss: 0.00000611\n",
      "Evaluation Loss: 0.00001052\n",
      "Epoch [622/2500],\n",
      " Training Loss: 0.00000657\n",
      "Evaluation Loss: 0.00000266\n",
      "Epoch [623/2500],\n",
      " Training Loss: 0.00000587\n",
      "Evaluation Loss: 0.00000605\n",
      "Epoch [624/2500],\n",
      " Training Loss: 0.00000498\n",
      "Evaluation Loss: 0.00000324\n",
      "Epoch [625/2500],\n",
      " Training Loss: 0.00001040\n",
      "Evaluation Loss: 0.00001203\n",
      "Epoch [626/2500],\n",
      " Training Loss: 0.00001488\n",
      "Evaluation Loss: 0.00000454\n",
      "Epoch [627/2500],\n",
      " Training Loss: 0.00002135\n",
      "Evaluation Loss: 0.00002309\n",
      "Epoch [628/2500],\n",
      " Training Loss: 0.00003256\n",
      "Evaluation Loss: 0.00002776\n",
      "Epoch [629/2500],\n",
      " Training Loss: 0.00003991\n",
      "Evaluation Loss: 0.00008121\n",
      "Epoch [630/2500],\n",
      " Training Loss: 0.00008313\n",
      "Evaluation Loss: 0.00006611\n",
      "Epoch [631/2500],\n",
      " Training Loss: 0.00005230\n",
      "Evaluation Loss: 0.00004461\n",
      "Epoch [632/2500],\n",
      " Training Loss: 0.00010649\n",
      "Evaluation Loss: 0.00023215\n",
      "Epoch [633/2500],\n",
      " Training Loss: 0.00022720\n",
      "Evaluation Loss: 0.00014190\n",
      "Epoch [634/2500],\n",
      " Training Loss: 0.00014056\n",
      "Evaluation Loss: 0.00003878\n",
      "Epoch [635/2500],\n",
      " Training Loss: 0.00007558\n",
      "Evaluation Loss: 0.00003339\n",
      "Epoch [636/2500],\n",
      " Training Loss: 0.00002517\n",
      "Evaluation Loss: 0.00001247\n",
      "Epoch [637/2500],\n",
      " Training Loss: 0.00001427\n",
      "Evaluation Loss: 0.00002501\n",
      "Epoch [638/2500],\n",
      " Training Loss: 0.00002466\n",
      "Evaluation Loss: 0.00001162\n",
      "Epoch [639/2500],\n",
      " Training Loss: 0.00001087\n",
      "Evaluation Loss: 0.00000968\n",
      "Epoch [640/2500],\n",
      " Training Loss: 0.00000810\n",
      "Evaluation Loss: 0.00000970\n",
      "Epoch [641/2500],\n",
      " Training Loss: 0.00001059\n",
      "Evaluation Loss: 0.00001421\n",
      "Epoch [642/2500],\n",
      " Training Loss: 0.00001691\n",
      "Evaluation Loss: 0.00000798\n",
      "Epoch [643/2500],\n",
      " Training Loss: 0.00000793\n",
      "Evaluation Loss: 0.00000280\n",
      "Epoch [644/2500],\n",
      " Training Loss: 0.00001512\n",
      "Evaluation Loss: 0.00000609\n",
      "Epoch [645/2500],\n",
      " Training Loss: 0.00000892\n",
      "Evaluation Loss: 0.00000377\n",
      "Epoch [646/2500],\n",
      " Training Loss: 0.00000574\n",
      "Evaluation Loss: 0.00000682\n",
      "Epoch [647/2500],\n",
      " Training Loss: 0.00000503\n",
      "Evaluation Loss: 0.00000701\n",
      "Epoch [648/2500],\n",
      " Training Loss: 0.00000550\n",
      "Evaluation Loss: 0.00000272\n",
      "Epoch [649/2500],\n",
      " Training Loss: 0.00000643\n",
      "Evaluation Loss: 0.00000972\n",
      "Epoch [650/2500],\n",
      " Training Loss: 0.00001220\n",
      "Evaluation Loss: 0.00001450\n",
      "Epoch [651/2500],\n",
      " Training Loss: 0.00000986\n",
      "Evaluation Loss: 0.00000469\n",
      "Epoch [652/2500],\n",
      " Training Loss: 0.00000678\n",
      "Evaluation Loss: 0.00000345\n",
      "Epoch [653/2500],\n",
      " Training Loss: 0.00000463\n",
      "Evaluation Loss: 0.00000430\n",
      "Epoch [654/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000874\n",
      "Epoch [655/2500],\n",
      " Training Loss: 0.00001104\n",
      "Evaluation Loss: 0.00001484\n",
      "Epoch [656/2500],\n",
      " Training Loss: 0.00001842\n",
      "Evaluation Loss: 0.00001996\n",
      "Epoch [657/2500],\n",
      " Training Loss: 0.00003868\n",
      "Evaluation Loss: 0.00001815\n",
      "Epoch [658/2500],\n",
      " Training Loss: 0.00002888\n",
      "Evaluation Loss: 0.00001413\n",
      "Epoch [659/2500],\n",
      " Training Loss: 0.00001491\n",
      "Evaluation Loss: 0.00001443\n",
      "Epoch [660/2500],\n",
      " Training Loss: 0.00001911\n",
      "Evaluation Loss: 0.00002573\n",
      "Epoch [661/2500],\n",
      " Training Loss: 0.00002312\n",
      "Evaluation Loss: 0.00001567\n",
      "Epoch [662/2500],\n",
      " Training Loss: 0.00001776\n",
      "Evaluation Loss: 0.00002486\n",
      "Epoch [663/2500],\n",
      " Training Loss: 0.00002801\n",
      "Evaluation Loss: 0.00001429\n",
      "Epoch [664/2500],\n",
      " Training Loss: 0.00003666\n",
      "Evaluation Loss: 0.00000641\n",
      "Epoch [665/2500],\n",
      " Training Loss: 0.00003327\n",
      "Evaluation Loss: 0.00002664\n",
      "Epoch [666/2500],\n",
      " Training Loss: 0.00003309\n",
      "Evaluation Loss: 0.00002122\n",
      "Epoch [667/2500],\n",
      " Training Loss: 0.00003198\n",
      "Evaluation Loss: 0.00001637\n",
      "Epoch [668/2500],\n",
      " Training Loss: 0.00002142\n",
      "Evaluation Loss: 0.00001339\n",
      "Epoch [669/2500],\n",
      " Training Loss: 0.00002928\n",
      "Evaluation Loss: 0.00005385\n",
      "Epoch [670/2500],\n",
      " Training Loss: 0.00003049\n",
      "Evaluation Loss: 0.00003286\n",
      "Epoch [671/2500],\n",
      " Training Loss: 0.00003227\n",
      "Evaluation Loss: 0.00001074\n",
      "Epoch [672/2500],\n",
      " Training Loss: 0.00002754\n",
      "Evaluation Loss: 0.00000977\n",
      "Epoch [673/2500],\n",
      " Training Loss: 0.00004726\n",
      "Evaluation Loss: 0.00008770\n",
      "Epoch [674/2500],\n",
      " Training Loss: 0.00003425\n",
      "Evaluation Loss: 0.00004126\n",
      "Epoch [675/2500],\n",
      " Training Loss: 0.00003417\n",
      "Evaluation Loss: 0.00002475\n",
      "Epoch [676/2500],\n",
      " Training Loss: 0.00003382\n",
      "Evaluation Loss: 0.00001026\n",
      "Epoch [677/2500],\n",
      " Training Loss: 0.00001963\n",
      "Evaluation Loss: 0.00003554\n",
      "Epoch [678/2500],\n",
      " Training Loss: 0.00003331\n",
      "Evaluation Loss: 0.00004125\n",
      "Epoch [679/2500],\n",
      " Training Loss: 0.00003857\n",
      "Evaluation Loss: 0.00001629\n",
      "Epoch [680/2500],\n",
      " Training Loss: 0.00001616\n",
      "Evaluation Loss: 0.00004045\n",
      "Epoch [681/2500],\n",
      " Training Loss: 0.00002811\n",
      "Evaluation Loss: 0.00001813\n",
      "Epoch [682/2500],\n",
      " Training Loss: 0.00001610\n",
      "Evaluation Loss: 0.00001433\n",
      "Epoch [683/2500],\n",
      " Training Loss: 0.00001616\n",
      "Evaluation Loss: 0.00002522\n",
      "Epoch [684/2500],\n",
      " Training Loss: 0.00001062\n",
      "Evaluation Loss: 0.00000408\n",
      "Epoch [685/2500],\n",
      " Training Loss: 0.00000403\n",
      "Evaluation Loss: 0.00000398\n",
      "Epoch [686/2500],\n",
      " Training Loss: 0.00000677\n",
      "Evaluation Loss: 0.00000526\n",
      "Epoch [687/2500],\n",
      " Training Loss: 0.00000682\n",
      "Evaluation Loss: 0.00000519\n",
      "Epoch [688/2500],\n",
      " Training Loss: 0.00001322\n",
      "Evaluation Loss: 0.00000562\n",
      "Epoch [689/2500],\n",
      " Training Loss: 0.00000903\n",
      "Evaluation Loss: 0.00000886\n",
      "Epoch [690/2500],\n",
      " Training Loss: 0.00002218\n",
      "Evaluation Loss: 0.00000513\n",
      "Epoch [691/2500],\n",
      " Training Loss: 0.00001527\n",
      "Evaluation Loss: 0.00002273\n",
      "Epoch [692/2500],\n",
      " Training Loss: 0.00003826\n",
      "Evaluation Loss: 0.00007448\n",
      "Epoch [693/2500],\n",
      " Training Loss: 0.00003632\n",
      "Evaluation Loss: 0.00002967\n",
      "Epoch [694/2500],\n",
      " Training Loss: 0.00013421\n",
      "Evaluation Loss: 0.00004523\n",
      "Epoch [695/2500],\n",
      " Training Loss: 0.00008661\n",
      "Evaluation Loss: 0.00004597\n",
      "Epoch [696/2500],\n",
      " Training Loss: 0.00008305\n",
      "Evaluation Loss: 0.00003218\n",
      "Epoch [697/2500],\n",
      " Training Loss: 0.00006841\n",
      "Evaluation Loss: 0.00001490\n",
      "Epoch [698/2500],\n",
      " Training Loss: 0.00004976\n",
      "Evaluation Loss: 0.00001735\n",
      "Epoch [699/2500],\n",
      " Training Loss: 0.00001957\n",
      "Evaluation Loss: 0.00000412\n",
      "Epoch [700/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000713\n",
      "Epoch [701/2500],\n",
      " Training Loss: 0.00001529\n",
      "Evaluation Loss: 0.00001418\n",
      "Epoch [702/2500],\n",
      " Training Loss: 0.00001237\n",
      "Evaluation Loss: 0.00000472\n",
      "Epoch [703/2500],\n",
      " Training Loss: 0.00000657\n",
      "Evaluation Loss: 0.00000638\n",
      "Epoch [704/2500],\n",
      " Training Loss: 0.00000468\n",
      "Evaluation Loss: 0.00000423\n",
      "Epoch [705/2500],\n",
      " Training Loss: 0.00000443\n",
      "Evaluation Loss: 0.00000238\n",
      "Epoch [706/2500],\n",
      " Training Loss: 0.00000291\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [707/2500],\n",
      " Training Loss: 0.00000240\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [708/2500],\n",
      " Training Loss: 0.00000252\n",
      "Evaluation Loss: 0.00000177\n",
      "Epoch [709/2500],\n",
      " Training Loss: 0.00000290\n",
      "Evaluation Loss: 0.00000252\n",
      "Epoch [710/2500],\n",
      " Training Loss: 0.00000852\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [711/2500],\n",
      " Training Loss: 0.00001429\n",
      "Evaluation Loss: 0.00001764\n",
      "Epoch [712/2500],\n",
      " Training Loss: 0.00003393\n",
      "Evaluation Loss: 0.00008530\n",
      "Epoch [713/2500],\n",
      " Training Loss: 0.00006767\n",
      "Evaluation Loss: 0.00002588\n",
      "Epoch [714/2500],\n",
      " Training Loss: 0.00002471\n",
      "Evaluation Loss: 0.00002346\n",
      "Epoch [715/2500],\n",
      " Training Loss: 0.00003252\n",
      "Evaluation Loss: 0.00006114\n",
      "Epoch [716/2500],\n",
      " Training Loss: 0.00002511\n",
      "Evaluation Loss: 0.00002317\n",
      "Epoch [717/2500],\n",
      " Training Loss: 0.00003272\n",
      "Evaluation Loss: 0.00001893\n",
      "Epoch [718/2500],\n",
      " Training Loss: 0.00001900\n",
      "Evaluation Loss: 0.00000724\n",
      "Epoch [719/2500],\n",
      " Training Loss: 0.00001400\n",
      "Evaluation Loss: 0.00000671\n",
      "Epoch [720/2500],\n",
      " Training Loss: 0.00000482\n",
      "Evaluation Loss: 0.00000309\n",
      "Epoch [721/2500],\n",
      " Training Loss: 0.00000490\n",
      "Evaluation Loss: 0.00000461\n",
      "Epoch [722/2500],\n",
      " Training Loss: 0.00000434\n",
      "Evaluation Loss: 0.00000357\n",
      "Epoch [723/2500],\n",
      " Training Loss: 0.00000297\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [724/2500],\n",
      " Training Loss: 0.00000346\n",
      "Evaluation Loss: 0.00000307\n",
      "Epoch [725/2500],\n",
      " Training Loss: 0.00000456\n",
      "Evaluation Loss: 0.00001095\n",
      "Epoch [726/2500],\n",
      " Training Loss: 0.00000994\n",
      "Evaluation Loss: 0.00000320\n",
      "Epoch [727/2500],\n",
      " Training Loss: 0.00001323\n",
      "Evaluation Loss: 0.00003262\n",
      "Epoch [728/2500],\n",
      " Training Loss: 0.00001650\n",
      "Evaluation Loss: 0.00001744\n",
      "Epoch [729/2500],\n",
      " Training Loss: 0.00001787\n",
      "Evaluation Loss: 0.00002296\n",
      "Epoch [730/2500],\n",
      " Training Loss: 0.00001580\n",
      "Evaluation Loss: 0.00000585\n",
      "Epoch [731/2500],\n",
      " Training Loss: 0.00000547\n",
      "Evaluation Loss: 0.00000967\n",
      "Epoch [732/2500],\n",
      " Training Loss: 0.00001385\n",
      "Evaluation Loss: 0.00000430\n",
      "Epoch [733/2500],\n",
      " Training Loss: 0.00001448\n",
      "Evaluation Loss: 0.00000675\n",
      "Epoch [734/2500],\n",
      " Training Loss: 0.00001455\n",
      "Evaluation Loss: 0.00003134\n",
      "Epoch [735/2500],\n",
      " Training Loss: 0.00003859\n",
      "Evaluation Loss: 0.00002195\n",
      "Epoch [736/2500],\n",
      " Training Loss: 0.00004653\n",
      "Evaluation Loss: 0.00002392\n",
      "Epoch [737/2500],\n",
      " Training Loss: 0.00025363\n",
      "Evaluation Loss: 0.00014615\n",
      "Epoch [738/2500],\n",
      " Training Loss: 0.00016598\n",
      "Evaluation Loss: 0.00004134\n",
      "Epoch [739/2500],\n",
      " Training Loss: 0.00008467\n",
      "Evaluation Loss: 0.00001676\n",
      "Epoch [740/2500],\n",
      " Training Loss: 0.00001383\n",
      "Evaluation Loss: 0.00000575\n",
      "Epoch [741/2500],\n",
      " Training Loss: 0.00000791\n",
      "Evaluation Loss: 0.00000990\n",
      "Epoch [742/2500],\n",
      " Training Loss: 0.00000674\n",
      "Evaluation Loss: 0.00000503\n",
      "Epoch [743/2500],\n",
      " Training Loss: 0.00000427\n",
      "Evaluation Loss: 0.00000381\n",
      "Epoch [744/2500],\n",
      " Training Loss: 0.00000335\n",
      "Evaluation Loss: 0.00000212\n",
      "Epoch [745/2500],\n",
      " Training Loss: 0.00000459\n",
      "Evaluation Loss: 0.00000204\n",
      "Epoch [746/2500],\n",
      " Training Loss: 0.00000412\n",
      "Evaluation Loss: 0.00000299\n",
      "Epoch [747/2500],\n",
      " Training Loss: 0.00000276\n",
      "Evaluation Loss: 0.00000247\n",
      "Epoch [748/2500],\n",
      " Training Loss: 0.00000395\n",
      "Evaluation Loss: 0.00000391\n",
      "Epoch [749/2500],\n",
      " Training Loss: 0.00000482\n",
      "Evaluation Loss: 0.00000679\n",
      "Epoch [750/2500],\n",
      " Training Loss: 0.00000411\n",
      "Evaluation Loss: 0.00000253\n",
      "Epoch [751/2500],\n",
      " Training Loss: 0.00000378\n",
      "Evaluation Loss: 0.00000195\n",
      "Epoch [752/2500],\n",
      " Training Loss: 0.00000505\n",
      "Evaluation Loss: 0.00000441\n",
      "Epoch [753/2500],\n",
      " Training Loss: 0.00000317\n",
      "Evaluation Loss: 0.00000323\n",
      "Epoch [754/2500],\n",
      " Training Loss: 0.00000267\n",
      "Evaluation Loss: 0.00000246\n",
      "Epoch [755/2500],\n",
      " Training Loss: 0.00000544\n",
      "Evaluation Loss: 0.00000343\n",
      "Epoch [756/2500],\n",
      " Training Loss: 0.00000377\n",
      "Evaluation Loss: 0.00000175\n",
      "Epoch [757/2500],\n",
      " Training Loss: 0.00000365\n",
      "Evaluation Loss: 0.00000295\n",
      "Epoch [758/2500],\n",
      " Training Loss: 0.00000738\n",
      "Evaluation Loss: 0.00000337\n",
      "Epoch [759/2500],\n",
      " Training Loss: 0.00001164\n",
      "Evaluation Loss: 0.00000563\n",
      "Epoch [760/2500],\n",
      " Training Loss: 0.00000827\n",
      "Evaluation Loss: 0.00000593\n",
      "Epoch [761/2500],\n",
      " Training Loss: 0.00001397\n",
      "Evaluation Loss: 0.00001280\n",
      "Epoch [762/2500],\n",
      " Training Loss: 0.00003662\n",
      "Evaluation Loss: 0.00003246\n",
      "Epoch [763/2500],\n",
      " Training Loss: 0.00002893\n",
      "Evaluation Loss: 0.00002007\n",
      "Epoch [764/2500],\n",
      " Training Loss: 0.00003020\n",
      "Evaluation Loss: 0.00002915\n",
      "Epoch [765/2500],\n",
      " Training Loss: 0.00001321\n",
      "Evaluation Loss: 0.00000837\n",
      "Epoch [766/2500],\n",
      " Training Loss: 0.00000814\n",
      "Evaluation Loss: 0.00000504\n",
      "Epoch [767/2500],\n",
      " Training Loss: 0.00000662\n",
      "Evaluation Loss: 0.00000439\n",
      "Epoch [768/2500],\n",
      " Training Loss: 0.00001422\n",
      "Evaluation Loss: 0.00002451\n",
      "Epoch [769/2500],\n",
      " Training Loss: 0.00002555\n",
      "Evaluation Loss: 0.00002057\n",
      "Epoch [770/2500],\n",
      " Training Loss: 0.00005507\n",
      "Evaluation Loss: 0.00013432\n",
      "Epoch [771/2500],\n",
      " Training Loss: 0.00017317\n",
      "Evaluation Loss: 0.00004455\n",
      "Epoch [772/2500],\n",
      " Training Loss: 0.00003523\n",
      "Evaluation Loss: 0.00001776\n",
      "Epoch [773/2500],\n",
      " Training Loss: 0.00001580\n",
      "Evaluation Loss: 0.00000710\n",
      "Epoch [774/2500],\n",
      " Training Loss: 0.00001821\n",
      "Evaluation Loss: 0.00004571\n",
      "Epoch [775/2500],\n",
      " Training Loss: 0.00003760\n",
      "Evaluation Loss: 0.00001737\n",
      "Epoch [776/2500],\n",
      " Training Loss: 0.00001423\n",
      "Evaluation Loss: 0.00000687\n",
      "Epoch [777/2500],\n",
      " Training Loss: 0.00001119\n",
      "Evaluation Loss: 0.00000423\n",
      "Epoch [778/2500],\n",
      " Training Loss: 0.00000753\n",
      "Evaluation Loss: 0.00000808\n",
      "Epoch [779/2500],\n",
      " Training Loss: 0.00000620\n",
      "Evaluation Loss: 0.00000526\n",
      "Epoch [780/2500],\n",
      " Training Loss: 0.00000482\n",
      "Evaluation Loss: 0.00000508\n",
      "Epoch [781/2500],\n",
      " Training Loss: 0.00000468\n",
      "Evaluation Loss: 0.00000602\n",
      "Epoch [782/2500],\n",
      " Training Loss: 0.00000602\n",
      "Evaluation Loss: 0.00000392\n",
      "Epoch [783/2500],\n",
      " Training Loss: 0.00000677\n",
      "Evaluation Loss: 0.00000648\n",
      "Epoch [784/2500],\n",
      " Training Loss: 0.00001860\n",
      "Evaluation Loss: 0.00001320\n",
      "Epoch [785/2500],\n",
      " Training Loss: 0.00003701\n",
      "Evaluation Loss: 0.00001658\n",
      "Epoch [786/2500],\n",
      " Training Loss: 0.00004416\n",
      "Evaluation Loss: 0.00001145\n",
      "Epoch [787/2500],\n",
      " Training Loss: 0.00003655\n",
      "Evaluation Loss: 0.00003269\n",
      "Epoch [788/2500],\n",
      " Training Loss: 0.00003947\n",
      "Evaluation Loss: 0.00001829\n",
      "Epoch [789/2500],\n",
      " Training Loss: 0.00004572\n",
      "Evaluation Loss: 0.00001151\n",
      "Epoch [790/2500],\n",
      " Training Loss: 0.00000947\n",
      "Evaluation Loss: 0.00000534\n",
      "Epoch [791/2500],\n",
      " Training Loss: 0.00001374\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [792/2500],\n",
      " Training Loss: 0.00001645\n",
      "Evaluation Loss: 0.00000891\n",
      "Epoch [793/2500],\n",
      " Training Loss: 0.00000726\n",
      "Evaluation Loss: 0.00001217\n",
      "Epoch [794/2500],\n",
      " Training Loss: 0.00000985\n",
      "Evaluation Loss: 0.00000383\n",
      "Epoch [795/2500],\n",
      " Training Loss: 0.00000496\n",
      "Evaluation Loss: 0.00000447\n",
      "Epoch [796/2500],\n",
      " Training Loss: 0.00000439\n",
      "Evaluation Loss: 0.00000409\n",
      "Epoch [797/2500],\n",
      " Training Loss: 0.00001274\n",
      "Evaluation Loss: 0.00001956\n",
      "Epoch [798/2500],\n",
      " Training Loss: 0.00001824\n",
      "Evaluation Loss: 0.00001867\n",
      "Epoch [799/2500],\n",
      " Training Loss: 0.00001677\n",
      "Evaluation Loss: 0.00000693\n",
      "Epoch [800/2500],\n",
      " Training Loss: 0.00001091\n",
      "Evaluation Loss: 0.00002839\n",
      "Epoch [801/2500],\n",
      " Training Loss: 0.00001478\n",
      "Evaluation Loss: 0.00001062\n",
      "Epoch [802/2500],\n",
      " Training Loss: 0.00000780\n",
      "Evaluation Loss: 0.00000402\n",
      "Epoch [803/2500],\n",
      " Training Loss: 0.00000996\n",
      "Evaluation Loss: 0.00001160\n",
      "Epoch [804/2500],\n",
      " Training Loss: 0.00001518\n",
      "Evaluation Loss: 0.00001251\n",
      "Epoch [805/2500],\n",
      " Training Loss: 0.00002126\n",
      "Evaluation Loss: 0.00001615\n",
      "Epoch [806/2500],\n",
      " Training Loss: 0.00004383\n",
      "Evaluation Loss: 0.00005282\n",
      "Epoch [807/2500],\n",
      " Training Loss: 0.00008561\n",
      "Evaluation Loss: 0.00003292\n",
      "Epoch [808/2500],\n",
      " Training Loss: 0.00003927\n",
      "Evaluation Loss: 0.00004045\n",
      "Epoch [809/2500],\n",
      " Training Loss: 0.00004607\n",
      "Evaluation Loss: 0.00001303\n",
      "Epoch [810/2500],\n",
      " Training Loss: 0.00002618\n",
      "Evaluation Loss: 0.00001101\n",
      "Epoch [811/2500],\n",
      " Training Loss: 0.00002188\n",
      "Evaluation Loss: 0.00001181\n",
      "Epoch [812/2500],\n",
      " Training Loss: 0.00001159\n",
      "Evaluation Loss: 0.00000622\n",
      "Epoch [813/2500],\n",
      " Training Loss: 0.00000990\n",
      "Evaluation Loss: 0.00000495\n",
      "Epoch [814/2500],\n",
      " Training Loss: 0.00000878\n",
      "Evaluation Loss: 0.00000974\n",
      "Epoch [815/2500],\n",
      " Training Loss: 0.00000688\n",
      "Evaluation Loss: 0.00000238\n",
      "Epoch [816/2500],\n",
      " Training Loss: 0.00000258\n",
      "Evaluation Loss: 0.00000248\n",
      "Epoch [817/2500],\n",
      " Training Loss: 0.00000943\n",
      "Evaluation Loss: 0.00001410\n",
      "Epoch [818/2500],\n",
      " Training Loss: 0.00000836\n",
      "Evaluation Loss: 0.00000478\n",
      "Epoch [819/2500],\n",
      " Training Loss: 0.00000622\n",
      "Evaluation Loss: 0.00000212\n",
      "Epoch [820/2500],\n",
      " Training Loss: 0.00000499\n",
      "Evaluation Loss: 0.00000207\n",
      "Epoch [821/2500],\n",
      " Training Loss: 0.00000365\n",
      "Evaluation Loss: 0.00000572\n",
      "Epoch [822/2500],\n",
      " Training Loss: 0.00000882\n",
      "Evaluation Loss: 0.00000956\n",
      "Epoch [823/2500],\n",
      " Training Loss: 0.00000711\n",
      "Evaluation Loss: 0.00000463\n",
      "Epoch [824/2500],\n",
      " Training Loss: 0.00000460\n",
      "Evaluation Loss: 0.00000427\n",
      "Epoch [825/2500],\n",
      " Training Loss: 0.00000529\n",
      "Evaluation Loss: 0.00000253\n",
      "Epoch [826/2500],\n",
      " Training Loss: 0.00000450\n",
      "Evaluation Loss: 0.00000351\n",
      "Epoch [827/2500],\n",
      " Training Loss: 0.00001032\n",
      "Evaluation Loss: 0.00000433\n",
      "Epoch [828/2500],\n",
      " Training Loss: 0.00000833\n",
      "Evaluation Loss: 0.00000924\n",
      "Epoch [829/2500],\n",
      " Training Loss: 0.00003949\n",
      "Evaluation Loss: 0.00001173\n",
      "Epoch [830/2500],\n",
      " Training Loss: 0.00003945\n",
      "Evaluation Loss: 0.00004728\n",
      "Epoch [831/2500],\n",
      " Training Loss: 0.00015270\n",
      "Evaluation Loss: 0.00004976\n",
      "Epoch [832/2500],\n",
      " Training Loss: 0.00014190\n",
      "Evaluation Loss: 0.00008737\n",
      "Epoch [833/2500],\n",
      " Training Loss: 0.00002665\n",
      "Evaluation Loss: 0.00000786\n",
      "Epoch [834/2500],\n",
      " Training Loss: 0.00002002\n",
      "Evaluation Loss: 0.00000879\n",
      "Epoch [835/2500],\n",
      " Training Loss: 0.00000878\n",
      "Evaluation Loss: 0.00000651\n",
      "Epoch [836/2500],\n",
      " Training Loss: 0.00000524\n",
      "Evaluation Loss: 0.00000397\n",
      "Epoch [837/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000427\n",
      "Epoch [838/2500],\n",
      " Training Loss: 0.00000545\n",
      "Evaluation Loss: 0.00000379\n",
      "Epoch [839/2500],\n",
      " Training Loss: 0.00000735\n",
      "Evaluation Loss: 0.00001376\n",
      "Epoch [840/2500],\n",
      " Training Loss: 0.00001084\n",
      "Evaluation Loss: 0.00000544\n",
      "Epoch [841/2500],\n",
      " Training Loss: 0.00000740\n",
      "Evaluation Loss: 0.00001186\n",
      "Epoch [842/2500],\n",
      " Training Loss: 0.00000475\n",
      "Evaluation Loss: 0.00000313\n",
      "Epoch [843/2500],\n",
      " Training Loss: 0.00000439\n",
      "Evaluation Loss: 0.00000183\n",
      "Epoch [844/2500],\n",
      " Training Loss: 0.00000355\n",
      "Evaluation Loss: 0.00000494\n",
      "Epoch [845/2500],\n",
      " Training Loss: 0.00000560\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [846/2500],\n",
      " Training Loss: 0.00001306\n",
      "Evaluation Loss: 0.00001105\n",
      "Epoch [847/2500],\n",
      " Training Loss: 0.00001137\n",
      "Evaluation Loss: 0.00000843\n",
      "Epoch [848/2500],\n",
      " Training Loss: 0.00001929\n",
      "Evaluation Loss: 0.00001567\n",
      "Epoch [849/2500],\n",
      " Training Loss: 0.00001625\n",
      "Evaluation Loss: 0.00001987\n",
      "Epoch [850/2500],\n",
      " Training Loss: 0.00002009\n",
      "Evaluation Loss: 0.00002063\n",
      "Epoch [851/2500],\n",
      " Training Loss: 0.00010525\n",
      "Evaluation Loss: 0.00014569\n",
      "Epoch [852/2500],\n",
      " Training Loss: 0.00009618\n",
      "Evaluation Loss: 0.00002661\n",
      "Epoch [853/2500],\n",
      " Training Loss: 0.00002875\n",
      "Evaluation Loss: 0.00005480\n",
      "Epoch [854/2500],\n",
      " Training Loss: 0.00003565\n",
      "Evaluation Loss: 0.00002461\n",
      "Epoch [855/2500],\n",
      " Training Loss: 0.00002371\n",
      "Evaluation Loss: 0.00001131\n",
      "Epoch [856/2500],\n",
      " Training Loss: 0.00002077\n",
      "Evaluation Loss: 0.00000748\n",
      "Epoch [857/2500],\n",
      " Training Loss: 0.00000984\n",
      "Evaluation Loss: 0.00000835\n",
      "Epoch [858/2500],\n",
      " Training Loss: 0.00001075\n",
      "Evaluation Loss: 0.00001630\n",
      "Epoch [859/2500],\n",
      " Training Loss: 0.00000846\n",
      "Evaluation Loss: 0.00000444\n",
      "Epoch [860/2500],\n",
      " Training Loss: 0.00000438\n",
      "Evaluation Loss: 0.00000338\n",
      "Epoch [861/2500],\n",
      " Training Loss: 0.00000331\n",
      "Evaluation Loss: 0.00000316\n",
      "Epoch [862/2500],\n",
      " Training Loss: 0.00000313\n",
      "Evaluation Loss: 0.00000304\n",
      "Epoch [863/2500],\n",
      " Training Loss: 0.00000215\n",
      "Evaluation Loss: 0.00000197\n",
      "Epoch [864/2500],\n",
      " Training Loss: 0.00000379\n",
      "Evaluation Loss: 0.00000249\n",
      "Epoch [865/2500],\n",
      " Training Loss: 0.00000304\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [866/2500],\n",
      " Training Loss: 0.00000376\n",
      "Evaluation Loss: 0.00000639\n",
      "Epoch [867/2500],\n",
      " Training Loss: 0.00000375\n",
      "Evaluation Loss: 0.00000174\n",
      "Epoch [868/2500],\n",
      " Training Loss: 0.00000585\n",
      "Evaluation Loss: 0.00000565\n",
      "Epoch [869/2500],\n",
      " Training Loss: 0.00000750\n",
      "Evaluation Loss: 0.00001708\n",
      "Epoch [870/2500],\n",
      " Training Loss: 0.00002249\n",
      "Evaluation Loss: 0.00002147\n",
      "Epoch [871/2500],\n",
      " Training Loss: 0.00003824\n",
      "Evaluation Loss: 0.00001784\n",
      "Epoch [872/2500],\n",
      " Training Loss: 0.00006602\n",
      "Evaluation Loss: 0.00006019\n",
      "Epoch [873/2500],\n",
      " Training Loss: 0.00003225\n",
      "Evaluation Loss: 0.00001703\n",
      "Epoch [874/2500],\n",
      " Training Loss: 0.00002143\n",
      "Evaluation Loss: 0.00000875\n",
      "Epoch [875/2500],\n",
      " Training Loss: 0.00001202\n",
      "Evaluation Loss: 0.00000838\n",
      "Epoch [876/2500],\n",
      " Training Loss: 0.00000443\n",
      "Evaluation Loss: 0.00000491\n",
      "Epoch [877/2500],\n",
      " Training Loss: 0.00000280\n",
      "Evaluation Loss: 0.00000253\n",
      "Epoch [878/2500],\n",
      " Training Loss: 0.00000420\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [879/2500],\n",
      " Training Loss: 0.00000786\n",
      "Evaluation Loss: 0.00000864\n",
      "Epoch [880/2500],\n",
      " Training Loss: 0.00000847\n",
      "Evaluation Loss: 0.00001125\n",
      "Epoch [881/2500],\n",
      " Training Loss: 0.00000655\n",
      "Evaluation Loss: 0.00000604\n",
      "Epoch [882/2500],\n",
      " Training Loss: 0.00000671\n",
      "Evaluation Loss: 0.00000605\n",
      "Epoch [883/2500],\n",
      " Training Loss: 0.00000601\n",
      "Evaluation Loss: 0.00000598\n",
      "Epoch [884/2500],\n",
      " Training Loss: 0.00000407\n",
      "Evaluation Loss: 0.00000566\n",
      "Epoch [885/2500],\n",
      " Training Loss: 0.00000548\n",
      "Evaluation Loss: 0.00000396\n",
      "Epoch [886/2500],\n",
      " Training Loss: 0.00000432\n",
      "Evaluation Loss: 0.00000615\n",
      "Epoch [887/2500],\n",
      " Training Loss: 0.00000540\n",
      "Evaluation Loss: 0.00001028\n",
      "Epoch [888/2500],\n",
      " Training Loss: 0.00000486\n",
      "Evaluation Loss: 0.00000452\n",
      "Epoch [889/2500],\n",
      " Training Loss: 0.00000381\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [890/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000264\n",
      "Epoch [891/2500],\n",
      " Training Loss: 0.00000288\n",
      "Evaluation Loss: 0.00000838\n",
      "Epoch [892/2500],\n",
      " Training Loss: 0.00000266\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [893/2500],\n",
      " Training Loss: 0.00000267\n",
      "Evaluation Loss: 0.00000146\n",
      "Epoch [894/2500],\n",
      " Training Loss: 0.00000288\n",
      "Evaluation Loss: 0.00000180\n",
      "Epoch [895/2500],\n",
      " Training Loss: 0.00000227\n",
      "Evaluation Loss: 0.00000099\n",
      "Epoch [896/2500],\n",
      " Training Loss: 0.00000178\n",
      "Evaluation Loss: 0.00000264\n",
      "Epoch [897/2500],\n",
      " Training Loss: 0.00000388\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [898/2500],\n",
      " Training Loss: 0.00000747\n",
      "Evaluation Loss: 0.00000927\n",
      "Epoch [899/2500],\n",
      " Training Loss: 0.00008305\n",
      "Evaluation Loss: 0.00003498\n",
      "Epoch [900/2500],\n",
      " Training Loss: 0.00002479\n",
      "Evaluation Loss: 0.00001295\n",
      "Epoch [901/2500],\n",
      " Training Loss: 0.00001173\n",
      "Evaluation Loss: 0.00000332\n",
      "Epoch [902/2500],\n",
      " Training Loss: 0.00000723\n",
      "Evaluation Loss: 0.00000509\n",
      "Epoch [903/2500],\n",
      " Training Loss: 0.00000451\n",
      "Evaluation Loss: 0.00000932\n",
      "Epoch [904/2500],\n",
      " Training Loss: 0.00000893\n",
      "Evaluation Loss: 0.00000719\n",
      "Epoch [905/2500],\n",
      " Training Loss: 0.00002334\n",
      "Evaluation Loss: 0.00000873\n",
      "Epoch [906/2500],\n",
      " Training Loss: 0.00002171\n",
      "Evaluation Loss: 0.00000804\n",
      "Epoch [907/2500],\n",
      " Training Loss: 0.00002119\n",
      "Evaluation Loss: 0.00000811\n",
      "Epoch [908/2500],\n",
      " Training Loss: 0.00001961\n",
      "Evaluation Loss: 0.00001027\n",
      "Epoch [909/2500],\n",
      " Training Loss: 0.00003029\n",
      "Evaluation Loss: 0.00002426\n",
      "Epoch [910/2500],\n",
      " Training Loss: 0.00020871\n",
      "Evaluation Loss: 0.00004737\n",
      "Epoch [911/2500],\n",
      " Training Loss: 0.00009262\n",
      "Evaluation Loss: 0.00003787\n",
      "Epoch [912/2500],\n",
      " Training Loss: 0.00012627\n",
      "Evaluation Loss: 0.00006969\n",
      "Epoch [913/2500],\n",
      " Training Loss: 0.00004759\n",
      "Evaluation Loss: 0.00003038\n",
      "Epoch [914/2500],\n",
      " Training Loss: 0.00001410\n",
      "Evaluation Loss: 0.00000404\n",
      "Epoch [915/2500],\n",
      " Training Loss: 0.00000998\n",
      "Evaluation Loss: 0.00000494\n",
      "Epoch [916/2500],\n",
      " Training Loss: 0.00000736\n",
      "Evaluation Loss: 0.00000557\n",
      "Epoch [917/2500],\n",
      " Training Loss: 0.00001354\n",
      "Evaluation Loss: 0.00000505\n",
      "Epoch [918/2500],\n",
      " Training Loss: 0.00000884\n",
      "Evaluation Loss: 0.00000873\n",
      "Epoch [919/2500],\n",
      " Training Loss: 0.00001563\n",
      "Evaluation Loss: 0.00004579\n",
      "Epoch [920/2500],\n",
      " Training Loss: 0.00001388\n",
      "Evaluation Loss: 0.00000437\n",
      "Epoch [921/2500],\n",
      " Training Loss: 0.00000755\n",
      "Evaluation Loss: 0.00000678\n",
      "Epoch [922/2500],\n",
      " Training Loss: 0.00000472\n",
      "Evaluation Loss: 0.00000306\n",
      "Epoch [923/2500],\n",
      " Training Loss: 0.00000395\n",
      "Evaluation Loss: 0.00000233\n",
      "Epoch [924/2500],\n",
      " Training Loss: 0.00000445\n",
      "Evaluation Loss: 0.00000255\n",
      "Epoch [925/2500],\n",
      " Training Loss: 0.00000392\n",
      "Evaluation Loss: 0.00000408\n",
      "Epoch [926/2500],\n",
      " Training Loss: 0.00000293\n",
      "Evaluation Loss: 0.00000237\n",
      "Epoch [927/2500],\n",
      " Training Loss: 0.00000196\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [928/2500],\n",
      " Training Loss: 0.00000287\n",
      "Evaluation Loss: 0.00000176\n",
      "Epoch [929/2500],\n",
      " Training Loss: 0.00000302\n",
      "Evaluation Loss: 0.00000216\n",
      "Epoch [930/2500],\n",
      " Training Loss: 0.00000241\n",
      "Evaluation Loss: 0.00000108\n",
      "Epoch [931/2500],\n",
      " Training Loss: 0.00000284\n",
      "Evaluation Loss: 0.00000228\n",
      "Epoch [932/2500],\n",
      " Training Loss: 0.00000664\n",
      "Evaluation Loss: 0.00000715\n",
      "Epoch [933/2500],\n",
      " Training Loss: 0.00002369\n",
      "Evaluation Loss: 0.00002743\n",
      "Epoch [934/2500],\n",
      " Training Loss: 0.00001994\n",
      "Evaluation Loss: 0.00001340\n",
      "Epoch [935/2500],\n",
      " Training Loss: 0.00001111\n",
      "Evaluation Loss: 0.00000339\n",
      "Epoch [936/2500],\n",
      " Training Loss: 0.00000555\n",
      "Evaluation Loss: 0.00000546\n",
      "Epoch [937/2500],\n",
      " Training Loss: 0.00000520\n",
      "Evaluation Loss: 0.00000306\n",
      "Epoch [938/2500],\n",
      " Training Loss: 0.00000446\n",
      "Evaluation Loss: 0.00000600\n",
      "Epoch [939/2500],\n",
      " Training Loss: 0.00000672\n",
      "Evaluation Loss: 0.00000684\n",
      "Epoch [940/2500],\n",
      " Training Loss: 0.00000527\n",
      "Evaluation Loss: 0.00000456\n",
      "Epoch [941/2500],\n",
      " Training Loss: 0.00000306\n",
      "Evaluation Loss: 0.00000197\n",
      "Epoch [942/2500],\n",
      " Training Loss: 0.00000291\n",
      "Evaluation Loss: 0.00000149\n",
      "Epoch [943/2500],\n",
      " Training Loss: 0.00000270\n",
      "Evaluation Loss: 0.00000337\n",
      "Epoch [944/2500],\n",
      " Training Loss: 0.00000573\n",
      "Evaluation Loss: 0.00000644\n",
      "Epoch [945/2500],\n",
      " Training Loss: 0.00002674\n",
      "Evaluation Loss: 0.00006743\n",
      "Epoch [946/2500],\n",
      " Training Loss: 0.00002558\n",
      "Evaluation Loss: 0.00001090\n",
      "Epoch [947/2500],\n",
      " Training Loss: 0.00003580\n",
      "Evaluation Loss: 0.00009396\n",
      "Epoch [948/2500],\n",
      " Training Loss: 0.00005322\n",
      "Evaluation Loss: 0.00002318\n",
      "Epoch [949/2500],\n",
      " Training Loss: 0.00002809\n",
      "Evaluation Loss: 0.00004793\n",
      "Epoch [950/2500],\n",
      " Training Loss: 0.00003730\n",
      "Evaluation Loss: 0.00000908\n",
      "Epoch [951/2500],\n",
      " Training Loss: 0.00001466\n",
      "Evaluation Loss: 0.00002089\n",
      "Epoch [952/2500],\n",
      " Training Loss: 0.00001700\n",
      "Evaluation Loss: 0.00000610\n",
      "Epoch [953/2500],\n",
      " Training Loss: 0.00000933\n",
      "Evaluation Loss: 0.00000460\n",
      "Epoch [954/2500],\n",
      " Training Loss: 0.00001449\n",
      "Evaluation Loss: 0.00001435\n",
      "Epoch [955/2500],\n",
      " Training Loss: 0.00001134\n",
      "Evaluation Loss: 0.00000314\n",
      "Epoch [956/2500],\n",
      " Training Loss: 0.00000710\n",
      "Evaluation Loss: 0.00001359\n",
      "Epoch [957/2500],\n",
      " Training Loss: 0.00000667\n",
      "Evaluation Loss: 0.00000441\n",
      "Epoch [958/2500],\n",
      " Training Loss: 0.00000665\n",
      "Evaluation Loss: 0.00001645\n",
      "Epoch [959/2500],\n",
      " Training Loss: 0.00000799\n",
      "Evaluation Loss: 0.00000846\n",
      "Epoch [960/2500],\n",
      " Training Loss: 0.00000538\n",
      "Evaluation Loss: 0.00000225\n",
      "Epoch [961/2500],\n",
      " Training Loss: 0.00001183\n",
      "Evaluation Loss: 0.00000716\n",
      "Epoch [962/2500],\n",
      " Training Loss: 0.00001263\n",
      "Evaluation Loss: 0.00000574\n",
      "Epoch [963/2500],\n",
      " Training Loss: 0.00001415\n",
      "Evaluation Loss: 0.00000405\n",
      "Epoch [964/2500],\n",
      " Training Loss: 0.00000623\n",
      "Evaluation Loss: 0.00000398\n",
      "Epoch [965/2500],\n",
      " Training Loss: 0.00000447\n",
      "Evaluation Loss: 0.00000774\n",
      "Epoch [966/2500],\n",
      " Training Loss: 0.00000359\n",
      "Evaluation Loss: 0.00000313\n",
      "Epoch [967/2500],\n",
      " Training Loss: 0.00000516\n",
      "Evaluation Loss: 0.00000349\n",
      "Epoch [968/2500],\n",
      " Training Loss: 0.00001671\n",
      "Evaluation Loss: 0.00001072\n",
      "Epoch [969/2500],\n",
      " Training Loss: 0.00001102\n",
      "Evaluation Loss: 0.00000857\n",
      "Epoch [970/2500],\n",
      " Training Loss: 0.00002551\n",
      "Evaluation Loss: 0.00002135\n",
      "Epoch [971/2500],\n",
      " Training Loss: 0.00004232\n",
      "Evaluation Loss: 0.00001662\n",
      "Epoch [972/2500],\n",
      " Training Loss: 0.00011821\n",
      "Evaluation Loss: 0.00002094\n",
      "Epoch [973/2500],\n",
      " Training Loss: 0.00002736\n",
      "Evaluation Loss: 0.00007267\n",
      "Epoch [974/2500],\n",
      " Training Loss: 0.00008206\n",
      "Evaluation Loss: 0.00002993\n",
      "Epoch [975/2500],\n",
      " Training Loss: 0.00004510\n",
      "Evaluation Loss: 0.00000902\n",
      "Epoch [976/2500],\n",
      " Training Loss: 0.00004970\n",
      "Evaluation Loss: 0.00001434\n",
      "Epoch [977/2500],\n",
      " Training Loss: 0.00003923\n",
      "Evaluation Loss: 0.00004521\n",
      "Epoch [978/2500],\n",
      " Training Loss: 0.00002811\n",
      "Evaluation Loss: 0.00002045\n",
      "Epoch [979/2500],\n",
      " Training Loss: 0.00001474\n",
      "Evaluation Loss: 0.00001939\n",
      "Epoch [980/2500],\n",
      " Training Loss: 0.00000931\n",
      "Evaluation Loss: 0.00000763\n",
      "Epoch [981/2500],\n",
      " Training Loss: 0.00000893\n",
      "Evaluation Loss: 0.00000349\n",
      "Epoch [982/2500],\n",
      " Training Loss: 0.00000394\n",
      "Evaluation Loss: 0.00000367\n",
      "Epoch [983/2500],\n",
      " Training Loss: 0.00000490\n",
      "Evaluation Loss: 0.00000499\n",
      "Epoch [984/2500],\n",
      " Training Loss: 0.00000728\n",
      "Evaluation Loss: 0.00000518\n",
      "Epoch [985/2500],\n",
      " Training Loss: 0.00000935\n",
      "Evaluation Loss: 0.00000911\n",
      "Epoch [986/2500],\n",
      " Training Loss: 0.00000543\n",
      "Evaluation Loss: 0.00000233\n",
      "Epoch [987/2500],\n",
      " Training Loss: 0.00000997\n",
      "Evaluation Loss: 0.00000798\n",
      "Epoch [988/2500],\n",
      " Training Loss: 0.00001342\n",
      "Evaluation Loss: 0.00000531\n",
      "Epoch [989/2500],\n",
      " Training Loss: 0.00001617\n",
      "Evaluation Loss: 0.00001052\n",
      "Epoch [990/2500],\n",
      " Training Loss: 0.00003450\n",
      "Evaluation Loss: 0.00001607\n",
      "Epoch [991/2500],\n",
      " Training Loss: 0.00001500\n",
      "Evaluation Loss: 0.00000362\n",
      "Epoch [992/2500],\n",
      " Training Loss: 0.00001188\n",
      "Evaluation Loss: 0.00002333\n",
      "Epoch [993/2500],\n",
      " Training Loss: 0.00001191\n",
      "Evaluation Loss: 0.00001433\n",
      "Epoch [994/2500],\n",
      " Training Loss: 0.00001211\n",
      "Evaluation Loss: 0.00000595\n",
      "Epoch [995/2500],\n",
      " Training Loss: 0.00000694\n",
      "Evaluation Loss: 0.00000700\n",
      "Epoch [996/2500],\n",
      " Training Loss: 0.00000902\n",
      "Evaluation Loss: 0.00000676\n",
      "Epoch [997/2500],\n",
      " Training Loss: 0.00001093\n",
      "Evaluation Loss: 0.00000619\n",
      "Epoch [998/2500],\n",
      " Training Loss: 0.00003317\n",
      "Evaluation Loss: 0.00001370\n",
      "Epoch [999/2500],\n",
      " Training Loss: 0.00002652\n",
      "Evaluation Loss: 0.00001568\n",
      "Epoch [1000/2500],\n",
      " Training Loss: 0.00002493\n",
      "Evaluation Loss: 0.00001101\n",
      "Epoch [1001/2500],\n",
      " Training Loss: 0.00001784\n",
      "Evaluation Loss: 0.00000877\n",
      "Epoch [1002/2500],\n",
      " Training Loss: 0.00000705\n",
      "Evaluation Loss: 0.00000237\n",
      "Epoch [1003/2500],\n",
      " Training Loss: 0.00000487\n",
      "Evaluation Loss: 0.00000353\n",
      "Epoch [1004/2500],\n",
      " Training Loss: 0.00000579\n",
      "Evaluation Loss: 0.00000258\n",
      "Epoch [1005/2500],\n",
      " Training Loss: 0.00000406\n",
      "Evaluation Loss: 0.00000393\n",
      "Epoch [1006/2500],\n",
      " Training Loss: 0.00000589\n",
      "Evaluation Loss: 0.00001045\n",
      "Epoch [1007/2500],\n",
      " Training Loss: 0.00001329\n",
      "Evaluation Loss: 0.00000662\n",
      "Epoch [1008/2500],\n",
      " Training Loss: 0.00001259\n",
      "Evaluation Loss: 0.00001615\n",
      "Epoch [1009/2500],\n",
      " Training Loss: 0.00000991\n",
      "Evaluation Loss: 0.00000627\n",
      "Epoch [1010/2500],\n",
      " Training Loss: 0.00000628\n",
      "Evaluation Loss: 0.00000433\n",
      "Epoch [1011/2500],\n",
      " Training Loss: 0.00000420\n",
      "Evaluation Loss: 0.00000211\n",
      "Epoch [1012/2500],\n",
      " Training Loss: 0.00000235\n",
      "Evaluation Loss: 0.00000277\n",
      "Epoch [1013/2500],\n",
      " Training Loss: 0.00000366\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [1014/2500],\n",
      " Training Loss: 0.00000158\n",
      "Evaluation Loss: 0.00000112\n",
      "Epoch [1015/2500],\n",
      " Training Loss: 0.00000128\n",
      "Evaluation Loss: 0.00000173\n",
      "Epoch [1016/2500],\n",
      " Training Loss: 0.00000169\n",
      "Evaluation Loss: 0.00000222\n",
      "Epoch [1017/2500],\n",
      " Training Loss: 0.00000263\n",
      "Evaluation Loss: 0.00000145\n",
      "Epoch [1018/2500],\n",
      " Training Loss: 0.00000186\n",
      "Evaluation Loss: 0.00000214\n",
      "Epoch [1019/2500],\n",
      " Training Loss: 0.00000434\n",
      "Evaluation Loss: 0.00000317\n",
      "Epoch [1020/2500],\n",
      " Training Loss: 0.00000569\n",
      "Evaluation Loss: 0.00000462\n",
      "Epoch [1021/2500],\n",
      " Training Loss: 0.00000310\n",
      "Evaluation Loss: 0.00000109\n",
      "Epoch [1022/2500],\n",
      " Training Loss: 0.00000457\n",
      "Evaluation Loss: 0.00000575\n",
      "Epoch [1023/2500],\n",
      " Training Loss: 0.00000808\n",
      "Evaluation Loss: 0.00001123\n",
      "Epoch [1024/2500],\n",
      " Training Loss: 0.00001968\n",
      "Evaluation Loss: 0.00001990\n",
      "Epoch [1025/2500],\n",
      " Training Loss: 0.00003973\n",
      "Evaluation Loss: 0.00007213\n",
      "Epoch [1026/2500],\n",
      " Training Loss: 0.00003735\n",
      "Evaluation Loss: 0.00001389\n",
      "Epoch [1027/2500],\n",
      " Training Loss: 0.00001577\n",
      "Evaluation Loss: 0.00001700\n",
      "Epoch [1028/2500],\n",
      " Training Loss: 0.00000885\n",
      "Evaluation Loss: 0.00000711\n",
      "Epoch [1029/2500],\n",
      " Training Loss: 0.00000721\n",
      "Evaluation Loss: 0.00000328\n",
      "Epoch [1030/2500],\n",
      " Training Loss: 0.00000338\n",
      "Evaluation Loss: 0.00000575\n",
      "Epoch [1031/2500],\n",
      " Training Loss: 0.00000512\n",
      "Evaluation Loss: 0.00001150\n",
      "Epoch [1032/2500],\n",
      " Training Loss: 0.00000808\n",
      "Evaluation Loss: 0.00001041\n",
      "Epoch [1033/2500],\n",
      " Training Loss: 0.00001317\n",
      "Evaluation Loss: 0.00000435\n",
      "Epoch [1034/2500],\n",
      " Training Loss: 0.00000630\n",
      "Evaluation Loss: 0.00000791\n",
      "Epoch [1035/2500],\n",
      " Training Loss: 0.00000445\n",
      "Evaluation Loss: 0.00000497\n",
      "Epoch [1036/2500],\n",
      " Training Loss: 0.00000578\n",
      "Evaluation Loss: 0.00000303\n",
      "Epoch [1037/2500],\n",
      " Training Loss: 0.00000440\n",
      "Evaluation Loss: 0.00000458\n",
      "Epoch [1038/2500],\n",
      " Training Loss: 0.00000380\n",
      "Evaluation Loss: 0.00000234\n",
      "Epoch [1039/2500],\n",
      " Training Loss: 0.00001477\n",
      "Evaluation Loss: 0.00001195\n",
      "Epoch [1040/2500],\n",
      " Training Loss: 0.00000780\n",
      "Evaluation Loss: 0.00000817\n",
      "Epoch [1041/2500],\n",
      " Training Loss: 0.00001271\n",
      "Evaluation Loss: 0.00000980\n",
      "Epoch [1042/2500],\n",
      " Training Loss: 0.00000777\n",
      "Evaluation Loss: 0.00000499\n",
      "Epoch [1043/2500],\n",
      " Training Loss: 0.00001171\n",
      "Evaluation Loss: 0.00001045\n",
      "Epoch [1044/2500],\n",
      " Training Loss: 0.00005404\n",
      "Evaluation Loss: 0.00008238\n",
      "Epoch [1045/2500],\n",
      " Training Loss: 0.00010179\n",
      "Evaluation Loss: 0.00005404\n",
      "Epoch [1046/2500],\n",
      " Training Loss: 0.00015687\n",
      "Evaluation Loss: 0.00019696\n",
      "Epoch [1047/2500],\n",
      " Training Loss: 0.00009982\n",
      "Evaluation Loss: 0.00004320\n",
      "Epoch [1048/2500],\n",
      " Training Loss: 0.00004301\n",
      "Evaluation Loss: 0.00001656\n",
      "Epoch [1049/2500],\n",
      " Training Loss: 0.00003323\n",
      "Evaluation Loss: 0.00003717\n",
      "Epoch [1050/2500],\n",
      " Training Loss: 0.00001779\n",
      "Evaluation Loss: 0.00000941\n",
      "Epoch [1051/2500],\n",
      " Training Loss: 0.00000750\n",
      "Evaluation Loss: 0.00000428\n",
      "Epoch [1052/2500],\n",
      " Training Loss: 0.00000555\n",
      "Evaluation Loss: 0.00000148\n",
      "Epoch [1053/2500],\n",
      " Training Loss: 0.00000563\n",
      "Evaluation Loss: 0.00000631\n",
      "Epoch [1054/2500],\n",
      " Training Loss: 0.00000831\n",
      "Evaluation Loss: 0.00000518\n",
      "Epoch [1055/2500],\n",
      " Training Loss: 0.00000686\n",
      "Evaluation Loss: 0.00000624\n",
      "Epoch [1056/2500],\n",
      " Training Loss: 0.00000256\n",
      "Evaluation Loss: 0.00000093\n",
      "Epoch [1057/2500],\n",
      " Training Loss: 0.00000154\n",
      "Evaluation Loss: 0.00000202\n",
      "Epoch [1058/2500],\n",
      " Training Loss: 0.00000136\n",
      "Evaluation Loss: 0.00000133\n",
      "Epoch [1059/2500],\n",
      " Training Loss: 0.00000189\n",
      "Evaluation Loss: 0.00000248\n",
      "Epoch [1060/2500],\n",
      " Training Loss: 0.00000188\n",
      "Evaluation Loss: 0.00000093\n",
      "Epoch [1061/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000276\n",
      "Epoch [1062/2500],\n",
      " Training Loss: 0.00000385\n",
      "Evaluation Loss: 0.00000941\n",
      "Epoch [1063/2500],\n",
      " Training Loss: 0.00000431\n",
      "Evaluation Loss: 0.00001016\n",
      "Epoch [1064/2500],\n",
      " Training Loss: 0.00000353\n",
      "Evaluation Loss: 0.00000131\n",
      "Epoch [1065/2500],\n",
      " Training Loss: 0.00000542\n",
      "Evaluation Loss: 0.00000255\n",
      "Epoch [1066/2500],\n",
      " Training Loss: 0.00000390\n",
      "Evaluation Loss: 0.00000324\n",
      "Epoch [1067/2500],\n",
      " Training Loss: 0.00001103\n",
      "Evaluation Loss: 0.00000679\n",
      "Epoch [1068/2500],\n",
      " Training Loss: 0.00001662\n",
      "Evaluation Loss: 0.00000626\n",
      "Epoch [1069/2500],\n",
      " Training Loss: 0.00002666\n",
      "Evaluation Loss: 0.00000977\n",
      "Epoch [1070/2500],\n",
      " Training Loss: 0.00002731\n",
      "Evaluation Loss: 0.00001291\n",
      "Epoch [1071/2500],\n",
      " Training Loss: 0.00002635\n",
      "Evaluation Loss: 0.00001885\n",
      "Epoch [1072/2500],\n",
      " Training Loss: 0.00002343\n",
      "Evaluation Loss: 0.00000648\n",
      "Epoch [1073/2500],\n",
      " Training Loss: 0.00001919\n",
      "Evaluation Loss: 0.00003164\n",
      "Epoch [1074/2500],\n",
      " Training Loss: 0.00003335\n",
      "Evaluation Loss: 0.00001842\n",
      "Epoch [1075/2500],\n",
      " Training Loss: 0.00001663\n",
      "Evaluation Loss: 0.00003526\n",
      "Epoch [1076/2500],\n",
      " Training Loss: 0.00001681\n",
      "Evaluation Loss: 0.00000289\n",
      "Epoch [1077/2500],\n",
      " Training Loss: 0.00000533\n",
      "Evaluation Loss: 0.00000366\n",
      "Epoch [1078/2500],\n",
      " Training Loss: 0.00000411\n",
      "Evaluation Loss: 0.00000732\n",
      "Epoch [1079/2500],\n",
      " Training Loss: 0.00000272\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [1080/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000228\n",
      "Epoch [1081/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000116\n",
      "Epoch [1082/2500],\n",
      " Training Loss: 0.00000183\n",
      "Evaluation Loss: 0.00000259\n",
      "Epoch [1083/2500],\n",
      " Training Loss: 0.00000222\n",
      "Evaluation Loss: 0.00000350\n",
      "Epoch [1084/2500],\n",
      " Training Loss: 0.00000338\n",
      "Evaluation Loss: 0.00000742\n",
      "Epoch [1085/2500],\n",
      " Training Loss: 0.00000454\n",
      "Evaluation Loss: 0.00000652\n",
      "Epoch [1086/2500],\n",
      " Training Loss: 0.00000435\n",
      "Evaluation Loss: 0.00000338\n",
      "Epoch [1087/2500],\n",
      " Training Loss: 0.00000230\n",
      "Evaluation Loss: 0.00000157\n",
      "Epoch [1088/2500],\n",
      " Training Loss: 0.00000256\n",
      "Evaluation Loss: 0.00000171\n",
      "Epoch [1089/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [1090/2500],\n",
      " Training Loss: 0.00000339\n",
      "Evaluation Loss: 0.00000237\n",
      "Epoch [1091/2500],\n",
      " Training Loss: 0.00000753\n",
      "Evaluation Loss: 0.00000403\n",
      "Epoch [1092/2500],\n",
      " Training Loss: 0.00000808\n",
      "Evaluation Loss: 0.00000597\n",
      "Epoch [1093/2500],\n",
      " Training Loss: 0.00000866\n",
      "Evaluation Loss: 0.00000631\n",
      "Epoch [1094/2500],\n",
      " Training Loss: 0.00000885\n",
      "Evaluation Loss: 0.00001430\n",
      "Epoch [1095/2500],\n",
      " Training Loss: 0.00001747\n",
      "Evaluation Loss: 0.00001328\n",
      "Epoch [1096/2500],\n",
      " Training Loss: 0.00003974\n",
      "Evaluation Loss: 0.00003177\n",
      "Epoch [1097/2500],\n",
      " Training Loss: 0.00003545\n",
      "Evaluation Loss: 0.00001585\n",
      "Epoch [1098/2500],\n",
      " Training Loss: 0.00001868\n",
      "Evaluation Loss: 0.00001191\n",
      "Epoch [1099/2500],\n",
      " Training Loss: 0.00000897\n",
      "Evaluation Loss: 0.00000719\n",
      "Epoch [1100/2500],\n",
      " Training Loss: 0.00000611\n",
      "Evaluation Loss: 0.00000317\n",
      "Epoch [1101/2500],\n",
      " Training Loss: 0.00000401\n",
      "Evaluation Loss: 0.00000441\n",
      "Epoch [1102/2500],\n",
      " Training Loss: 0.00000268\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [1103/2500],\n",
      " Training Loss: 0.00000187\n",
      "Evaluation Loss: 0.00000272\n",
      "Epoch [1104/2500],\n",
      " Training Loss: 0.00000262\n",
      "Evaluation Loss: 0.00000107\n",
      "Epoch [1105/2500],\n",
      " Training Loss: 0.00000238\n",
      "Evaluation Loss: 0.00000290\n",
      "Epoch [1106/2500],\n",
      " Training Loss: 0.00000224\n",
      "Evaluation Loss: 0.00000354\n",
      "Epoch [1107/2500],\n",
      " Training Loss: 0.00000318\n",
      "Evaluation Loss: 0.00000108\n",
      "Epoch [1108/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000336\n",
      "Epoch [1109/2500],\n",
      " Training Loss: 0.00000373\n",
      "Evaluation Loss: 0.00000235\n",
      "Epoch [1110/2500],\n",
      " Training Loss: 0.00000332\n",
      "Evaluation Loss: 0.00000765\n",
      "Epoch [1111/2500],\n",
      " Training Loss: 0.00000507\n",
      "Evaluation Loss: 0.00000472\n",
      "Epoch [1112/2500],\n",
      " Training Loss: 0.00001953\n",
      "Evaluation Loss: 0.00001448\n",
      "Epoch [1113/2500],\n",
      " Training Loss: 0.00002188\n",
      "Evaluation Loss: 0.00000763\n",
      "Epoch [1114/2500],\n",
      " Training Loss: 0.00001533\n",
      "Evaluation Loss: 0.00001231\n",
      "Epoch [1115/2500],\n",
      " Training Loss: 0.00001890\n",
      "Evaluation Loss: 0.00006500\n",
      "Epoch [1116/2500],\n",
      " Training Loss: 0.00003236\n",
      "Evaluation Loss: 0.00003960\n",
      "Epoch [1117/2500],\n",
      " Training Loss: 0.00004413\n",
      "Evaluation Loss: 0.00002143\n",
      "Epoch [1118/2500],\n",
      " Training Loss: 0.00004780\n",
      "Evaluation Loss: 0.00005865\n",
      "Epoch [1119/2500],\n",
      " Training Loss: 0.00006241\n",
      "Evaluation Loss: 0.00004048\n",
      "Epoch [1120/2500],\n",
      " Training Loss: 0.00003657\n",
      "Evaluation Loss: 0.00003148\n",
      "Epoch [1121/2500],\n",
      " Training Loss: 0.00002046\n",
      "Evaluation Loss: 0.00001421\n",
      "Epoch [1122/2500],\n",
      " Training Loss: 0.00001138\n",
      "Evaluation Loss: 0.00000778\n",
      "Epoch [1123/2500],\n",
      " Training Loss: 0.00000875\n",
      "Evaluation Loss: 0.00000530\n",
      "Epoch [1124/2500],\n",
      " Training Loss: 0.00000833\n",
      "Evaluation Loss: 0.00000531\n",
      "Epoch [1125/2500],\n",
      " Training Loss: 0.00000595\n",
      "Evaluation Loss: 0.00000207\n",
      "Epoch [1126/2500],\n",
      " Training Loss: 0.00000610\n",
      "Evaluation Loss: 0.00000925\n",
      "Epoch [1127/2500],\n",
      " Training Loss: 0.00000731\n",
      "Evaluation Loss: 0.00001479\n",
      "Epoch [1128/2500],\n",
      " Training Loss: 0.00001757\n",
      "Evaluation Loss: 0.00001320\n",
      "Epoch [1129/2500],\n",
      " Training Loss: 0.00001072\n",
      "Evaluation Loss: 0.00000944\n",
      "Epoch [1130/2500],\n",
      " Training Loss: 0.00001237\n",
      "Evaluation Loss: 0.00000773\n",
      "Epoch [1131/2500],\n",
      " Training Loss: 0.00000992\n",
      "Evaluation Loss: 0.00001229\n",
      "Epoch [1132/2500],\n",
      " Training Loss: 0.00001086\n",
      "Evaluation Loss: 0.00000289\n",
      "Epoch [1133/2500],\n",
      " Training Loss: 0.00000729\n",
      "Evaluation Loss: 0.00000438\n",
      "Epoch [1134/2500],\n",
      " Training Loss: 0.00000373\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [1135/2500],\n",
      " Training Loss: 0.00000235\n",
      "Evaluation Loss: 0.00000122\n",
      "Epoch [1136/2500],\n",
      " Training Loss: 0.00000295\n",
      "Evaluation Loss: 0.00000177\n",
      "Epoch [1137/2500],\n",
      " Training Loss: 0.00000183\n",
      "Evaluation Loss: 0.00000085\n",
      "Epoch [1138/2500],\n",
      " Training Loss: 0.00000140\n",
      "Evaluation Loss: 0.00000149\n",
      "Epoch [1139/2500],\n",
      " Training Loss: 0.00000154\n",
      "Evaluation Loss: 0.00000136\n",
      "Epoch [1140/2500],\n",
      " Training Loss: 0.00000128\n",
      "Evaluation Loss: 0.00000209\n",
      "Epoch [1141/2500],\n",
      " Training Loss: 0.00000197\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [1142/2500],\n",
      " Training Loss: 0.00000194\n",
      "Evaluation Loss: 0.00000281\n",
      "Epoch [1143/2500],\n",
      " Training Loss: 0.00000328\n",
      "Evaluation Loss: 0.00000271\n",
      "Epoch [1144/2500],\n",
      " Training Loss: 0.00001493\n",
      "Evaluation Loss: 0.00002020\n",
      "Epoch [1145/2500],\n",
      " Training Loss: 0.00001565\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [1146/2500],\n",
      " Training Loss: 0.00000917\n",
      "Evaluation Loss: 0.00000985\n",
      "Epoch [1147/2500],\n",
      " Training Loss: 0.00001268\n",
      "Evaluation Loss: 0.00002253\n",
      "Epoch [1148/2500],\n",
      " Training Loss: 0.00002260\n",
      "Evaluation Loss: 0.00003412\n",
      "Epoch [1149/2500],\n",
      " Training Loss: 0.00004185\n",
      "Evaluation Loss: 0.00001831\n",
      "Epoch [1150/2500],\n",
      " Training Loss: 0.00001238\n",
      "Evaluation Loss: 0.00000793\n",
      "Epoch [1151/2500],\n",
      " Training Loss: 0.00000896\n",
      "Evaluation Loss: 0.00000466\n",
      "Epoch [1152/2500],\n",
      " Training Loss: 0.00000834\n",
      "Evaluation Loss: 0.00000158\n",
      "Epoch [1153/2500],\n",
      " Training Loss: 0.00000612\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [1154/2500],\n",
      " Training Loss: 0.00000543\n",
      "Evaluation Loss: 0.00000217\n",
      "Epoch [1155/2500],\n",
      " Training Loss: 0.00001135\n",
      "Evaluation Loss: 0.00000995\n",
      "Epoch [1156/2500],\n",
      " Training Loss: 0.00001105\n",
      "Evaluation Loss: 0.00000539\n",
      "Epoch [1157/2500],\n",
      " Training Loss: 0.00000995\n",
      "Evaluation Loss: 0.00000241\n",
      "Epoch [1158/2500],\n",
      " Training Loss: 0.00001323\n",
      "Evaluation Loss: 0.00000942\n",
      "Epoch [1159/2500],\n",
      " Training Loss: 0.00002710\n",
      "Evaluation Loss: 0.00000648\n",
      "Epoch [1160/2500],\n",
      " Training Loss: 0.00001592\n",
      "Evaluation Loss: 0.00000347\n",
      "Epoch [1161/2500],\n",
      " Training Loss: 0.00005884\n",
      "Evaluation Loss: 0.00005964\n",
      "Epoch [1162/2500],\n",
      " Training Loss: 0.00009548\n",
      "Evaluation Loss: 0.00006581\n",
      "Epoch [1163/2500],\n",
      " Training Loss: 0.00003642\n",
      "Evaluation Loss: 0.00002117\n",
      "Epoch [1164/2500],\n",
      " Training Loss: 0.00002093\n",
      "Evaluation Loss: 0.00001377\n",
      "Epoch [1165/2500],\n",
      " Training Loss: 0.00001270\n",
      "Evaluation Loss: 0.00000664\n",
      "Epoch [1166/2500],\n",
      " Training Loss: 0.00001188\n",
      "Evaluation Loss: 0.00000498\n",
      "Epoch [1167/2500],\n",
      " Training Loss: 0.00000694\n",
      "Evaluation Loss: 0.00000200\n",
      "Epoch [1168/2500],\n",
      " Training Loss: 0.00000552\n",
      "Evaluation Loss: 0.00000208\n",
      "Epoch [1169/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000517\n",
      "Epoch [1170/2500],\n",
      " Training Loss: 0.00000697\n",
      "Evaluation Loss: 0.00000370\n",
      "Epoch [1171/2500],\n",
      " Training Loss: 0.00000785\n",
      "Evaluation Loss: 0.00001934\n",
      "Epoch [1172/2500],\n",
      " Training Loss: 0.00002886\n",
      "Evaluation Loss: 0.00001216\n",
      "Epoch [1173/2500],\n",
      " Training Loss: 0.00005794\n",
      "Evaluation Loss: 0.00003280\n",
      "Epoch [1174/2500],\n",
      " Training Loss: 0.00002486\n",
      "Evaluation Loss: 0.00001078\n",
      "Epoch [1175/2500],\n",
      " Training Loss: 0.00001694\n",
      "Evaluation Loss: 0.00000954\n",
      "Epoch [1176/2500],\n",
      " Training Loss: 0.00001804\n",
      "Evaluation Loss: 0.00001885\n",
      "Epoch [1177/2500],\n",
      " Training Loss: 0.00001658\n",
      "Evaluation Loss: 0.00002271\n",
      "Epoch [1178/2500],\n",
      " Training Loss: 0.00001145\n",
      "Evaluation Loss: 0.00000269\n",
      "Epoch [1179/2500],\n",
      " Training Loss: 0.00000683\n",
      "Evaluation Loss: 0.00000634\n",
      "Epoch [1180/2500],\n",
      " Training Loss: 0.00000486\n",
      "Evaluation Loss: 0.00000526\n",
      "Epoch [1181/2500],\n",
      " Training Loss: 0.00000504\n",
      "Evaluation Loss: 0.00000194\n",
      "Epoch [1182/2500],\n",
      " Training Loss: 0.00000334\n",
      "Evaluation Loss: 0.00000235\n",
      "Epoch [1183/2500],\n",
      " Training Loss: 0.00000335\n",
      "Evaluation Loss: 0.00000205\n",
      "Epoch [1184/2500],\n",
      " Training Loss: 0.00000283\n",
      "Evaluation Loss: 0.00000170\n",
      "Epoch [1185/2500],\n",
      " Training Loss: 0.00000250\n",
      "Evaluation Loss: 0.00000261\n",
      "Epoch [1186/2500],\n",
      " Training Loss: 0.00000335\n",
      "Evaluation Loss: 0.00000178\n",
      "Epoch [1187/2500],\n",
      " Training Loss: 0.00000651\n",
      "Evaluation Loss: 0.00000301\n",
      "Epoch [1188/2500],\n",
      " Training Loss: 0.00000497\n",
      "Evaluation Loss: 0.00000284\n",
      "Epoch [1189/2500],\n",
      " Training Loss: 0.00000256\n",
      "Evaluation Loss: 0.00000191\n",
      "Epoch [1190/2500],\n",
      " Training Loss: 0.00000572\n",
      "Evaluation Loss: 0.00000187\n",
      "Epoch [1191/2500],\n",
      " Training Loss: 0.00000263\n",
      "Evaluation Loss: 0.00000251\n",
      "Epoch [1192/2500],\n",
      " Training Loss: 0.00000679\n",
      "Evaluation Loss: 0.00000321\n",
      "Epoch [1193/2500],\n",
      " Training Loss: 0.00000485\n",
      "Evaluation Loss: 0.00000388\n",
      "Epoch [1194/2500],\n",
      " Training Loss: 0.00000628\n",
      "Evaluation Loss: 0.00000633\n",
      "Epoch [1195/2500],\n",
      " Training Loss: 0.00000919\n",
      "Evaluation Loss: 0.00000612\n",
      "Epoch [1196/2500],\n",
      " Training Loss: 0.00000530\n",
      "Evaluation Loss: 0.00000507\n",
      "Epoch [1197/2500],\n",
      " Training Loss: 0.00000802\n",
      "Evaluation Loss: 0.00000580\n",
      "Epoch [1198/2500],\n",
      " Training Loss: 0.00000696\n",
      "Evaluation Loss: 0.00001075\n",
      "Epoch [1199/2500],\n",
      " Training Loss: 0.00001350\n",
      "Evaluation Loss: 0.00001237\n",
      "Epoch [1200/2500],\n",
      " Training Loss: 0.00001000\n",
      "Evaluation Loss: 0.00001064\n",
      "Epoch [1201/2500],\n",
      " Training Loss: 0.00001014\n",
      "Evaluation Loss: 0.00000310\n",
      "Epoch [1202/2500],\n",
      " Training Loss: 0.00000407\n",
      "Evaluation Loss: 0.00000822\n",
      "Epoch [1203/2500],\n",
      " Training Loss: 0.00000471\n",
      "Evaluation Loss: 0.00000517\n",
      "Epoch [1204/2500],\n",
      " Training Loss: 0.00000494\n",
      "Evaluation Loss: 0.00000495\n",
      "Epoch [1205/2500],\n",
      " Training Loss: 0.00000209\n",
      "Evaluation Loss: 0.00000173\n",
      "Epoch [1206/2500],\n",
      " Training Loss: 0.00000261\n",
      "Evaluation Loss: 0.00000213\n",
      "Epoch [1207/2500],\n",
      " Training Loss: 0.00000254\n",
      "Evaluation Loss: 0.00000109\n",
      "Epoch [1208/2500],\n",
      " Training Loss: 0.00000166\n",
      "Evaluation Loss: 0.00000192\n",
      "Epoch [1209/2500],\n",
      " Training Loss: 0.00000195\n",
      "Evaluation Loss: 0.00000179\n",
      "Epoch [1210/2500],\n",
      " Training Loss: 0.00000184\n",
      "Evaluation Loss: 0.00000201\n",
      "Epoch [1211/2500],\n",
      " Training Loss: 0.00000159\n",
      "Evaluation Loss: 0.00000220\n",
      "Epoch [1212/2500],\n",
      " Training Loss: 0.00000175\n",
      "Evaluation Loss: 0.00000138\n",
      "Epoch [1213/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000272\n",
      "Epoch [1214/2500],\n",
      " Training Loss: 0.00000248\n",
      "Evaluation Loss: 0.00000308\n",
      "Epoch [1215/2500],\n",
      " Training Loss: 0.00000662\n",
      "Evaluation Loss: 0.00000244\n",
      "Epoch [1216/2500],\n",
      " Training Loss: 0.00002299\n",
      "Evaluation Loss: 0.00001107\n",
      "Epoch [1217/2500],\n",
      " Training Loss: 0.00003281\n",
      "Evaluation Loss: 0.00001481\n",
      "Epoch [1218/2500],\n",
      " Training Loss: 0.00004790\n",
      "Evaluation Loss: 0.00005285\n",
      "Epoch [1219/2500],\n",
      " Training Loss: 0.00003464\n",
      "Evaluation Loss: 0.00001939\n",
      "Epoch [1220/2500],\n",
      " Training Loss: 0.00002192\n",
      "Evaluation Loss: 0.00000617\n",
      "Epoch [1221/2500],\n",
      " Training Loss: 0.00000696\n",
      "Evaluation Loss: 0.00000580\n",
      "Epoch [1222/2500],\n",
      " Training Loss: 0.00000645\n",
      "Evaluation Loss: 0.00000260\n",
      "Epoch [1223/2500],\n",
      " Training Loss: 0.00000417\n",
      "Evaluation Loss: 0.00000449\n",
      "Epoch [1224/2500],\n",
      " Training Loss: 0.00000438\n",
      "Evaluation Loss: 0.00000513\n",
      "Epoch [1225/2500],\n",
      " Training Loss: 0.00000732\n",
      "Evaluation Loss: 0.00000435\n",
      "Epoch [1226/2500],\n",
      " Training Loss: 0.00000610\n",
      "Evaluation Loss: 0.00000553\n",
      "Epoch [1227/2500],\n",
      " Training Loss: 0.00000582\n",
      "Evaluation Loss: 0.00000335\n",
      "Epoch [1228/2500],\n",
      " Training Loss: 0.00000379\n",
      "Evaluation Loss: 0.00000221\n",
      "Epoch [1229/2500],\n",
      " Training Loss: 0.00000298\n",
      "Evaluation Loss: 0.00000406\n",
      "Epoch [1230/2500],\n",
      " Training Loss: 0.00000673\n",
      "Evaluation Loss: 0.00000508\n",
      "Epoch [1231/2500],\n",
      " Training Loss: 0.00000715\n",
      "Evaluation Loss: 0.00000962\n",
      "Epoch [1232/2500],\n",
      " Training Loss: 0.00001055\n",
      "Evaluation Loss: 0.00000350\n",
      "Epoch [1233/2500],\n",
      " Training Loss: 0.00002716\n",
      "Evaluation Loss: 0.00000959\n",
      "Epoch [1234/2500],\n",
      " Training Loss: 0.00003859\n",
      "Evaluation Loss: 0.00008307\n",
      "Epoch [1235/2500],\n",
      " Training Loss: 0.00006279\n",
      "Evaluation Loss: 0.00002198\n",
      "Epoch [1236/2500],\n",
      " Training Loss: 0.00002222\n",
      "Evaluation Loss: 0.00004404\n",
      "Epoch [1237/2500],\n",
      " Training Loss: 0.00003326\n",
      "Evaluation Loss: 0.00002271\n",
      "Epoch [1238/2500],\n",
      " Training Loss: 0.00001556\n",
      "Evaluation Loss: 0.00001554\n",
      "Epoch [1239/2500],\n",
      " Training Loss: 0.00000901\n",
      "Evaluation Loss: 0.00000620\n",
      "Epoch [1240/2500],\n",
      " Training Loss: 0.00000442\n",
      "Evaluation Loss: 0.00000262\n",
      "Epoch [1241/2500],\n",
      " Training Loss: 0.00000386\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [1242/2500],\n",
      " Training Loss: 0.00000336\n",
      "Evaluation Loss: 0.00000712\n",
      "Epoch [1243/2500],\n",
      " Training Loss: 0.00000411\n",
      "Evaluation Loss: 0.00000225\n",
      "Epoch [1244/2500],\n",
      " Training Loss: 0.00000326\n",
      "Evaluation Loss: 0.00000361\n",
      "Epoch [1245/2500],\n",
      " Training Loss: 0.00000211\n",
      "Evaluation Loss: 0.00000223\n",
      "Epoch [1246/2500],\n",
      " Training Loss: 0.00000386\n",
      "Evaluation Loss: 0.00000250\n",
      "Epoch [1247/2500],\n",
      " Training Loss: 0.00000476\n",
      "Evaluation Loss: 0.00000287\n",
      "Epoch [1248/2500],\n",
      " Training Loss: 0.00000772\n",
      "Evaluation Loss: 0.00000463\n",
      "Epoch [1249/2500],\n",
      " Training Loss: 0.00000458\n",
      "Evaluation Loss: 0.00000364\n",
      "Epoch [1250/2500],\n",
      " Training Loss: 0.00000620\n",
      "Evaluation Loss: 0.00000613\n",
      "Epoch [1251/2500],\n",
      " Training Loss: 0.00001378\n",
      "Evaluation Loss: 0.00001429\n",
      "Epoch [1252/2500],\n",
      " Training Loss: 0.00001648\n",
      "Evaluation Loss: 0.00000626\n",
      "Epoch [1253/2500],\n",
      " Training Loss: 0.00001474\n",
      "Evaluation Loss: 0.00000359\n",
      "Epoch [1254/2500],\n",
      " Training Loss: 0.00002134\n",
      "Evaluation Loss: 0.00001050\n",
      "Epoch [1255/2500],\n",
      " Training Loss: 0.00002296\n",
      "Evaluation Loss: 0.00001454\n",
      "Epoch [1256/2500],\n",
      " Training Loss: 0.00001146\n",
      "Evaluation Loss: 0.00000338\n",
      "Epoch [1257/2500],\n",
      " Training Loss: 0.00000780\n",
      "Evaluation Loss: 0.00000621\n",
      "Epoch [1258/2500],\n",
      " Training Loss: 0.00000465\n",
      "Evaluation Loss: 0.00000409\n",
      "Epoch [1259/2500],\n",
      " Training Loss: 0.00000425\n",
      "Evaluation Loss: 0.00000202\n",
      "Epoch [1260/2500],\n",
      " Training Loss: 0.00000277\n",
      "Evaluation Loss: 0.00000135\n",
      "Epoch [1261/2500],\n",
      " Training Loss: 0.00000238\n",
      "Evaluation Loss: 0.00000288\n",
      "Epoch [1262/2500],\n",
      " Training Loss: 0.00000284\n",
      "Evaluation Loss: 0.00000427\n",
      "Epoch [1263/2500],\n",
      " Training Loss: 0.00000451\n",
      "Evaluation Loss: 0.00000380\n",
      "Epoch [1264/2500],\n",
      " Training Loss: 0.00000401\n",
      "Evaluation Loss: 0.00000400\n",
      "Epoch [1265/2500],\n",
      " Training Loss: 0.00000715\n",
      "Evaluation Loss: 0.00000647\n",
      "Epoch [1266/2500],\n",
      " Training Loss: 0.00001095\n",
      "Evaluation Loss: 0.00000397\n",
      "Epoch [1267/2500],\n",
      " Training Loss: 0.00001628\n",
      "Evaluation Loss: 0.00000735\n",
      "Epoch [1268/2500],\n",
      " Training Loss: 0.00002449\n",
      "Evaluation Loss: 0.00001537\n",
      "Epoch [1269/2500],\n",
      " Training Loss: 0.00003711\n",
      "Evaluation Loss: 0.00006208\n",
      "Epoch [1270/2500],\n",
      " Training Loss: 0.00003612\n",
      "Evaluation Loss: 0.00001604\n",
      "Epoch [1271/2500],\n",
      " Training Loss: 0.00002448\n",
      "Evaluation Loss: 0.00001029\n",
      "Epoch [1272/2500],\n",
      " Training Loss: 0.00001279\n",
      "Evaluation Loss: 0.00000485\n",
      "Epoch [1273/2500],\n",
      " Training Loss: 0.00000785\n",
      "Evaluation Loss: 0.00000236\n",
      "Epoch [1274/2500],\n",
      " Training Loss: 0.00000265\n",
      "Evaluation Loss: 0.00000250\n",
      "Epoch [1275/2500],\n",
      " Training Loss: 0.00000224\n",
      "Evaluation Loss: 0.00000085\n",
      "Epoch [1276/2500],\n",
      " Training Loss: 0.00000136\n",
      "Evaluation Loss: 0.00000214\n",
      "Epoch [1277/2500],\n",
      " Training Loss: 0.00000169\n",
      "Evaluation Loss: 0.00000091\n",
      "Epoch [1278/2500],\n",
      " Training Loss: 0.00000163\n",
      "Evaluation Loss: 0.00000203\n",
      "Epoch [1279/2500],\n",
      " Training Loss: 0.00000227\n",
      "Evaluation Loss: 0.00000185\n",
      "Epoch [1280/2500],\n",
      " Training Loss: 0.00000227\n",
      "Evaluation Loss: 0.00000194\n",
      "Epoch [1281/2500],\n",
      " Training Loss: 0.00000271\n",
      "Evaluation Loss: 0.00000130\n",
      "Epoch [1282/2500],\n",
      " Training Loss: 0.00000204\n",
      "Evaluation Loss: 0.00000167\n",
      "Epoch [1283/2500],\n",
      " Training Loss: 0.00000209\n",
      "Evaluation Loss: 0.00000164\n",
      "Epoch [1284/2500],\n",
      " Training Loss: 0.00000137\n",
      "Evaluation Loss: 0.00000107\n",
      "Epoch [1285/2500],\n",
      " Training Loss: 0.00000133\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [1286/2500],\n",
      " Training Loss: 0.00000199\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [1287/2500],\n",
      " Training Loss: 0.00000281\n",
      "Evaluation Loss: 0.00000311\n",
      "Epoch [1288/2500],\n",
      " Training Loss: 0.00000241\n",
      "Evaluation Loss: 0.00000245\n",
      "Epoch [1289/2500],\n",
      " Training Loss: 0.00000395\n",
      "Evaluation Loss: 0.00000449\n",
      "Epoch [1290/2500],\n",
      " Training Loss: 0.00000408\n",
      "Evaluation Loss: 0.00000660\n",
      "Epoch [1291/2500],\n",
      " Training Loss: 0.00000380\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [1292/2500],\n",
      " Training Loss: 0.00000504\n",
      "Evaluation Loss: 0.00002392\n",
      "Epoch [1293/2500],\n",
      " Training Loss: 0.00001733\n",
      "Evaluation Loss: 0.00003092\n",
      "Epoch [1294/2500],\n",
      " Training Loss: 0.00002359\n",
      "Evaluation Loss: 0.00001579\n",
      "Epoch [1295/2500],\n",
      " Training Loss: 0.00007184\n",
      "Evaluation Loss: 0.00007959\n",
      "Epoch [1296/2500],\n",
      " Training Loss: 0.00008336\n",
      "Evaluation Loss: 0.00004710\n",
      "Epoch [1297/2500],\n",
      " Training Loss: 0.00003631\n",
      "Evaluation Loss: 0.00001828\n",
      "Epoch [1298/2500],\n",
      " Training Loss: 0.00001624\n",
      "Evaluation Loss: 0.00001620\n",
      "Epoch [1299/2500],\n",
      " Training Loss: 0.00001290\n",
      "Evaluation Loss: 0.00000514\n",
      "Epoch [1300/2500],\n",
      " Training Loss: 0.00000855\n",
      "Evaluation Loss: 0.00001009\n",
      "Epoch [1301/2500],\n",
      " Training Loss: 0.00001235\n",
      "Evaluation Loss: 0.00000738\n",
      "Epoch [1302/2500],\n",
      " Training Loss: 0.00000457\n",
      "Evaluation Loss: 0.00000170\n",
      "Epoch [1303/2500],\n",
      " Training Loss: 0.00000837\n",
      "Evaluation Loss: 0.00002563\n",
      "Epoch [1304/2500],\n",
      " Training Loss: 0.00001091\n",
      "Evaluation Loss: 0.00000733\n",
      "Epoch [1305/2500],\n",
      " Training Loss: 0.00000862\n",
      "Evaluation Loss: 0.00000454\n",
      "Epoch [1306/2500],\n",
      " Training Loss: 0.00000766\n",
      "Evaluation Loss: 0.00000366\n",
      "Epoch [1307/2500],\n",
      " Training Loss: 0.00000497\n",
      "Evaluation Loss: 0.00000223\n",
      "Epoch [1308/2500],\n",
      " Training Loss: 0.00000556\n",
      "Evaluation Loss: 0.00000501\n",
      "Epoch [1309/2500],\n",
      " Training Loss: 0.00000739\n",
      "Evaluation Loss: 0.00000673\n",
      "Epoch [1310/2500],\n",
      " Training Loss: 0.00000362\n",
      "Evaluation Loss: 0.00000200\n",
      "Epoch [1311/2500],\n",
      " Training Loss: 0.00000465\n",
      "Evaluation Loss: 0.00000131\n",
      "Epoch [1312/2500],\n",
      " Training Loss: 0.00000672\n",
      "Evaluation Loss: 0.00000772\n",
      "Epoch [1313/2500],\n",
      " Training Loss: 0.00000545\n",
      "Evaluation Loss: 0.00000213\n",
      "Epoch [1314/2500],\n",
      " Training Loss: 0.00000935\n",
      "Evaluation Loss: 0.00000220\n",
      "Epoch [1315/2500],\n",
      " Training Loss: 0.00000389\n",
      "Evaluation Loss: 0.00000327\n",
      "Epoch [1316/2500],\n",
      " Training Loss: 0.00000253\n",
      "Evaluation Loss: 0.00000244\n",
      "Epoch [1317/2500],\n",
      " Training Loss: 0.00000143\n",
      "Evaluation Loss: 0.00000123\n",
      "Epoch [1318/2500],\n",
      " Training Loss: 0.00000225\n",
      "Evaluation Loss: 0.00000141\n",
      "Epoch [1319/2500],\n",
      " Training Loss: 0.00000159\n",
      "Evaluation Loss: 0.00000354\n",
      "Epoch [1320/2500],\n",
      " Training Loss: 0.00000317\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [1321/2500],\n",
      " Training Loss: 0.00000397\n",
      "Evaluation Loss: 0.00000547\n",
      "Epoch [1322/2500],\n",
      " Training Loss: 0.00000511\n",
      "Evaluation Loss: 0.00000272\n",
      "Epoch [1323/2500],\n",
      " Training Loss: 0.00000283\n",
      "Evaluation Loss: 0.00000247\n",
      "Epoch [1324/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000090\n",
      "Epoch [1325/2500],\n",
      " Training Loss: 0.00000289\n",
      "Evaluation Loss: 0.00000266\n",
      "Epoch [1326/2500],\n",
      " Training Loss: 0.00000454\n",
      "Evaluation Loss: 0.00000202\n",
      "Epoch [1327/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000624\n",
      "Epoch [1328/2500],\n",
      " Training Loss: 0.00000405\n",
      "Evaluation Loss: 0.00000277\n",
      "Epoch [1329/2500],\n",
      " Training Loss: 0.00000502\n",
      "Evaluation Loss: 0.00000560\n",
      "Epoch [1330/2500],\n",
      " Training Loss: 0.00001381\n",
      "Evaluation Loss: 0.00003467\n",
      "Epoch [1331/2500],\n",
      " Training Loss: 0.00002381\n",
      "Evaluation Loss: 0.00001795\n",
      "Epoch [1332/2500],\n",
      " Training Loss: 0.00001546\n",
      "Evaluation Loss: 0.00000496\n",
      "Epoch [1333/2500],\n",
      " Training Loss: 0.00001048\n",
      "Evaluation Loss: 0.00000702\n",
      "Epoch [1334/2500],\n",
      " Training Loss: 0.00000885\n",
      "Evaluation Loss: 0.00000980\n",
      "Epoch [1335/2500],\n",
      " Training Loss: 0.00001591\n",
      "Evaluation Loss: 0.00000686\n",
      "Epoch [1336/2500],\n",
      " Training Loss: 0.00000747\n",
      "Evaluation Loss: 0.00000478\n",
      "Epoch [1337/2500],\n",
      " Training Loss: 0.00001106\n",
      "Evaluation Loss: 0.00000693\n",
      "Epoch [1338/2500],\n",
      " Training Loss: 0.00003054\n",
      "Evaluation Loss: 0.00003885\n",
      "Epoch [1339/2500],\n",
      " Training Loss: 0.00003820\n",
      "Evaluation Loss: 0.00003387\n",
      "Epoch [1340/2500],\n",
      " Training Loss: 0.00001735\n",
      "Evaluation Loss: 0.00000862\n",
      "Epoch [1341/2500],\n",
      " Training Loss: 0.00000733\n",
      "Evaluation Loss: 0.00000524\n",
      "Epoch [1342/2500],\n",
      " Training Loss: 0.00000491\n",
      "Evaluation Loss: 0.00000312\n",
      "Epoch [1343/2500],\n",
      " Training Loss: 0.00000213\n",
      "Evaluation Loss: 0.00000161\n",
      "Epoch [1344/2500],\n",
      " Training Loss: 0.00000428\n",
      "Evaluation Loss: 0.00000844\n",
      "Epoch [1345/2500],\n",
      " Training Loss: 0.00000393\n",
      "Evaluation Loss: 0.00000241\n",
      "Epoch [1346/2500],\n",
      " Training Loss: 0.00000475\n",
      "Evaluation Loss: 0.00000361\n",
      "Epoch [1347/2500],\n",
      " Training Loss: 0.00000374\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [1348/2500],\n",
      " Training Loss: 0.00000447\n",
      "Evaluation Loss: 0.00000210\n",
      "Epoch [1349/2500],\n",
      " Training Loss: 0.00000263\n",
      "Evaluation Loss: 0.00000294\n",
      "Epoch [1350/2500],\n",
      " Training Loss: 0.00000305\n",
      "Evaluation Loss: 0.00000133\n",
      "Epoch [1351/2500],\n",
      " Training Loss: 0.00000421\n",
      "Evaluation Loss: 0.00000310\n",
      "Epoch [1352/2500],\n",
      " Training Loss: 0.00000297\n",
      "Evaluation Loss: 0.00000193\n",
      "Epoch [1353/2500],\n",
      " Training Loss: 0.00000411\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [1354/2500],\n",
      " Training Loss: 0.00000835\n",
      "Evaluation Loss: 0.00000320\n",
      "Epoch [1355/2500],\n",
      " Training Loss: 0.00000539\n",
      "Evaluation Loss: 0.00000290\n",
      "Epoch [1356/2500],\n",
      " Training Loss: 0.00000904\n",
      "Evaluation Loss: 0.00000844\n",
      "Epoch [1357/2500],\n",
      " Training Loss: 0.00001158\n",
      "Evaluation Loss: 0.00003273\n",
      "Epoch [1358/2500],\n",
      " Training Loss: 0.00003292\n",
      "Evaluation Loss: 0.00001457\n",
      "Epoch [1359/2500],\n",
      " Training Loss: 0.00004024\n",
      "Evaluation Loss: 0.00001195\n",
      "Epoch [1360/2500],\n",
      " Training Loss: 0.00003181\n",
      "Evaluation Loss: 0.00000538\n",
      "Epoch [1361/2500],\n",
      " Training Loss: 0.00002948\n",
      "Evaluation Loss: 0.00002244\n",
      "Epoch [1362/2500],\n",
      " Training Loss: 0.00004280\n",
      "Evaluation Loss: 0.00005353\n",
      "Epoch [1363/2500],\n",
      " Training Loss: 0.00006382\n",
      "Evaluation Loss: 0.00008583\n",
      "Epoch [1364/2500],\n",
      " Training Loss: 0.00003737\n",
      "Evaluation Loss: 0.00001208\n",
      "Epoch [1365/2500],\n",
      " Training Loss: 0.00001628\n",
      "Evaluation Loss: 0.00000894\n",
      "Epoch [1366/2500],\n",
      " Training Loss: 0.00001595\n",
      "Evaluation Loss: 0.00000663\n",
      "Epoch [1367/2500],\n",
      " Training Loss: 0.00000508\n",
      "Evaluation Loss: 0.00000185\n",
      "Epoch [1368/2500],\n",
      " Training Loss: 0.00000353\n",
      "Evaluation Loss: 0.00000184\n",
      "Epoch [1369/2500],\n",
      " Training Loss: 0.00000277\n",
      "Evaluation Loss: 0.00000212\n",
      "Epoch [1370/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000116\n",
      "Epoch [1371/2500],\n",
      " Training Loss: 0.00000102\n",
      "Evaluation Loss: 0.00000095\n",
      "Epoch [1372/2500],\n",
      " Training Loss: 0.00000099\n",
      "Evaluation Loss: 0.00000220\n",
      "Epoch [1373/2500],\n",
      " Training Loss: 0.00000129\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [1374/2500],\n",
      " Training Loss: 0.00000119\n",
      "Evaluation Loss: 0.00000118\n",
      "Epoch [1375/2500],\n",
      " Training Loss: 0.00000145\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [1376/2500],\n",
      " Training Loss: 0.00000183\n",
      "Evaluation Loss: 0.00000099\n",
      "Epoch [1377/2500],\n",
      " Training Loss: 0.00000177\n",
      "Evaluation Loss: 0.00000133\n",
      "Epoch [1378/2500],\n",
      " Training Loss: 0.00000230\n",
      "Evaluation Loss: 0.00000111\n",
      "Epoch [1379/2500],\n",
      " Training Loss: 0.00000208\n",
      "Evaluation Loss: 0.00000236\n",
      "Epoch [1380/2500],\n",
      " Training Loss: 0.00000204\n",
      "Evaluation Loss: 0.00000260\n",
      "Epoch [1381/2500],\n",
      " Training Loss: 0.00000187\n",
      "Evaluation Loss: 0.00000266\n",
      "Epoch [1382/2500],\n",
      " Training Loss: 0.00000527\n",
      "Evaluation Loss: 0.00000679\n",
      "Epoch [1383/2500],\n",
      " Training Loss: 0.00000988\n",
      "Evaluation Loss: 0.00000564\n",
      "Epoch [1384/2500],\n",
      " Training Loss: 0.00000442\n",
      "Evaluation Loss: 0.00000703\n",
      "Epoch [1385/2500],\n",
      " Training Loss: 0.00000518\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1386/2500],\n",
      " Training Loss: 0.00000451\n",
      "Evaluation Loss: 0.00000441\n",
      "Epoch [1387/2500],\n",
      " Training Loss: 0.00000358\n",
      "Evaluation Loss: 0.00000260\n",
      "Epoch [1388/2500],\n",
      " Training Loss: 0.00000476\n",
      "Evaluation Loss: 0.00000369\n",
      "Epoch [1389/2500],\n",
      " Training Loss: 0.00000603\n",
      "Evaluation Loss: 0.00000373\n",
      "Epoch [1390/2500],\n",
      " Training Loss: 0.00000812\n",
      "Evaluation Loss: 0.00000502\n",
      "Epoch [1391/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000593\n",
      "Epoch [1392/2500],\n",
      " Training Loss: 0.00002076\n",
      "Evaluation Loss: 0.00000500\n",
      "Epoch [1393/2500],\n",
      " Training Loss: 0.00000624\n",
      "Evaluation Loss: 0.00000666\n",
      "Epoch [1394/2500],\n",
      " Training Loss: 0.00000443\n",
      "Evaluation Loss: 0.00001066\n",
      "Epoch [1395/2500],\n",
      " Training Loss: 0.00000726\n",
      "Evaluation Loss: 0.00000533\n",
      "Epoch [1396/2500],\n",
      " Training Loss: 0.00001198\n",
      "Evaluation Loss: 0.00000556\n",
      "Epoch [1397/2500],\n",
      " Training Loss: 0.00000496\n",
      "Evaluation Loss: 0.00000320\n",
      "Epoch [1398/2500],\n",
      " Training Loss: 0.00000519\n",
      "Evaluation Loss: 0.00001058\n",
      "Epoch [1399/2500],\n",
      " Training Loss: 0.00000719\n",
      "Evaluation Loss: 0.00001189\n",
      "Epoch [1400/2500],\n",
      " Training Loss: 0.00001739\n",
      "Evaluation Loss: 0.00001074\n",
      "Epoch [1401/2500],\n",
      " Training Loss: 0.00001315\n",
      "Evaluation Loss: 0.00000532\n",
      "Epoch [1402/2500],\n",
      " Training Loss: 0.00001890\n",
      "Evaluation Loss: 0.00000793\n",
      "Epoch [1403/2500],\n",
      " Training Loss: 0.00001113\n",
      "Evaluation Loss: 0.00000312\n",
      "Epoch [1404/2500],\n",
      " Training Loss: 0.00001139\n",
      "Evaluation Loss: 0.00000958\n",
      "Epoch [1405/2500],\n",
      " Training Loss: 0.00000798\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1406/2500],\n",
      " Training Loss: 0.00000876\n",
      "Evaluation Loss: 0.00000890\n",
      "Epoch [1407/2500],\n",
      " Training Loss: 0.00000839\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [1408/2500],\n",
      " Training Loss: 0.00000519\n",
      "Evaluation Loss: 0.00000460\n",
      "Epoch [1409/2500],\n",
      " Training Loss: 0.00000301\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [1410/2500],\n",
      " Training Loss: 0.00000232\n",
      "Evaluation Loss: 0.00000195\n",
      "Epoch [1411/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000132\n",
      "Epoch [1412/2500],\n",
      " Training Loss: 0.00000394\n",
      "Evaluation Loss: 0.00000233\n",
      "Epoch [1413/2500],\n",
      " Training Loss: 0.00000324\n",
      "Evaluation Loss: 0.00000206\n",
      "Epoch [1414/2500],\n",
      " Training Loss: 0.00000232\n",
      "Evaluation Loss: 0.00000523\n",
      "Epoch [1415/2500],\n",
      " Training Loss: 0.00000502\n",
      "Evaluation Loss: 0.00000410\n",
      "Epoch [1416/2500],\n",
      " Training Loss: 0.00000455\n",
      "Evaluation Loss: 0.00000321\n",
      "Epoch [1417/2500],\n",
      " Training Loss: 0.00000649\n",
      "Evaluation Loss: 0.00000384\n",
      "Epoch [1418/2500],\n",
      " Training Loss: 0.00000655\n",
      "Evaluation Loss: 0.00000469\n",
      "Epoch [1419/2500],\n",
      " Training Loss: 0.00000786\n",
      "Evaluation Loss: 0.00000363\n",
      "Epoch [1420/2500],\n",
      " Training Loss: 0.00000994\n",
      "Evaluation Loss: 0.00001358\n",
      "Epoch [1421/2500],\n",
      " Training Loss: 0.00000950\n",
      "Evaluation Loss: 0.00000994\n",
      "Epoch [1422/2500],\n",
      " Training Loss: 0.00001175\n",
      "Evaluation Loss: 0.00000906\n",
      "Epoch [1423/2500],\n",
      " Training Loss: 0.00001006\n",
      "Evaluation Loss: 0.00000867\n",
      "Epoch [1424/2500],\n",
      " Training Loss: 0.00002029\n",
      "Evaluation Loss: 0.00001091\n",
      "Epoch [1425/2500],\n",
      " Training Loss: 0.00001837\n",
      "Evaluation Loss: 0.00001006\n",
      "Epoch [1426/2500],\n",
      " Training Loss: 0.00001458\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [1427/2500],\n",
      " Training Loss: 0.00002298\n",
      "Evaluation Loss: 0.00001834\n",
      "Epoch [1428/2500],\n",
      " Training Loss: 0.00002981\n",
      "Evaluation Loss: 0.00001174\n",
      "Epoch [1429/2500],\n",
      " Training Loss: 0.00003930\n",
      "Evaluation Loss: 0.00002151\n",
      "Epoch [1430/2500],\n",
      " Training Loss: 0.00001818\n",
      "Evaluation Loss: 0.00002855\n",
      "Epoch [1431/2500],\n",
      " Training Loss: 0.00001203\n",
      "Evaluation Loss: 0.00000546\n",
      "Epoch [1432/2500],\n",
      " Training Loss: 0.00000686\n",
      "Evaluation Loss: 0.00000641\n",
      "Epoch [1433/2500],\n",
      " Training Loss: 0.00000531\n",
      "Evaluation Loss: 0.00000536\n",
      "Epoch [1434/2500],\n",
      " Training Loss: 0.00000858\n",
      "Evaluation Loss: 0.00000738\n",
      "Epoch [1435/2500],\n",
      " Training Loss: 0.00000912\n",
      "Evaluation Loss: 0.00001075\n",
      "Epoch [1436/2500],\n",
      " Training Loss: 0.00000388\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1437/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [1438/2500],\n",
      " Training Loss: 0.00000323\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [1439/2500],\n",
      " Training Loss: 0.00000195\n",
      "Evaluation Loss: 0.00000152\n",
      "Epoch [1440/2500],\n",
      " Training Loss: 0.00000279\n",
      "Evaluation Loss: 0.00000343\n",
      "Epoch [1441/2500],\n",
      " Training Loss: 0.00000185\n",
      "Evaluation Loss: 0.00000114\n",
      "Epoch [1442/2500],\n",
      " Training Loss: 0.00000191\n",
      "Evaluation Loss: 0.00000209\n",
      "Epoch [1443/2500],\n",
      " Training Loss: 0.00000256\n",
      "Evaluation Loss: 0.00000120\n",
      "Epoch [1444/2500],\n",
      " Training Loss: 0.00000550\n",
      "Evaluation Loss: 0.00000346\n",
      "Epoch [1445/2500],\n",
      " Training Loss: 0.00000661\n",
      "Evaluation Loss: 0.00000289\n",
      "Epoch [1446/2500],\n",
      " Training Loss: 0.00000701\n",
      "Evaluation Loss: 0.00001341\n",
      "Epoch [1447/2500],\n",
      " Training Loss: 0.00003735\n",
      "Evaluation Loss: 0.00003394\n",
      "Epoch [1448/2500],\n",
      " Training Loss: 0.00003976\n",
      "Evaluation Loss: 0.00003234\n",
      "Epoch [1449/2500],\n",
      " Training Loss: 0.00002063\n",
      "Evaluation Loss: 0.00006142\n",
      "Epoch [1450/2500],\n",
      " Training Loss: 0.00001610\n",
      "Evaluation Loss: 0.00000783\n",
      "Epoch [1451/2500],\n",
      " Training Loss: 0.00000510\n",
      "Evaluation Loss: 0.00000309\n",
      "Epoch [1452/2500],\n",
      " Training Loss: 0.00000665\n",
      "Evaluation Loss: 0.00000366\n",
      "Epoch [1453/2500],\n",
      " Training Loss: 0.00000290\n",
      "Evaluation Loss: 0.00000240\n",
      "Epoch [1454/2500],\n",
      " Training Loss: 0.00000373\n",
      "Evaluation Loss: 0.00000224\n",
      "Epoch [1455/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000211\n",
      "Epoch [1456/2500],\n",
      " Training Loss: 0.00000320\n",
      "Evaluation Loss: 0.00000129\n",
      "Epoch [1457/2500],\n",
      " Training Loss: 0.00000245\n",
      "Evaluation Loss: 0.00000208\n",
      "Epoch [1458/2500],\n",
      " Training Loss: 0.00000797\n",
      "Evaluation Loss: 0.00001414\n",
      "Epoch [1459/2500],\n",
      " Training Loss: 0.00000974\n",
      "Evaluation Loss: 0.00000281\n",
      "Epoch [1460/2500],\n",
      " Training Loss: 0.00001156\n",
      "Evaluation Loss: 0.00000234\n",
      "Epoch [1461/2500],\n",
      " Training Loss: 0.00001186\n",
      "Evaluation Loss: 0.00000238\n",
      "Epoch [1462/2500],\n",
      " Training Loss: 0.00000419\n",
      "Evaluation Loss: 0.00000119\n",
      "Epoch [1463/2500],\n",
      " Training Loss: 0.00000492\n",
      "Evaluation Loss: 0.00000738\n",
      "Epoch [1464/2500],\n",
      " Training Loss: 0.00001025\n",
      "Evaluation Loss: 0.00001075\n",
      "Epoch [1465/2500],\n",
      " Training Loss: 0.00000997\n",
      "Evaluation Loss: 0.00000523\n",
      "Epoch [1466/2500],\n",
      " Training Loss: 0.00001139\n",
      "Evaluation Loss: 0.00000836\n",
      "Epoch [1467/2500],\n",
      " Training Loss: 0.00003568\n",
      "Evaluation Loss: 0.00003689\n",
      "Epoch [1468/2500],\n",
      " Training Loss: 0.00003377\n",
      "Evaluation Loss: 0.00000812\n",
      "Epoch [1469/2500],\n",
      " Training Loss: 0.00001878\n",
      "Evaluation Loss: 0.00000610\n",
      "Epoch [1470/2500],\n",
      " Training Loss: 0.00002228\n",
      "Evaluation Loss: 0.00001882\n",
      "Epoch [1471/2500],\n",
      " Training Loss: 0.00002888\n",
      "Evaluation Loss: 0.00001552\n",
      "Epoch [1472/2500],\n",
      " Training Loss: 0.00001396\n",
      "Evaluation Loss: 0.00000377\n",
      "Epoch [1473/2500],\n",
      " Training Loss: 0.00001068\n",
      "Evaluation Loss: 0.00001893\n",
      "Epoch [1474/2500],\n",
      " Training Loss: 0.00002196\n",
      "Evaluation Loss: 0.00001972\n",
      "Epoch [1475/2500],\n",
      " Training Loss: 0.00000832\n",
      "Evaluation Loss: 0.00000688\n",
      "Epoch [1476/2500],\n",
      " Training Loss: 0.00000469\n",
      "Evaluation Loss: 0.00000342\n",
      "Epoch [1477/2500],\n",
      " Training Loss: 0.00000385\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [1478/2500],\n",
      " Training Loss: 0.00000399\n",
      "Evaluation Loss: 0.00000298\n",
      "Epoch [1479/2500],\n",
      " Training Loss: 0.00000634\n",
      "Evaluation Loss: 0.00000182\n",
      "Epoch [1480/2500],\n",
      " Training Loss: 0.00000285\n",
      "Evaluation Loss: 0.00000378\n",
      "Epoch [1481/2500],\n",
      " Training Loss: 0.00000665\n",
      "Evaluation Loss: 0.00000538\n",
      "Epoch [1482/2500],\n",
      " Training Loss: 0.00000780\n",
      "Evaluation Loss: 0.00000639\n",
      "Epoch [1483/2500],\n",
      " Training Loss: 0.00000449\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [1484/2500],\n",
      " Training Loss: 0.00000389\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [1485/2500],\n",
      " Training Loss: 0.00000180\n",
      "Evaluation Loss: 0.00000295\n",
      "Epoch [1486/2500],\n",
      " Training Loss: 0.00000217\n",
      "Evaluation Loss: 0.00000174\n",
      "Epoch [1487/2500],\n",
      " Training Loss: 0.00000257\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [1488/2500],\n",
      " Training Loss: 0.00000156\n",
      "Evaluation Loss: 0.00000089\n",
      "Epoch [1489/2500],\n",
      " Training Loss: 0.00000205\n",
      "Evaluation Loss: 0.00000127\n",
      "Epoch [1490/2500],\n",
      " Training Loss: 0.00000320\n",
      "Evaluation Loss: 0.00000427\n",
      "Epoch [1491/2500],\n",
      " Training Loss: 0.00000404\n",
      "Evaluation Loss: 0.00000221\n",
      "Epoch [1492/2500],\n",
      " Training Loss: 0.00000754\n",
      "Evaluation Loss: 0.00001162\n",
      "Epoch [1493/2500],\n",
      " Training Loss: 0.00001664\n",
      "Evaluation Loss: 0.00000474\n",
      "Epoch [1494/2500],\n",
      " Training Loss: 0.00000890\n",
      "Evaluation Loss: 0.00001084\n",
      "Epoch [1495/2500],\n",
      " Training Loss: 0.00002490\n",
      "Evaluation Loss: 0.00002118\n",
      "Epoch [1496/2500],\n",
      " Training Loss: 0.00002148\n",
      "Evaluation Loss: 0.00001675\n",
      "Epoch [1497/2500],\n",
      " Training Loss: 0.00002186\n",
      "Evaluation Loss: 0.00001642\n",
      "Epoch [1498/2500],\n",
      " Training Loss: 0.00002101\n",
      "Evaluation Loss: 0.00001845\n",
      "Epoch [1499/2500],\n",
      " Training Loss: 0.00002505\n",
      "Evaluation Loss: 0.00004302\n",
      "Epoch [1500/2500],\n",
      " Training Loss: 0.00001706\n",
      "Evaluation Loss: 0.00002200\n",
      "Epoch [1501/2500],\n",
      " Training Loss: 0.00001193\n",
      "Evaluation Loss: 0.00000489\n",
      "Epoch [1502/2500],\n",
      " Training Loss: 0.00000357\n",
      "Evaluation Loss: 0.00000152\n",
      "Epoch [1503/2500],\n",
      " Training Loss: 0.00000571\n",
      "Evaluation Loss: 0.00000752\n",
      "Epoch [1504/2500],\n",
      " Training Loss: 0.00000737\n",
      "Evaluation Loss: 0.00000339\n",
      "Epoch [1505/2500],\n",
      " Training Loss: 0.00001013\n",
      "Evaluation Loss: 0.00000192\n",
      "Epoch [1506/2500],\n",
      " Training Loss: 0.00001389\n",
      "Evaluation Loss: 0.00002415\n",
      "Epoch [1507/2500],\n",
      " Training Loss: 0.00002121\n",
      "Evaluation Loss: 0.00004665\n",
      "Epoch [1508/2500],\n",
      " Training Loss: 0.00003548\n",
      "Evaluation Loss: 0.00001005\n",
      "Epoch [1509/2500],\n",
      " Training Loss: 0.00001863\n",
      "Evaluation Loss: 0.00000477\n",
      "Epoch [1510/2500],\n",
      " Training Loss: 0.00001892\n",
      "Evaluation Loss: 0.00007748\n",
      "Epoch [1511/2500],\n",
      " Training Loss: 0.00003623\n",
      "Evaluation Loss: 0.00001195\n",
      "Epoch [1512/2500],\n",
      " Training Loss: 0.00001189\n",
      "Evaluation Loss: 0.00001170\n",
      "Epoch [1513/2500],\n",
      " Training Loss: 0.00001466\n",
      "Evaluation Loss: 0.00000754\n",
      "Epoch [1514/2500],\n",
      " Training Loss: 0.00000913\n",
      "Evaluation Loss: 0.00000723\n",
      "Epoch [1515/2500],\n",
      " Training Loss: 0.00000865\n",
      "Evaluation Loss: 0.00000439\n",
      "Epoch [1516/2500],\n",
      " Training Loss: 0.00001178\n",
      "Evaluation Loss: 0.00000615\n",
      "Epoch [1517/2500],\n",
      " Training Loss: 0.00001099\n",
      "Evaluation Loss: 0.00000515\n",
      "Epoch [1518/2500],\n",
      " Training Loss: 0.00000823\n",
      "Evaluation Loss: 0.00000785\n",
      "Epoch [1519/2500],\n",
      " Training Loss: 0.00000838\n",
      "Evaluation Loss: 0.00000255\n",
      "Epoch [1520/2500],\n",
      " Training Loss: 0.00000315\n",
      "Evaluation Loss: 0.00000158\n",
      "Epoch [1521/2500],\n",
      " Training Loss: 0.00000236\n",
      "Evaluation Loss: 0.00000133\n",
      "Epoch [1522/2500],\n",
      " Training Loss: 0.00000154\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [1523/2500],\n",
      " Training Loss: 0.00000133\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [1524/2500],\n",
      " Training Loss: 0.00000226\n",
      "Evaluation Loss: 0.00000146\n",
      "Epoch [1525/2500],\n",
      " Training Loss: 0.00000165\n",
      "Evaluation Loss: 0.00000072\n",
      "Epoch [1526/2500],\n",
      " Training Loss: 0.00000153\n",
      "Evaluation Loss: 0.00000211\n",
      "Epoch [1527/2500],\n",
      " Training Loss: 0.00000390\n",
      "Evaluation Loss: 0.00000182\n",
      "Epoch [1528/2500],\n",
      " Training Loss: 0.00000447\n",
      "Evaluation Loss: 0.00000216\n",
      "Epoch [1529/2500],\n",
      " Training Loss: 0.00000730\n",
      "Evaluation Loss: 0.00000678\n",
      "Epoch [1530/2500],\n",
      " Training Loss: 0.00001010\n",
      "Evaluation Loss: 0.00001168\n",
      "Epoch [1531/2500],\n",
      " Training Loss: 0.00001168\n",
      "Evaluation Loss: 0.00001511\n",
      "Epoch [1532/2500],\n",
      " Training Loss: 0.00002891\n",
      "Evaluation Loss: 0.00000678\n",
      "Epoch [1533/2500],\n",
      " Training Loss: 0.00000850\n",
      "Evaluation Loss: 0.00000399\n",
      "Epoch [1534/2500],\n",
      " Training Loss: 0.00001035\n",
      "Evaluation Loss: 0.00000377\n",
      "Epoch [1535/2500],\n",
      " Training Loss: 0.00001274\n",
      "Evaluation Loss: 0.00001515\n",
      "Epoch [1536/2500],\n",
      " Training Loss: 0.00001605\n",
      "Evaluation Loss: 0.00003167\n",
      "Epoch [1537/2500],\n",
      " Training Loss: 0.00003478\n",
      "Evaluation Loss: 0.00000468\n",
      "Epoch [1538/2500],\n",
      " Training Loss: 0.00000609\n",
      "Evaluation Loss: 0.00000246\n",
      "Epoch [1539/2500],\n",
      " Training Loss: 0.00000375\n",
      "Evaluation Loss: 0.00000383\n",
      "Epoch [1540/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000217\n",
      "Epoch [1541/2500],\n",
      " Training Loss: 0.00000318\n",
      "Evaluation Loss: 0.00000190\n",
      "Epoch [1542/2500],\n",
      " Training Loss: 0.00000178\n",
      "Evaluation Loss: 0.00000136\n",
      "Epoch [1543/2500],\n",
      " Training Loss: 0.00000178\n",
      "Evaluation Loss: 0.00000226\n",
      "Epoch [1544/2500],\n",
      " Training Loss: 0.00000388\n",
      "Evaluation Loss: 0.00000557\n",
      "Epoch [1545/2500],\n",
      " Training Loss: 0.00000516\n",
      "Evaluation Loss: 0.00000492\n",
      "Epoch [1546/2500],\n",
      " Training Loss: 0.00000450\n",
      "Evaluation Loss: 0.00000561\n",
      "Epoch [1547/2500],\n",
      " Training Loss: 0.00000803\n",
      "Evaluation Loss: 0.00000345\n",
      "Epoch [1548/2500],\n",
      " Training Loss: 0.00001087\n",
      "Evaluation Loss: 0.00000335\n",
      "Epoch [1549/2500],\n",
      " Training Loss: 0.00000597\n",
      "Evaluation Loss: 0.00000281\n",
      "Epoch [1550/2500],\n",
      " Training Loss: 0.00000540\n",
      "Evaluation Loss: 0.00000753\n",
      "Epoch [1551/2500],\n",
      " Training Loss: 0.00000529\n",
      "Evaluation Loss: 0.00000252\n",
      "Epoch [1552/2500],\n",
      " Training Loss: 0.00000266\n",
      "Evaluation Loss: 0.00000867\n",
      "Epoch [1553/2500],\n",
      " Training Loss: 0.00000515\n",
      "Evaluation Loss: 0.00000243\n",
      "Epoch [1554/2500],\n",
      " Training Loss: 0.00000304\n",
      "Evaluation Loss: 0.00000111\n",
      "Epoch [1555/2500],\n",
      " Training Loss: 0.00000196\n",
      "Evaluation Loss: 0.00000113\n",
      "Epoch [1556/2500],\n",
      " Training Loss: 0.00000204\n",
      "Evaluation Loss: 0.00000275\n",
      "Epoch [1557/2500],\n",
      " Training Loss: 0.00001057\n",
      "Evaluation Loss: 0.00000521\n",
      "Epoch [1558/2500],\n",
      " Training Loss: 0.00000613\n",
      "Evaluation Loss: 0.00000259\n",
      "Epoch [1559/2500],\n",
      " Training Loss: 0.00000686\n",
      "Evaluation Loss: 0.00000435\n",
      "Epoch [1560/2500],\n",
      " Training Loss: 0.00001109\n",
      "Evaluation Loss: 0.00000688\n",
      "Epoch [1561/2500],\n",
      " Training Loss: 0.00000898\n",
      "Evaluation Loss: 0.00000711\n",
      "Epoch [1562/2500],\n",
      " Training Loss: 0.00000575\n",
      "Evaluation Loss: 0.00000315\n",
      "Epoch [1563/2500],\n",
      " Training Loss: 0.00000606\n",
      "Evaluation Loss: 0.00000466\n",
      "Epoch [1564/2500],\n",
      " Training Loss: 0.00000965\n",
      "Evaluation Loss: 0.00000343\n",
      "Epoch [1565/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000468\n",
      "Epoch [1566/2500],\n",
      " Training Loss: 0.00000400\n",
      "Evaluation Loss: 0.00000394\n",
      "Epoch [1567/2500],\n",
      " Training Loss: 0.00000439\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [1568/2500],\n",
      " Training Loss: 0.00000512\n",
      "Evaluation Loss: 0.00000336\n",
      "Epoch [1569/2500],\n",
      " Training Loss: 0.00000508\n",
      "Evaluation Loss: 0.00000309\n",
      "Epoch [1570/2500],\n",
      " Training Loss: 0.00001798\n",
      "Evaluation Loss: 0.00001712\n",
      "Epoch [1571/2500],\n",
      " Training Loss: 0.00002737\n",
      "Evaluation Loss: 0.00001039\n",
      "Epoch [1572/2500],\n",
      " Training Loss: 0.00003580\n",
      "Evaluation Loss: 0.00001099\n",
      "Epoch [1573/2500],\n",
      " Training Loss: 0.00001515\n",
      "Evaluation Loss: 0.00001221\n",
      "Epoch [1574/2500],\n",
      " Training Loss: 0.00001663\n",
      "Evaluation Loss: 0.00001442\n",
      "Epoch [1575/2500],\n",
      " Training Loss: 0.00000909\n",
      "Evaluation Loss: 0.00000183\n",
      "Epoch [1576/2500],\n",
      " Training Loss: 0.00000383\n",
      "Evaluation Loss: 0.00000267\n",
      "Epoch [1577/2500],\n",
      " Training Loss: 0.00000438\n",
      "Evaluation Loss: 0.00000175\n",
      "Epoch [1578/2500],\n",
      " Training Loss: 0.00000335\n",
      "Evaluation Loss: 0.00000256\n",
      "Epoch [1579/2500],\n",
      " Training Loss: 0.00000380\n",
      "Evaluation Loss: 0.00000275\n",
      "Epoch [1580/2500],\n",
      " Training Loss: 0.00001148\n",
      "Evaluation Loss: 0.00001344\n",
      "Epoch [1581/2500],\n",
      " Training Loss: 0.00001528\n",
      "Evaluation Loss: 0.00000589\n",
      "Epoch [1582/2500],\n",
      " Training Loss: 0.00001629\n",
      "Evaluation Loss: 0.00000822\n",
      "Epoch [1583/2500],\n",
      " Training Loss: 0.00000552\n",
      "Evaluation Loss: 0.00000285\n",
      "Epoch [1584/2500],\n",
      " Training Loss: 0.00000613\n",
      "Evaluation Loss: 0.00000357\n",
      "Epoch [1585/2500],\n",
      " Training Loss: 0.00000730\n",
      "Evaluation Loss: 0.00000811\n",
      "Epoch [1586/2500],\n",
      " Training Loss: 0.00000500\n",
      "Evaluation Loss: 0.00000213\n",
      "Epoch [1587/2500],\n",
      " Training Loss: 0.00000430\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [1588/2500],\n",
      " Training Loss: 0.00000297\n",
      "Evaluation Loss: 0.00000279\n",
      "Epoch [1589/2500],\n",
      " Training Loss: 0.00000163\n",
      "Evaluation Loss: 0.00000115\n",
      "Epoch [1590/2500],\n",
      " Training Loss: 0.00000198\n",
      "Evaluation Loss: 0.00000479\n",
      "Epoch [1591/2500],\n",
      " Training Loss: 0.00000321\n",
      "Evaluation Loss: 0.00000378\n",
      "Epoch [1592/2500],\n",
      " Training Loss: 0.00000220\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [1593/2500],\n",
      " Training Loss: 0.00000144\n",
      "Evaluation Loss: 0.00000115\n",
      "Epoch [1594/2500],\n",
      " Training Loss: 0.00000265\n",
      "Evaluation Loss: 0.00001151\n",
      "Epoch [1595/2500],\n",
      " Training Loss: 0.00000622\n",
      "Evaluation Loss: 0.00000286\n",
      "Epoch [1596/2500],\n",
      " Training Loss: 0.00000358\n",
      "Evaluation Loss: 0.00000310\n",
      "Epoch [1597/2500],\n",
      " Training Loss: 0.00000398\n",
      "Evaluation Loss: 0.00000368\n",
      "Epoch [1598/2500],\n",
      " Training Loss: 0.00000628\n",
      "Evaluation Loss: 0.00001077\n",
      "Epoch [1599/2500],\n",
      " Training Loss: 0.00001505\n",
      "Evaluation Loss: 0.00000615\n",
      "Epoch [1600/2500],\n",
      " Training Loss: 0.00000974\n",
      "Evaluation Loss: 0.00000538\n",
      "Epoch [1601/2500],\n",
      " Training Loss: 0.00001230\n",
      "Evaluation Loss: 0.00000892\n",
      "Epoch [1602/2500],\n",
      " Training Loss: 0.00001126\n",
      "Evaluation Loss: 0.00000571\n",
      "Epoch [1603/2500],\n",
      " Training Loss: 0.00001366\n",
      "Evaluation Loss: 0.00000623\n",
      "Epoch [1604/2500],\n",
      " Training Loss: 0.00001498\n",
      "Evaluation Loss: 0.00001206\n",
      "Epoch [1605/2500],\n",
      " Training Loss: 0.00001496\n",
      "Evaluation Loss: 0.00000381\n",
      "Epoch [1606/2500],\n",
      " Training Loss: 0.00001141\n",
      "Evaluation Loss: 0.00000274\n",
      "Epoch [1607/2500],\n",
      " Training Loss: 0.00000338\n",
      "Evaluation Loss: 0.00000215\n",
      "Epoch [1608/2500],\n",
      " Training Loss: 0.00000283\n",
      "Evaluation Loss: 0.00000424\n",
      "Epoch [1609/2500],\n",
      " Training Loss: 0.00000404\n",
      "Evaluation Loss: 0.00000572\n",
      "Epoch [1610/2500],\n",
      " Training Loss: 0.00000658\n",
      "Evaluation Loss: 0.00002872\n",
      "Epoch [1611/2500],\n",
      " Training Loss: 0.00001370\n",
      "Evaluation Loss: 0.00001078\n",
      "Epoch [1612/2500],\n",
      " Training Loss: 0.00000796\n",
      "Evaluation Loss: 0.00001512\n",
      "Epoch [1613/2500],\n",
      " Training Loss: 0.00000694\n",
      "Evaluation Loss: 0.00000536\n",
      "Epoch [1614/2500],\n",
      " Training Loss: 0.00000455\n",
      "Evaluation Loss: 0.00000239\n",
      "Epoch [1615/2500],\n",
      " Training Loss: 0.00000252\n",
      "Evaluation Loss: 0.00000115\n",
      "Epoch [1616/2500],\n",
      " Training Loss: 0.00000188\n",
      "Evaluation Loss: 0.00000269\n",
      "Epoch [1617/2500],\n",
      " Training Loss: 0.00000298\n",
      "Evaluation Loss: 0.00000161\n",
      "Epoch [1618/2500],\n",
      " Training Loss: 0.00000527\n",
      "Evaluation Loss: 0.00000615\n",
      "Epoch [1619/2500],\n",
      " Training Loss: 0.00000348\n",
      "Evaluation Loss: 0.00000149\n",
      "Epoch [1620/2500],\n",
      " Training Loss: 0.00000206\n",
      "Evaluation Loss: 0.00000307\n",
      "Epoch [1621/2500],\n",
      " Training Loss: 0.00000546\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [1622/2500],\n",
      " Training Loss: 0.00000532\n",
      "Evaluation Loss: 0.00000451\n",
      "Epoch [1623/2500],\n",
      " Training Loss: 0.00001177\n",
      "Evaluation Loss: 0.00001262\n",
      "Epoch [1624/2500],\n",
      " Training Loss: 0.00001832\n",
      "Evaluation Loss: 0.00000696\n",
      "Epoch [1625/2500],\n",
      " Training Loss: 0.00001268\n",
      "Evaluation Loss: 0.00001177\n",
      "Epoch [1626/2500],\n",
      " Training Loss: 0.00002202\n",
      "Evaluation Loss: 0.00001417\n",
      "Epoch [1627/2500],\n",
      " Training Loss: 0.00001999\n",
      "Evaluation Loss: 0.00000584\n",
      "Epoch [1628/2500],\n",
      " Training Loss: 0.00002865\n",
      "Evaluation Loss: 0.00002643\n",
      "Epoch [1629/2500],\n",
      " Training Loss: 0.00001547\n",
      "Evaluation Loss: 0.00000691\n",
      "Epoch [1630/2500],\n",
      " Training Loss: 0.00000580\n",
      "Evaluation Loss: 0.00000367\n",
      "Epoch [1631/2500],\n",
      " Training Loss: 0.00000605\n",
      "Evaluation Loss: 0.00000435\n",
      "Epoch [1632/2500],\n",
      " Training Loss: 0.00002571\n",
      "Evaluation Loss: 0.00004847\n",
      "Epoch [1633/2500],\n",
      " Training Loss: 0.00003176\n",
      "Evaluation Loss: 0.00001335\n",
      "Epoch [1634/2500],\n",
      " Training Loss: 0.00003402\n",
      "Evaluation Loss: 0.00002093\n",
      "Epoch [1635/2500],\n",
      " Training Loss: 0.00001763\n",
      "Evaluation Loss: 0.00001258\n",
      "Epoch [1636/2500],\n",
      " Training Loss: 0.00001036\n",
      "Evaluation Loss: 0.00000329\n",
      "Epoch [1637/2500],\n",
      " Training Loss: 0.00000411\n",
      "Evaluation Loss: 0.00000373\n",
      "Epoch [1638/2500],\n",
      " Training Loss: 0.00000264\n",
      "Evaluation Loss: 0.00000145\n",
      "Epoch [1639/2500],\n",
      " Training Loss: 0.00000345\n",
      "Evaluation Loss: 0.00000216\n",
      "Epoch [1640/2500],\n",
      " Training Loss: 0.00000195\n",
      "Evaluation Loss: 0.00000097\n",
      "Epoch [1641/2500],\n",
      " Training Loss: 0.00000146\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [1642/2500],\n",
      " Training Loss: 0.00000133\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1643/2500],\n",
      " Training Loss: 0.00000084\n",
      "Evaluation Loss: 0.00000093\n",
      "Epoch [1644/2500],\n",
      " Training Loss: 0.00000073\n",
      "Evaluation Loss: 0.00000068\n",
      "Epoch [1645/2500],\n",
      " Training Loss: 0.00000076\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1646/2500],\n",
      " Training Loss: 0.00000083\n",
      "Evaluation Loss: 0.00000415\n",
      "Epoch [1647/2500],\n",
      " Training Loss: 0.00000123\n",
      "Evaluation Loss: 0.00000065\n",
      "Epoch [1648/2500],\n",
      " Training Loss: 0.00000087\n",
      "Evaluation Loss: 0.00000067\n",
      "Epoch [1649/2500],\n",
      " Training Loss: 0.00000075\n",
      "Evaluation Loss: 0.00000072\n",
      "Epoch [1650/2500],\n",
      " Training Loss: 0.00000106\n",
      "Evaluation Loss: 0.00000337\n",
      "Epoch [1651/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000135\n",
      "Epoch [1652/2500],\n",
      " Training Loss: 0.00000136\n",
      "Evaluation Loss: 0.00000075\n",
      "Epoch [1653/2500],\n",
      " Training Loss: 0.00000113\n",
      "Evaluation Loss: 0.00000059\n",
      "Epoch [1654/2500],\n",
      " Training Loss: 0.00000174\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1655/2500],\n",
      " Training Loss: 0.00000253\n",
      "Evaluation Loss: 0.00000141\n",
      "Epoch [1656/2500],\n",
      " Training Loss: 0.00000196\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [1657/2500],\n",
      " Training Loss: 0.00000169\n",
      "Evaluation Loss: 0.00000352\n",
      "Epoch [1658/2500],\n",
      " Training Loss: 0.00000400\n",
      "Evaluation Loss: 0.00000357\n",
      "Epoch [1659/2500],\n",
      " Training Loss: 0.00000369\n",
      "Evaluation Loss: 0.00000185\n",
      "Epoch [1660/2500],\n",
      " Training Loss: 0.00000745\n",
      "Evaluation Loss: 0.00000398\n",
      "Epoch [1661/2500],\n",
      " Training Loss: 0.00001031\n",
      "Evaluation Loss: 0.00000636\n",
      "Epoch [1662/2500],\n",
      " Training Loss: 0.00001751\n",
      "Evaluation Loss: 0.00000477\n",
      "Epoch [1663/2500],\n",
      " Training Loss: 0.00002120\n",
      "Evaluation Loss: 0.00002495\n",
      "Epoch [1664/2500],\n",
      " Training Loss: 0.00001761\n",
      "Evaluation Loss: 0.00001461\n",
      "Epoch [1665/2500],\n",
      " Training Loss: 0.00002285\n",
      "Evaluation Loss: 0.00000745\n",
      "Epoch [1666/2500],\n",
      " Training Loss: 0.00001276\n",
      "Evaluation Loss: 0.00001585\n",
      "Epoch [1667/2500],\n",
      " Training Loss: 0.00001797\n",
      "Evaluation Loss: 0.00002183\n",
      "Epoch [1668/2500],\n",
      " Training Loss: 0.00001059\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [1669/2500],\n",
      " Training Loss: 0.00002025\n",
      "Evaluation Loss: 0.00001188\n",
      "Epoch [1670/2500],\n",
      " Training Loss: 0.00003705\n",
      "Evaluation Loss: 0.00001183\n",
      "Epoch [1671/2500],\n",
      " Training Loss: 0.00001028\n",
      "Evaluation Loss: 0.00001224\n",
      "Epoch [1672/2500],\n",
      " Training Loss: 0.00000812\n",
      "Evaluation Loss: 0.00000370\n",
      "Epoch [1673/2500],\n",
      " Training Loss: 0.00000467\n",
      "Evaluation Loss: 0.00000242\n",
      "Epoch [1674/2500],\n",
      " Training Loss: 0.00000397\n",
      "Evaluation Loss: 0.00000364\n",
      "Epoch [1675/2500],\n",
      " Training Loss: 0.00000361\n",
      "Evaluation Loss: 0.00000336\n",
      "Epoch [1676/2500],\n",
      " Training Loss: 0.00000273\n",
      "Evaluation Loss: 0.00000641\n",
      "Epoch [1677/2500],\n",
      " Training Loss: 0.00000281\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [1678/2500],\n",
      " Training Loss: 0.00000215\n",
      "Evaluation Loss: 0.00000171\n",
      "Epoch [1679/2500],\n",
      " Training Loss: 0.00000293\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [1680/2500],\n",
      " Training Loss: 0.00000224\n",
      "Evaluation Loss: 0.00000091\n",
      "Epoch [1681/2500],\n",
      " Training Loss: 0.00000153\n",
      "Evaluation Loss: 0.00000065\n",
      "Epoch [1682/2500],\n",
      " Training Loss: 0.00000103\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [1683/2500],\n",
      " Training Loss: 0.00000416\n",
      "Evaluation Loss: 0.00000203\n",
      "Epoch [1684/2500],\n",
      " Training Loss: 0.00000276\n",
      "Evaluation Loss: 0.00000106\n",
      "Epoch [1685/2500],\n",
      " Training Loss: 0.00000102\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1686/2500],\n",
      " Training Loss: 0.00000107\n",
      "Evaluation Loss: 0.00000067\n",
      "Epoch [1687/2500],\n",
      " Training Loss: 0.00000136\n",
      "Evaluation Loss: 0.00000215\n",
      "Epoch [1688/2500],\n",
      " Training Loss: 0.00000196\n",
      "Evaluation Loss: 0.00000230\n",
      "Epoch [1689/2500],\n",
      " Training Loss: 0.00000416\n",
      "Evaluation Loss: 0.00000917\n",
      "Epoch [1690/2500],\n",
      " Training Loss: 0.00001153\n",
      "Evaluation Loss: 0.00000880\n",
      "Epoch [1691/2500],\n",
      " Training Loss: 0.00000934\n",
      "Evaluation Loss: 0.00001804\n",
      "Epoch [1692/2500],\n",
      " Training Loss: 0.00002831\n",
      "Evaluation Loss: 0.00001322\n",
      "Epoch [1693/2500],\n",
      " Training Loss: 0.00002337\n",
      "Evaluation Loss: 0.00001725\n",
      "Epoch [1694/2500],\n",
      " Training Loss: 0.00002023\n",
      "Evaluation Loss: 0.00001081\n",
      "Epoch [1695/2500],\n",
      " Training Loss: 0.00001378\n",
      "Evaluation Loss: 0.00000916\n",
      "Epoch [1696/2500],\n",
      " Training Loss: 0.00001185\n",
      "Evaluation Loss: 0.00000633\n",
      "Epoch [1697/2500],\n",
      " Training Loss: 0.00002770\n",
      "Evaluation Loss: 0.00002559\n",
      "Epoch [1698/2500],\n",
      " Training Loss: 0.00003716\n",
      "Evaluation Loss: 0.00002025\n",
      "Epoch [1699/2500],\n",
      " Training Loss: 0.00002118\n",
      "Evaluation Loss: 0.00000926\n",
      "Epoch [1700/2500],\n",
      " Training Loss: 0.00000528\n",
      "Evaluation Loss: 0.00000234\n",
      "Epoch [1701/2500],\n",
      " Training Loss: 0.00000357\n",
      "Evaluation Loss: 0.00000447\n",
      "Epoch [1702/2500],\n",
      " Training Loss: 0.00000305\n",
      "Evaluation Loss: 0.00000190\n",
      "Epoch [1703/2500],\n",
      " Training Loss: 0.00000310\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1704/2500],\n",
      " Training Loss: 0.00000269\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [1705/2500],\n",
      " Training Loss: 0.00000229\n",
      "Evaluation Loss: 0.00000127\n",
      "Epoch [1706/2500],\n",
      " Training Loss: 0.00000399\n",
      "Evaluation Loss: 0.00000156\n",
      "Epoch [1707/2500],\n",
      " Training Loss: 0.00000556\n",
      "Evaluation Loss: 0.00000222\n",
      "Epoch [1708/2500],\n",
      " Training Loss: 0.00000440\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1709/2500],\n",
      " Training Loss: 0.00000485\n",
      "Evaluation Loss: 0.00000190\n",
      "Epoch [1710/2500],\n",
      " Training Loss: 0.00000379\n",
      "Evaluation Loss: 0.00000375\n",
      "Epoch [1711/2500],\n",
      " Training Loss: 0.00000885\n",
      "Evaluation Loss: 0.00000534\n",
      "Epoch [1712/2500],\n",
      " Training Loss: 0.00000633\n",
      "Evaluation Loss: 0.00000284\n",
      "Epoch [1713/2500],\n",
      " Training Loss: 0.00000271\n",
      "Evaluation Loss: 0.00000120\n",
      "Epoch [1714/2500],\n",
      " Training Loss: 0.00000148\n",
      "Evaluation Loss: 0.00000090\n",
      "Epoch [1715/2500],\n",
      " Training Loss: 0.00000247\n",
      "Evaluation Loss: 0.00000177\n",
      "Epoch [1716/2500],\n",
      " Training Loss: 0.00000260\n",
      "Evaluation Loss: 0.00000208\n",
      "Epoch [1717/2500],\n",
      " Training Loss: 0.00000234\n",
      "Evaluation Loss: 0.00000197\n",
      "Epoch [1718/2500],\n",
      " Training Loss: 0.00000226\n",
      "Evaluation Loss: 0.00000309\n",
      "Epoch [1719/2500],\n",
      " Training Loss: 0.00000404\n",
      "Evaluation Loss: 0.00000341\n",
      "Epoch [1720/2500],\n",
      " Training Loss: 0.00000835\n",
      "Evaluation Loss: 0.00000842\n",
      "Epoch [1721/2500],\n",
      " Training Loss: 0.00000863\n",
      "Evaluation Loss: 0.00000698\n",
      "Epoch [1722/2500],\n",
      " Training Loss: 0.00000945\n",
      "Evaluation Loss: 0.00000841\n",
      "Epoch [1723/2500],\n",
      " Training Loss: 0.00002801\n",
      "Evaluation Loss: 0.00001480\n",
      "Epoch [1724/2500],\n",
      " Training Loss: 0.00002657\n",
      "Evaluation Loss: 0.00000918\n",
      "Epoch [1725/2500],\n",
      " Training Loss: 0.00004268\n",
      "Evaluation Loss: 0.00000668\n",
      "Epoch [1726/2500],\n",
      " Training Loss: 0.00005284\n",
      "Evaluation Loss: 0.00002177\n",
      "Epoch [1727/2500],\n",
      " Training Loss: 0.00002000\n",
      "Evaluation Loss: 0.00001864\n",
      "Epoch [1728/2500],\n",
      " Training Loss: 0.00001794\n",
      "Evaluation Loss: 0.00003166\n",
      "Epoch [1729/2500],\n",
      " Training Loss: 0.00002291\n",
      "Evaluation Loss: 0.00000604\n",
      "Epoch [1730/2500],\n",
      " Training Loss: 0.00001528\n",
      "Evaluation Loss: 0.00001049\n",
      "Epoch [1731/2500],\n",
      " Training Loss: 0.00000572\n",
      "Evaluation Loss: 0.00000962\n",
      "Epoch [1732/2500],\n",
      " Training Loss: 0.00000581\n",
      "Evaluation Loss: 0.00000679\n",
      "Epoch [1733/2500],\n",
      " Training Loss: 0.00000368\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [1734/2500],\n",
      " Training Loss: 0.00000360\n",
      "Evaluation Loss: 0.00000287\n",
      "Epoch [1735/2500],\n",
      " Training Loss: 0.00000177\n",
      "Evaluation Loss: 0.00000135\n",
      "Epoch [1736/2500],\n",
      " Training Loss: 0.00000155\n",
      "Evaluation Loss: 0.00000318\n",
      "Epoch [1737/2500],\n",
      " Training Loss: 0.00000324\n",
      "Evaluation Loss: 0.00000191\n",
      "Epoch [1738/2500],\n",
      " Training Loss: 0.00000424\n",
      "Evaluation Loss: 0.00000300\n",
      "Epoch [1739/2500],\n",
      " Training Loss: 0.00000400\n",
      "Evaluation Loss: 0.00000280\n",
      "Epoch [1740/2500],\n",
      " Training Loss: 0.00000217\n",
      "Evaluation Loss: 0.00000201\n",
      "Epoch [1741/2500],\n",
      " Training Loss: 0.00000172\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [1742/2500],\n",
      " Training Loss: 0.00000140\n",
      "Evaluation Loss: 0.00000190\n",
      "Epoch [1743/2500],\n",
      " Training Loss: 0.00000222\n",
      "Evaluation Loss: 0.00000300\n",
      "Epoch [1744/2500],\n",
      " Training Loss: 0.00000448\n",
      "Evaluation Loss: 0.00000212\n",
      "Epoch [1745/2500],\n",
      " Training Loss: 0.00000383\n",
      "Evaluation Loss: 0.00000175\n",
      "Epoch [1746/2500],\n",
      " Training Loss: 0.00000198\n",
      "Evaluation Loss: 0.00000270\n",
      "Epoch [1747/2500],\n",
      " Training Loss: 0.00000518\n",
      "Evaluation Loss: 0.00000129\n",
      "Epoch [1748/2500],\n",
      " Training Loss: 0.00000324\n",
      "Evaluation Loss: 0.00000241\n",
      "Epoch [1749/2500],\n",
      " Training Loss: 0.00000492\n",
      "Evaluation Loss: 0.00000328\n",
      "Epoch [1750/2500],\n",
      " Training Loss: 0.00000534\n",
      "Evaluation Loss: 0.00000222\n",
      "Epoch [1751/2500],\n",
      " Training Loss: 0.00000731\n",
      "Evaluation Loss: 0.00001435\n",
      "Epoch [1752/2500],\n",
      " Training Loss: 0.00001727\n",
      "Evaluation Loss: 0.00001129\n",
      "Epoch [1753/2500],\n",
      " Training Loss: 0.00001214\n",
      "Evaluation Loss: 0.00001367\n",
      "Epoch [1754/2500],\n",
      " Training Loss: 0.00000715\n",
      "Evaluation Loss: 0.00000562\n",
      "Epoch [1755/2500],\n",
      " Training Loss: 0.00000722\n",
      "Evaluation Loss: 0.00000511\n",
      "Epoch [1756/2500],\n",
      " Training Loss: 0.00001042\n",
      "Evaluation Loss: 0.00000429\n",
      "Epoch [1757/2500],\n",
      " Training Loss: 0.00000808\n",
      "Evaluation Loss: 0.00000814\n",
      "Epoch [1758/2500],\n",
      " Training Loss: 0.00000556\n",
      "Evaluation Loss: 0.00000517\n",
      "Epoch [1759/2500],\n",
      " Training Loss: 0.00000757\n",
      "Evaluation Loss: 0.00000773\n",
      "Epoch [1760/2500],\n",
      " Training Loss: 0.00000528\n",
      "Evaluation Loss: 0.00000177\n",
      "Epoch [1761/2500],\n",
      " Training Loss: 0.00000389\n",
      "Evaluation Loss: 0.00000296\n",
      "Epoch [1762/2500],\n",
      " Training Loss: 0.00000595\n",
      "Evaluation Loss: 0.00000537\n",
      "Epoch [1763/2500],\n",
      " Training Loss: 0.00000538\n",
      "Evaluation Loss: 0.00000727\n",
      "Epoch [1764/2500],\n",
      " Training Loss: 0.00000395\n",
      "Evaluation Loss: 0.00000173\n",
      "Epoch [1765/2500],\n",
      " Training Loss: 0.00000476\n",
      "Evaluation Loss: 0.00000475\n",
      "Epoch [1766/2500],\n",
      " Training Loss: 0.00000456\n",
      "Evaluation Loss: 0.00000677\n",
      "Epoch [1767/2500],\n",
      " Training Loss: 0.00000519\n",
      "Evaluation Loss: 0.00000389\n",
      "Epoch [1768/2500],\n",
      " Training Loss: 0.00000555\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [1769/2500],\n",
      " Training Loss: 0.00000727\n",
      "Evaluation Loss: 0.00000504\n",
      "Epoch [1770/2500],\n",
      " Training Loss: 0.00001081\n",
      "Evaluation Loss: 0.00000401\n",
      "Epoch [1771/2500],\n",
      " Training Loss: 0.00001343\n",
      "Evaluation Loss: 0.00000481\n",
      "Epoch [1772/2500],\n",
      " Training Loss: 0.00001367\n",
      "Evaluation Loss: 0.00000536\n",
      "Epoch [1773/2500],\n",
      " Training Loss: 0.00001847\n",
      "Evaluation Loss: 0.00000708\n",
      "Epoch [1774/2500],\n",
      " Training Loss: 0.00001344\n",
      "Evaluation Loss: 0.00000511\n",
      "Epoch [1775/2500],\n",
      " Training Loss: 0.00000679\n",
      "Evaluation Loss: 0.00001741\n",
      "Epoch [1776/2500],\n",
      " Training Loss: 0.00001446\n",
      "Evaluation Loss: 0.00000863\n",
      "Epoch [1777/2500],\n",
      " Training Loss: 0.00000844\n",
      "Evaluation Loss: 0.00000786\n",
      "Epoch [1778/2500],\n",
      " Training Loss: 0.00000443\n",
      "Evaluation Loss: 0.00000245\n",
      "Epoch [1779/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000201\n",
      "Epoch [1780/2500],\n",
      " Training Loss: 0.00000327\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [1781/2500],\n",
      " Training Loss: 0.00000289\n",
      "Evaluation Loss: 0.00000151\n",
      "Epoch [1782/2500],\n",
      " Training Loss: 0.00000270\n",
      "Evaluation Loss: 0.00000189\n",
      "Epoch [1783/2500],\n",
      " Training Loss: 0.00000251\n",
      "Evaluation Loss: 0.00000185\n",
      "Epoch [1784/2500],\n",
      " Training Loss: 0.00000256\n",
      "Evaluation Loss: 0.00000204\n",
      "Epoch [1785/2500],\n",
      " Training Loss: 0.00000375\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1786/2500],\n",
      " Training Loss: 0.00000243\n",
      "Evaluation Loss: 0.00000200\n",
      "Epoch [1787/2500],\n",
      " Training Loss: 0.00000243\n",
      "Evaluation Loss: 0.00000164\n",
      "Epoch [1788/2500],\n",
      " Training Loss: 0.00000192\n",
      "Evaluation Loss: 0.00000158\n",
      "Epoch [1789/2500],\n",
      " Training Loss: 0.00000158\n",
      "Evaluation Loss: 0.00000086\n",
      "Epoch [1790/2500],\n",
      " Training Loss: 0.00000120\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [1791/2500],\n",
      " Training Loss: 0.00000121\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [1792/2500],\n",
      " Training Loss: 0.00000134\n",
      "Evaluation Loss: 0.00000073\n",
      "Epoch [1793/2500],\n",
      " Training Loss: 0.00000222\n",
      "Evaluation Loss: 0.00000273\n",
      "Epoch [1794/2500],\n",
      " Training Loss: 0.00000164\n",
      "Evaluation Loss: 0.00000141\n",
      "Epoch [1795/2500],\n",
      " Training Loss: 0.00000183\n",
      "Evaluation Loss: 0.00000084\n",
      "Epoch [1796/2500],\n",
      " Training Loss: 0.00000093\n",
      "Evaluation Loss: 0.00000142\n",
      "Epoch [1797/2500],\n",
      " Training Loss: 0.00000188\n",
      "Evaluation Loss: 0.00000241\n",
      "Epoch [1798/2500],\n",
      " Training Loss: 0.00000320\n",
      "Evaluation Loss: 0.00000668\n",
      "Epoch [1799/2500],\n",
      " Training Loss: 0.00000482\n",
      "Evaluation Loss: 0.00000437\n",
      "Epoch [1800/2500],\n",
      " Training Loss: 0.00002970\n",
      "Evaluation Loss: 0.00001462\n",
      "Epoch [1801/2500],\n",
      " Training Loss: 0.00007755\n",
      "Evaluation Loss: 0.00004914\n",
      "Epoch [1802/2500],\n",
      " Training Loss: 0.00003677\n",
      "Evaluation Loss: 0.00005878\n",
      "Epoch [1803/2500],\n",
      " Training Loss: 0.00002935\n",
      "Evaluation Loss: 0.00006918\n",
      "Epoch [1804/2500],\n",
      " Training Loss: 0.00004413\n",
      "Evaluation Loss: 0.00004238\n",
      "Epoch [1805/2500],\n",
      " Training Loss: 0.00002023\n",
      "Evaluation Loss: 0.00001256\n",
      "Epoch [1806/2500],\n",
      " Training Loss: 0.00001323\n",
      "Evaluation Loss: 0.00000474\n",
      "Epoch [1807/2500],\n",
      " Training Loss: 0.00000553\n",
      "Evaluation Loss: 0.00000863\n",
      "Epoch [1808/2500],\n",
      " Training Loss: 0.00000329\n",
      "Evaluation Loss: 0.00000409\n",
      "Epoch [1809/2500],\n",
      " Training Loss: 0.00000272\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [1810/2500],\n",
      " Training Loss: 0.00000179\n",
      "Evaluation Loss: 0.00000236\n",
      "Epoch [1811/2500],\n",
      " Training Loss: 0.00000109\n",
      "Evaluation Loss: 0.00000087\n",
      "Epoch [1812/2500],\n",
      " Training Loss: 0.00000100\n",
      "Evaluation Loss: 0.00000118\n",
      "Epoch [1813/2500],\n",
      " Training Loss: 0.00000134\n",
      "Evaluation Loss: 0.00000131\n",
      "Epoch [1814/2500],\n",
      " Training Loss: 0.00000098\n",
      "Evaluation Loss: 0.00000143\n",
      "Epoch [1815/2500],\n",
      " Training Loss: 0.00000104\n",
      "Evaluation Loss: 0.00000081\n",
      "Epoch [1816/2500],\n",
      " Training Loss: 0.00000124\n",
      "Evaluation Loss: 0.00000066\n",
      "Epoch [1817/2500],\n",
      " Training Loss: 0.00000154\n",
      "Evaluation Loss: 0.00000110\n",
      "Epoch [1818/2500],\n",
      " Training Loss: 0.00000186\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1819/2500],\n",
      " Training Loss: 0.00000258\n",
      "Evaluation Loss: 0.00000137\n",
      "Epoch [1820/2500],\n",
      " Training Loss: 0.00000453\n",
      "Evaluation Loss: 0.00000424\n",
      "Epoch [1821/2500],\n",
      " Training Loss: 0.00000486\n",
      "Evaluation Loss: 0.00000119\n",
      "Epoch [1822/2500],\n",
      " Training Loss: 0.00000397\n",
      "Evaluation Loss: 0.00000342\n",
      "Epoch [1823/2500],\n",
      " Training Loss: 0.00000550\n",
      "Evaluation Loss: 0.00000799\n",
      "Epoch [1824/2500],\n",
      " Training Loss: 0.00002300\n",
      "Evaluation Loss: 0.00002705\n",
      "Epoch [1825/2500],\n",
      " Training Loss: 0.00001660\n",
      "Evaluation Loss: 0.00001211\n",
      "Epoch [1826/2500],\n",
      " Training Loss: 0.00001237\n",
      "Evaluation Loss: 0.00000492\n",
      "Epoch [1827/2500],\n",
      " Training Loss: 0.00000843\n",
      "Evaluation Loss: 0.00000628\n",
      "Epoch [1828/2500],\n",
      " Training Loss: 0.00000375\n",
      "Evaluation Loss: 0.00000343\n",
      "Epoch [1829/2500],\n",
      " Training Loss: 0.00000302\n",
      "Evaluation Loss: 0.00000300\n",
      "Epoch [1830/2500],\n",
      " Training Loss: 0.00000520\n",
      "Evaluation Loss: 0.00000151\n",
      "Epoch [1831/2500],\n",
      " Training Loss: 0.00000582\n",
      "Evaluation Loss: 0.00000624\n",
      "Epoch [1832/2500],\n",
      " Training Loss: 0.00000795\n",
      "Evaluation Loss: 0.00000403\n",
      "Epoch [1833/2500],\n",
      " Training Loss: 0.00000911\n",
      "Evaluation Loss: 0.00000471\n",
      "Epoch [1834/2500],\n",
      " Training Loss: 0.00000397\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [1835/2500],\n",
      " Training Loss: 0.00000226\n",
      "Evaluation Loss: 0.00000419\n",
      "Epoch [1836/2500],\n",
      " Training Loss: 0.00000486\n",
      "Evaluation Loss: 0.00000900\n",
      "Epoch [1837/2500],\n",
      " Training Loss: 0.00000373\n",
      "Evaluation Loss: 0.00000186\n",
      "Epoch [1838/2500],\n",
      " Training Loss: 0.00000291\n",
      "Evaluation Loss: 0.00000368\n",
      "Epoch [1839/2500],\n",
      " Training Loss: 0.00000186\n",
      "Evaluation Loss: 0.00000110\n",
      "Epoch [1840/2500],\n",
      " Training Loss: 0.00000209\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [1841/2500],\n",
      " Training Loss: 0.00000180\n",
      "Evaluation Loss: 0.00000106\n",
      "Epoch [1842/2500],\n",
      " Training Loss: 0.00000412\n",
      "Evaluation Loss: 0.00000574\n",
      "Epoch [1843/2500],\n",
      " Training Loss: 0.00000732\n",
      "Evaluation Loss: 0.00000656\n",
      "Epoch [1844/2500],\n",
      " Training Loss: 0.00000718\n",
      "Evaluation Loss: 0.00000311\n",
      "Epoch [1845/2500],\n",
      " Training Loss: 0.00000872\n",
      "Evaluation Loss: 0.00000598\n",
      "Epoch [1846/2500],\n",
      " Training Loss: 0.00000898\n",
      "Evaluation Loss: 0.00000947\n",
      "Epoch [1847/2500],\n",
      " Training Loss: 0.00001477\n",
      "Evaluation Loss: 0.00000784\n",
      "Epoch [1848/2500],\n",
      " Training Loss: 0.00002079\n",
      "Evaluation Loss: 0.00003176\n",
      "Epoch [1849/2500],\n",
      " Training Loss: 0.00001180\n",
      "Evaluation Loss: 0.00000749\n",
      "Epoch [1850/2500],\n",
      " Training Loss: 0.00001178\n",
      "Evaluation Loss: 0.00000262\n",
      "Epoch [1851/2500],\n",
      " Training Loss: 0.00001032\n",
      "Evaluation Loss: 0.00000292\n",
      "Epoch [1852/2500],\n",
      " Training Loss: 0.00000396\n",
      "Evaluation Loss: 0.00000608\n",
      "Epoch [1853/2500],\n",
      " Training Loss: 0.00000260\n",
      "Evaluation Loss: 0.00000254\n",
      "Epoch [1854/2500],\n",
      " Training Loss: 0.00000301\n",
      "Evaluation Loss: 0.00000187\n",
      "Epoch [1855/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000087\n",
      "Epoch [1856/2500],\n",
      " Training Loss: 0.00000158\n",
      "Evaluation Loss: 0.00000165\n",
      "Epoch [1857/2500],\n",
      " Training Loss: 0.00000194\n",
      "Evaluation Loss: 0.00000136\n",
      "Epoch [1858/2500],\n",
      " Training Loss: 0.00000367\n",
      "Evaluation Loss: 0.00000186\n",
      "Epoch [1859/2500],\n",
      " Training Loss: 0.00000276\n",
      "Evaluation Loss: 0.00000074\n",
      "Epoch [1860/2500],\n",
      " Training Loss: 0.00000115\n",
      "Evaluation Loss: 0.00000079\n",
      "Epoch [1861/2500],\n",
      " Training Loss: 0.00000149\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [1862/2500],\n",
      " Training Loss: 0.00000168\n",
      "Evaluation Loss: 0.00000095\n",
      "Epoch [1863/2500],\n",
      " Training Loss: 0.00000104\n",
      "Evaluation Loss: 0.00000159\n",
      "Epoch [1864/2500],\n",
      " Training Loss: 0.00000217\n",
      "Evaluation Loss: 0.00000277\n",
      "Epoch [1865/2500],\n",
      " Training Loss: 0.00000348\n",
      "Evaluation Loss: 0.00000141\n",
      "Epoch [1866/2500],\n",
      " Training Loss: 0.00000419\n",
      "Evaluation Loss: 0.00000159\n",
      "Epoch [1867/2500],\n",
      " Training Loss: 0.00000277\n",
      "Evaluation Loss: 0.00000509\n",
      "Epoch [1868/2500],\n",
      " Training Loss: 0.00000604\n",
      "Evaluation Loss: 0.00000743\n",
      "Epoch [1869/2500],\n",
      " Training Loss: 0.00001048\n",
      "Evaluation Loss: 0.00001314\n",
      "Epoch [1870/2500],\n",
      " Training Loss: 0.00000995\n",
      "Evaluation Loss: 0.00002585\n",
      "Epoch [1871/2500],\n",
      " Training Loss: 0.00001931\n",
      "Evaluation Loss: 0.00001965\n",
      "Epoch [1872/2500],\n",
      " Training Loss: 0.00000965\n",
      "Evaluation Loss: 0.00000602\n",
      "Epoch [1873/2500],\n",
      " Training Loss: 0.00000503\n",
      "Evaluation Loss: 0.00000258\n",
      "Epoch [1874/2500],\n",
      " Training Loss: 0.00000332\n",
      "Evaluation Loss: 0.00000223\n",
      "Epoch [1875/2500],\n",
      " Training Loss: 0.00000593\n",
      "Evaluation Loss: 0.00000730\n",
      "Epoch [1876/2500],\n",
      " Training Loss: 0.00001482\n",
      "Evaluation Loss: 0.00001573\n",
      "Epoch [1877/2500],\n",
      " Training Loss: 0.00001870\n",
      "Evaluation Loss: 0.00000436\n",
      "Epoch [1878/2500],\n",
      " Training Loss: 0.00000731\n",
      "Evaluation Loss: 0.00000247\n",
      "Epoch [1879/2500],\n",
      " Training Loss: 0.00000483\n",
      "Evaluation Loss: 0.00000407\n",
      "Epoch [1880/2500],\n",
      " Training Loss: 0.00000473\n",
      "Evaluation Loss: 0.00000170\n",
      "Epoch [1881/2500],\n",
      " Training Loss: 0.00000859\n",
      "Evaluation Loss: 0.00001205\n",
      "Epoch [1882/2500],\n",
      " Training Loss: 0.00003245\n",
      "Evaluation Loss: 0.00001288\n",
      "Epoch [1883/2500],\n",
      " Training Loss: 0.00002716\n",
      "Evaluation Loss: 0.00001411\n",
      "Epoch [1884/2500],\n",
      " Training Loss: 0.00001833\n",
      "Evaluation Loss: 0.00000940\n",
      "Epoch [1885/2500],\n",
      " Training Loss: 0.00001232\n",
      "Evaluation Loss: 0.00003329\n",
      "Epoch [1886/2500],\n",
      " Training Loss: 0.00002980\n",
      "Evaluation Loss: 0.00000424\n",
      "Epoch [1887/2500],\n",
      " Training Loss: 0.00001914\n",
      "Evaluation Loss: 0.00002260\n",
      "Epoch [1888/2500],\n",
      " Training Loss: 0.00004149\n",
      "Evaluation Loss: 0.00005234\n",
      "Epoch [1889/2500],\n",
      " Training Loss: 0.00002113\n",
      "Evaluation Loss: 0.00000952\n",
      "Epoch [1890/2500],\n",
      " Training Loss: 0.00001535\n",
      "Evaluation Loss: 0.00000890\n",
      "Epoch [1891/2500],\n",
      " Training Loss: 0.00001060\n",
      "Evaluation Loss: 0.00000474\n",
      "Epoch [1892/2500],\n",
      " Training Loss: 0.00000523\n",
      "Evaluation Loss: 0.00000145\n",
      "Epoch [1893/2500],\n",
      " Training Loss: 0.00000427\n",
      "Evaluation Loss: 0.00000157\n",
      "Epoch [1894/2500],\n",
      " Training Loss: 0.00000214\n",
      "Evaluation Loss: 0.00000367\n",
      "Epoch [1895/2500],\n",
      " Training Loss: 0.00000198\n",
      "Evaluation Loss: 0.00000151\n",
      "Epoch [1896/2500],\n",
      " Training Loss: 0.00000137\n",
      "Evaluation Loss: 0.00000182\n",
      "Epoch [1897/2500],\n",
      " Training Loss: 0.00000120\n",
      "Evaluation Loss: 0.00000051\n",
      "Epoch [1898/2500],\n",
      " Training Loss: 0.00000100\n",
      "Evaluation Loss: 0.00000136\n",
      "Epoch [1899/2500],\n",
      " Training Loss: 0.00000104\n",
      "Evaluation Loss: 0.00000088\n",
      "Epoch [1900/2500],\n",
      " Training Loss: 0.00000088\n",
      "Evaluation Loss: 0.00000048\n",
      "Epoch [1901/2500],\n",
      " Training Loss: 0.00000081\n",
      "Evaluation Loss: 0.00000066\n",
      "Epoch [1902/2500],\n",
      " Training Loss: 0.00000085\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [1903/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000558\n",
      "Epoch [1904/2500],\n",
      " Training Loss: 0.00000157\n",
      "Evaluation Loss: 0.00000049\n",
      "Epoch [1905/2500],\n",
      " Training Loss: 0.00000058\n",
      "Evaluation Loss: 0.00000045\n",
      "Epoch [1906/2500],\n",
      " Training Loss: 0.00000061\n",
      "Evaluation Loss: 0.00000091\n",
      "Epoch [1907/2500],\n",
      " Training Loss: 0.00000073\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [1908/2500],\n",
      " Training Loss: 0.00000201\n",
      "Evaluation Loss: 0.00000262\n",
      "Epoch [1909/2500],\n",
      " Training Loss: 0.00000172\n",
      "Evaluation Loss: 0.00000084\n",
      "Epoch [1910/2500],\n",
      " Training Loss: 0.00000263\n",
      "Evaluation Loss: 0.00000562\n",
      "Epoch [1911/2500],\n",
      " Training Loss: 0.00000566\n",
      "Evaluation Loss: 0.00000558\n",
      "Epoch [1912/2500],\n",
      " Training Loss: 0.00000501\n",
      "Evaluation Loss: 0.00000193\n",
      "Epoch [1913/2500],\n",
      " Training Loss: 0.00000524\n",
      "Evaluation Loss: 0.00000182\n",
      "Epoch [1914/2500],\n",
      " Training Loss: 0.00000435\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [1915/2500],\n",
      " Training Loss: 0.00000414\n",
      "Evaluation Loss: 0.00000433\n",
      "Epoch [1916/2500],\n",
      " Training Loss: 0.00000723\n",
      "Evaluation Loss: 0.00000295\n",
      "Epoch [1917/2500],\n",
      " Training Loss: 0.00000467\n",
      "Evaluation Loss: 0.00000314\n",
      "Epoch [1918/2500],\n",
      " Training Loss: 0.00000510\n",
      "Evaluation Loss: 0.00000405\n",
      "Epoch [1919/2500],\n",
      " Training Loss: 0.00000354\n",
      "Evaluation Loss: 0.00000210\n",
      "Epoch [1920/2500],\n",
      " Training Loss: 0.00000609\n",
      "Evaluation Loss: 0.00001046\n",
      "Epoch [1921/2500],\n",
      " Training Loss: 0.00000716\n",
      "Evaluation Loss: 0.00000852\n",
      "Epoch [1922/2500],\n",
      " Training Loss: 0.00001008\n",
      "Evaluation Loss: 0.00000339\n",
      "Epoch [1923/2500],\n",
      " Training Loss: 0.00000931\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [1924/2500],\n",
      " Training Loss: 0.00001075\n",
      "Evaluation Loss: 0.00001209\n",
      "Epoch [1925/2500],\n",
      " Training Loss: 0.00001299\n",
      "Evaluation Loss: 0.00000643\n",
      "Epoch [1926/2500],\n",
      " Training Loss: 0.00002007\n",
      "Evaluation Loss: 0.00001192\n",
      "Epoch [1927/2500],\n",
      " Training Loss: 0.00001356\n",
      "Evaluation Loss: 0.00001045\n",
      "Epoch [1928/2500],\n",
      " Training Loss: 0.00001145\n",
      "Evaluation Loss: 0.00000919\n",
      "Epoch [1929/2500],\n",
      " Training Loss: 0.00001152\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [1930/2500],\n",
      " Training Loss: 0.00000659\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [1931/2500],\n",
      " Training Loss: 0.00000688\n",
      "Evaluation Loss: 0.00000395\n",
      "Epoch [1932/2500],\n",
      " Training Loss: 0.00000320\n",
      "Evaluation Loss: 0.00000358\n",
      "Epoch [1933/2500],\n",
      " Training Loss: 0.00000215\n",
      "Evaluation Loss: 0.00000131\n",
      "Epoch [1934/2500],\n",
      " Training Loss: 0.00000145\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [1935/2500],\n",
      " Training Loss: 0.00000116\n",
      "Evaluation Loss: 0.00000205\n",
      "Epoch [1936/2500],\n",
      " Training Loss: 0.00000240\n",
      "Evaluation Loss: 0.00000215\n",
      "Epoch [1937/2500],\n",
      " Training Loss: 0.00000216\n",
      "Evaluation Loss: 0.00000071\n",
      "Epoch [1938/2500],\n",
      " Training Loss: 0.00000190\n",
      "Evaluation Loss: 0.00000520\n",
      "Epoch [1939/2500],\n",
      " Training Loss: 0.00000577\n",
      "Evaluation Loss: 0.00001261\n",
      "Epoch [1940/2500],\n",
      " Training Loss: 0.00000945\n",
      "Evaluation Loss: 0.00000939\n",
      "Epoch [1941/2500],\n",
      " Training Loss: 0.00000652\n",
      "Evaluation Loss: 0.00000744\n",
      "Epoch [1942/2500],\n",
      " Training Loss: 0.00000431\n",
      "Evaluation Loss: 0.00000170\n",
      "Epoch [1943/2500],\n",
      " Training Loss: 0.00000143\n",
      "Evaluation Loss: 0.00000085\n",
      "Epoch [1944/2500],\n",
      " Training Loss: 0.00000150\n",
      "Evaluation Loss: 0.00000090\n",
      "Epoch [1945/2500],\n",
      " Training Loss: 0.00000117\n",
      "Evaluation Loss: 0.00000105\n",
      "Epoch [1946/2500],\n",
      " Training Loss: 0.00000258\n",
      "Evaluation Loss: 0.00000225\n",
      "Epoch [1947/2500],\n",
      " Training Loss: 0.00000211\n",
      "Evaluation Loss: 0.00000060\n",
      "Epoch [1948/2500],\n",
      " Training Loss: 0.00000151\n",
      "Evaluation Loss: 0.00000094\n",
      "Epoch [1949/2500],\n",
      " Training Loss: 0.00000248\n",
      "Evaluation Loss: 0.00000397\n",
      "Epoch [1950/2500],\n",
      " Training Loss: 0.00000366\n",
      "Evaluation Loss: 0.00000190\n",
      "Epoch [1951/2500],\n",
      " Training Loss: 0.00000319\n",
      "Evaluation Loss: 0.00000254\n",
      "Epoch [1952/2500],\n",
      " Training Loss: 0.00000487\n",
      "Evaluation Loss: 0.00000421\n",
      "Epoch [1953/2500],\n",
      " Training Loss: 0.00001030\n",
      "Evaluation Loss: 0.00001825\n",
      "Epoch [1954/2500],\n",
      " Training Loss: 0.00001296\n",
      "Evaluation Loss: 0.00000936\n",
      "Epoch [1955/2500],\n",
      " Training Loss: 0.00000719\n",
      "Evaluation Loss: 0.00000561\n",
      "Epoch [1956/2500],\n",
      " Training Loss: 0.00000565\n",
      "Evaluation Loss: 0.00000507\n",
      "Epoch [1957/2500],\n",
      " Training Loss: 0.00001377\n",
      "Evaluation Loss: 0.00004898\n",
      "Epoch [1958/2500],\n",
      " Training Loss: 0.00001077\n",
      "Evaluation Loss: 0.00000363\n",
      "Epoch [1959/2500],\n",
      " Training Loss: 0.00001698\n",
      "Evaluation Loss: 0.00000718\n",
      "Epoch [1960/2500],\n",
      " Training Loss: 0.00001986\n",
      "Evaluation Loss: 0.00002230\n",
      "Epoch [1961/2500],\n",
      " Training Loss: 0.00002010\n",
      "Evaluation Loss: 0.00001050\n",
      "Epoch [1962/2500],\n",
      " Training Loss: 0.00002484\n",
      "Evaluation Loss: 0.00003721\n",
      "Epoch [1963/2500],\n",
      " Training Loss: 0.00003150\n",
      "Evaluation Loss: 0.00001244\n",
      "Epoch [1964/2500],\n",
      " Training Loss: 0.00002251\n",
      "Evaluation Loss: 0.00000487\n",
      "Epoch [1965/2500],\n",
      " Training Loss: 0.00000980\n",
      "Evaluation Loss: 0.00000847\n",
      "Epoch [1966/2500],\n",
      " Training Loss: 0.00001465\n",
      "Evaluation Loss: 0.00000593\n",
      "Epoch [1967/2500],\n",
      " Training Loss: 0.00000848\n",
      "Evaluation Loss: 0.00000479\n",
      "Epoch [1968/2500],\n",
      " Training Loss: 0.00000636\n",
      "Evaluation Loss: 0.00000347\n",
      "Epoch [1969/2500],\n",
      " Training Loss: 0.00000491\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [1970/2500],\n",
      " Training Loss: 0.00000276\n",
      "Evaluation Loss: 0.00000128\n",
      "Epoch [1971/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [1972/2500],\n",
      " Training Loss: 0.00000197\n",
      "Evaluation Loss: 0.00000264\n",
      "Epoch [1973/2500],\n",
      " Training Loss: 0.00000284\n",
      "Evaluation Loss: 0.00000237\n",
      "Epoch [1974/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000230\n",
      "Epoch [1975/2500],\n",
      " Training Loss: 0.00000335\n",
      "Evaluation Loss: 0.00000292\n",
      "Epoch [1976/2500],\n",
      " Training Loss: 0.00000445\n",
      "Evaluation Loss: 0.00000553\n",
      "Epoch [1977/2500],\n",
      " Training Loss: 0.00000376\n",
      "Evaluation Loss: 0.00000248\n",
      "Epoch [1978/2500],\n",
      " Training Loss: 0.00000145\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [1979/2500],\n",
      " Training Loss: 0.00000162\n",
      "Evaluation Loss: 0.00000137\n",
      "Epoch [1980/2500],\n",
      " Training Loss: 0.00000243\n",
      "Evaluation Loss: 0.00000110\n",
      "Epoch [1981/2500],\n",
      " Training Loss: 0.00000165\n",
      "Evaluation Loss: 0.00000107\n",
      "Epoch [1982/2500],\n",
      " Training Loss: 0.00000363\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [1983/2500],\n",
      " Training Loss: 0.00000352\n",
      "Evaluation Loss: 0.00000252\n",
      "Epoch [1984/2500],\n",
      " Training Loss: 0.00000342\n",
      "Evaluation Loss: 0.00000157\n",
      "Epoch [1985/2500],\n",
      " Training Loss: 0.00000370\n",
      "Evaluation Loss: 0.00000297\n",
      "Epoch [1986/2500],\n",
      " Training Loss: 0.00000713\n",
      "Evaluation Loss: 0.00000290\n",
      "Epoch [1987/2500],\n",
      " Training Loss: 0.00000339\n",
      "Evaluation Loss: 0.00000299\n",
      "Epoch [1988/2500],\n",
      " Training Loss: 0.00000304\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [1989/2500],\n",
      " Training Loss: 0.00000172\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [1990/2500],\n",
      " Training Loss: 0.00000207\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [1991/2500],\n",
      " Training Loss: 0.00000176\n",
      "Evaluation Loss: 0.00000114\n",
      "Epoch [1992/2500],\n",
      " Training Loss: 0.00000111\n",
      "Evaluation Loss: 0.00000086\n",
      "Epoch [1993/2500],\n",
      " Training Loss: 0.00000121\n",
      "Evaluation Loss: 0.00000090\n",
      "Epoch [1994/2500],\n",
      " Training Loss: 0.00000130\n",
      "Evaluation Loss: 0.00000128\n",
      "Epoch [1995/2500],\n",
      " Training Loss: 0.00000327\n",
      "Evaluation Loss: 0.00000323\n",
      "Epoch [1996/2500],\n",
      " Training Loss: 0.00000521\n",
      "Evaluation Loss: 0.00000359\n",
      "Epoch [1997/2500],\n",
      " Training Loss: 0.00000371\n",
      "Evaluation Loss: 0.00000365\n",
      "Epoch [1998/2500],\n",
      " Training Loss: 0.00000538\n",
      "Evaluation Loss: 0.00000577\n",
      "Epoch [1999/2500],\n",
      " Training Loss: 0.00000585\n",
      "Evaluation Loss: 0.00000294\n",
      "Epoch [2000/2500],\n",
      " Training Loss: 0.00000670\n",
      "Evaluation Loss: 0.00000963\n",
      "Epoch [2001/2500],\n",
      " Training Loss: 0.00000865\n",
      "Evaluation Loss: 0.00000626\n",
      "Epoch [2002/2500],\n",
      " Training Loss: 0.00001070\n",
      "Evaluation Loss: 0.00000383\n",
      "Epoch [2003/2500],\n",
      " Training Loss: 0.00001450\n",
      "Evaluation Loss: 0.00001275\n",
      "Epoch [2004/2500],\n",
      " Training Loss: 0.00001632\n",
      "Evaluation Loss: 0.00001434\n",
      "Epoch [2005/2500],\n",
      " Training Loss: 0.00001631\n",
      "Evaluation Loss: 0.00002061\n",
      "Epoch [2006/2500],\n",
      " Training Loss: 0.00001972\n",
      "Evaluation Loss: 0.00001084\n",
      "Epoch [2007/2500],\n",
      " Training Loss: 0.00000846\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [2008/2500],\n",
      " Training Loss: 0.00000622\n",
      "Evaluation Loss: 0.00000378\n",
      "Epoch [2009/2500],\n",
      " Training Loss: 0.00000988\n",
      "Evaluation Loss: 0.00000848\n",
      "Epoch [2010/2500],\n",
      " Training Loss: 0.00000954\n",
      "Evaluation Loss: 0.00000499\n",
      "Epoch [2011/2500],\n",
      " Training Loss: 0.00000523\n",
      "Evaluation Loss: 0.00000303\n",
      "Epoch [2012/2500],\n",
      " Training Loss: 0.00000264\n",
      "Evaluation Loss: 0.00000127\n",
      "Epoch [2013/2500],\n",
      " Training Loss: 0.00000548\n",
      "Evaluation Loss: 0.00000217\n",
      "Epoch [2014/2500],\n",
      " Training Loss: 0.00000370\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [2015/2500],\n",
      " Training Loss: 0.00000290\n",
      "Evaluation Loss: 0.00000169\n",
      "Epoch [2016/2500],\n",
      " Training Loss: 0.00000401\n",
      "Evaluation Loss: 0.00000197\n",
      "Epoch [2017/2500],\n",
      " Training Loss: 0.00000509\n",
      "Evaluation Loss: 0.00000457\n",
      "Epoch [2018/2500],\n",
      " Training Loss: 0.00000719\n",
      "Evaluation Loss: 0.00000482\n",
      "Epoch [2019/2500],\n",
      " Training Loss: 0.00000752\n",
      "Evaluation Loss: 0.00001977\n",
      "Epoch [2020/2500],\n",
      " Training Loss: 0.00001263\n",
      "Evaluation Loss: 0.00000369\n",
      "Epoch [2021/2500],\n",
      " Training Loss: 0.00000705\n",
      "Evaluation Loss: 0.00000411\n",
      "Epoch [2022/2500],\n",
      " Training Loss: 0.00000553\n",
      "Evaluation Loss: 0.00000298\n",
      "Epoch [2023/2500],\n",
      " Training Loss: 0.00000447\n",
      "Evaluation Loss: 0.00000323\n",
      "Epoch [2024/2500],\n",
      " Training Loss: 0.00000531\n",
      "Evaluation Loss: 0.00000295\n",
      "Epoch [2025/2500],\n",
      " Training Loss: 0.00001022\n",
      "Evaluation Loss: 0.00000377\n",
      "Epoch [2026/2500],\n",
      " Training Loss: 0.00000500\n",
      "Evaluation Loss: 0.00000853\n",
      "Epoch [2027/2500],\n",
      " Training Loss: 0.00000574\n",
      "Evaluation Loss: 0.00000613\n",
      "Epoch [2028/2500],\n",
      " Training Loss: 0.00000850\n",
      "Evaluation Loss: 0.00000403\n",
      "Epoch [2029/2500],\n",
      " Training Loss: 0.00000576\n",
      "Evaluation Loss: 0.00000186\n",
      "Epoch [2030/2500],\n",
      " Training Loss: 0.00000552\n",
      "Evaluation Loss: 0.00000632\n",
      "Epoch [2031/2500],\n",
      " Training Loss: 0.00000381\n",
      "Evaluation Loss: 0.00000283\n",
      "Epoch [2032/2500],\n",
      " Training Loss: 0.00000528\n",
      "Evaluation Loss: 0.00000204\n",
      "Epoch [2033/2500],\n",
      " Training Loss: 0.00000597\n",
      "Evaluation Loss: 0.00000279\n",
      "Epoch [2034/2500],\n",
      " Training Loss: 0.00000695\n",
      "Evaluation Loss: 0.00000343\n",
      "Epoch [2035/2500],\n",
      " Training Loss: 0.00000818\n",
      "Evaluation Loss: 0.00000222\n",
      "Epoch [2036/2500],\n",
      " Training Loss: 0.00000798\n",
      "Evaluation Loss: 0.00000654\n",
      "Epoch [2037/2500],\n",
      " Training Loss: 0.00000687\n",
      "Evaluation Loss: 0.00002554\n",
      "Epoch [2038/2500],\n",
      " Training Loss: 0.00001479\n",
      "Evaluation Loss: 0.00001270\n",
      "Epoch [2039/2500],\n",
      " Training Loss: 0.00001855\n",
      "Evaluation Loss: 0.00002725\n",
      "Epoch [2040/2500],\n",
      " Training Loss: 0.00002120\n",
      "Evaluation Loss: 0.00001213\n",
      "Epoch [2041/2500],\n",
      " Training Loss: 0.00001505\n",
      "Evaluation Loss: 0.00000584\n",
      "Epoch [2042/2500],\n",
      " Training Loss: 0.00000986\n",
      "Evaluation Loss: 0.00000429\n",
      "Epoch [2043/2500],\n",
      " Training Loss: 0.00000563\n",
      "Evaluation Loss: 0.00000679\n",
      "Epoch [2044/2500],\n",
      " Training Loss: 0.00000467\n",
      "Evaluation Loss: 0.00000239\n",
      "Epoch [2045/2500],\n",
      " Training Loss: 0.00000982\n",
      "Evaluation Loss: 0.00000927\n",
      "Epoch [2046/2500],\n",
      " Training Loss: 0.00000815\n",
      "Evaluation Loss: 0.00000467\n",
      "Epoch [2047/2500],\n",
      " Training Loss: 0.00000761\n",
      "Evaluation Loss: 0.00000299\n",
      "Epoch [2048/2500],\n",
      " Training Loss: 0.00000386\n",
      "Evaluation Loss: 0.00000440\n",
      "Epoch [2049/2500],\n",
      " Training Loss: 0.00000474\n",
      "Evaluation Loss: 0.00000209\n",
      "Epoch [2050/2500],\n",
      " Training Loss: 0.00000864\n",
      "Evaluation Loss: 0.00001001\n",
      "Epoch [2051/2500],\n",
      " Training Loss: 0.00001380\n",
      "Evaluation Loss: 0.00001368\n",
      "Epoch [2052/2500],\n",
      " Training Loss: 0.00001070\n",
      "Evaluation Loss: 0.00000371\n",
      "Epoch [2053/2500],\n",
      " Training Loss: 0.00001306\n",
      "Evaluation Loss: 0.00000756\n",
      "Epoch [2054/2500],\n",
      " Training Loss: 0.00000719\n",
      "Evaluation Loss: 0.00000589\n",
      "Epoch [2055/2500],\n",
      " Training Loss: 0.00000327\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [2056/2500],\n",
      " Training Loss: 0.00000166\n",
      "Evaluation Loss: 0.00000093\n",
      "Epoch [2057/2500],\n",
      " Training Loss: 0.00000114\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [2058/2500],\n",
      " Training Loss: 0.00000111\n",
      "Evaluation Loss: 0.00000095\n",
      "Epoch [2059/2500],\n",
      " Training Loss: 0.00000112\n",
      "Evaluation Loss: 0.00000248\n",
      "Epoch [2060/2500],\n",
      " Training Loss: 0.00000155\n",
      "Evaluation Loss: 0.00000248\n",
      "Epoch [2061/2500],\n",
      " Training Loss: 0.00000267\n",
      "Evaluation Loss: 0.00000260\n",
      "Epoch [2062/2500],\n",
      " Training Loss: 0.00000939\n",
      "Evaluation Loss: 0.00000888\n",
      "Epoch [2063/2500],\n",
      " Training Loss: 0.00000498\n",
      "Evaluation Loss: 0.00000212\n",
      "Epoch [2064/2500],\n",
      " Training Loss: 0.00000288\n",
      "Evaluation Loss: 0.00000179\n",
      "Epoch [2065/2500],\n",
      " Training Loss: 0.00000258\n",
      "Evaluation Loss: 0.00000793\n",
      "Epoch [2066/2500],\n",
      " Training Loss: 0.00000323\n",
      "Evaluation Loss: 0.00000167\n",
      "Epoch [2067/2500],\n",
      " Training Loss: 0.00000237\n",
      "Evaluation Loss: 0.00000230\n",
      "Epoch [2068/2500],\n",
      " Training Loss: 0.00000191\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [2069/2500],\n",
      " Training Loss: 0.00000197\n",
      "Evaluation Loss: 0.00000436\n",
      "Epoch [2070/2500],\n",
      " Training Loss: 0.00000442\n",
      "Evaluation Loss: 0.00000304\n",
      "Epoch [2071/2500],\n",
      " Training Loss: 0.00000359\n",
      "Evaluation Loss: 0.00000273\n",
      "Epoch [2072/2500],\n",
      " Training Loss: 0.00000496\n",
      "Evaluation Loss: 0.00000320\n",
      "Epoch [2073/2500],\n",
      " Training Loss: 0.00001175\n",
      "Evaluation Loss: 0.00000847\n",
      "Epoch [2074/2500],\n",
      " Training Loss: 0.00001592\n",
      "Evaluation Loss: 0.00001045\n",
      "Epoch [2075/2500],\n",
      " Training Loss: 0.00001223\n",
      "Evaluation Loss: 0.00000553\n",
      "Epoch [2076/2500],\n",
      " Training Loss: 0.00001471\n",
      "Evaluation Loss: 0.00001423\n",
      "Epoch [2077/2500],\n",
      " Training Loss: 0.00001817\n",
      "Evaluation Loss: 0.00000751\n",
      "Epoch [2078/2500],\n",
      " Training Loss: 0.00001884\n",
      "Evaluation Loss: 0.00002796\n",
      "Epoch [2079/2500],\n",
      " Training Loss: 0.00003603\n",
      "Evaluation Loss: 0.00001939\n",
      "Epoch [2080/2500],\n",
      " Training Loss: 0.00006982\n",
      "Evaluation Loss: 0.00007509\n",
      "Epoch [2081/2500],\n",
      " Training Loss: 0.00003108\n",
      "Evaluation Loss: 0.00000484\n",
      "Epoch [2082/2500],\n",
      " Training Loss: 0.00001391\n",
      "Evaluation Loss: 0.00000656\n",
      "Epoch [2083/2500],\n",
      " Training Loss: 0.00001185\n",
      "Evaluation Loss: 0.00000396\n",
      "Epoch [2084/2500],\n",
      " Training Loss: 0.00000677\n",
      "Evaluation Loss: 0.00000319\n",
      "Epoch [2085/2500],\n",
      " Training Loss: 0.00000459\n",
      "Evaluation Loss: 0.00000352\n",
      "Epoch [2086/2500],\n",
      " Training Loss: 0.00000424\n",
      "Evaluation Loss: 0.00000387\n",
      "Epoch [2087/2500],\n",
      " Training Loss: 0.00000454\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [2088/2500],\n",
      " Training Loss: 0.00000184\n",
      "Evaluation Loss: 0.00000087\n",
      "Epoch [2089/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000274\n",
      "Epoch [2090/2500],\n",
      " Training Loss: 0.00000157\n",
      "Evaluation Loss: 0.00000151\n",
      "Epoch [2091/2500],\n",
      " Training Loss: 0.00000137\n",
      "Evaluation Loss: 0.00000126\n",
      "Epoch [2092/2500],\n",
      " Training Loss: 0.00000181\n",
      "Evaluation Loss: 0.00000073\n",
      "Epoch [2093/2500],\n",
      " Training Loss: 0.00000139\n",
      "Evaluation Loss: 0.00000139\n",
      "Epoch [2094/2500],\n",
      " Training Loss: 0.00000178\n",
      "Evaluation Loss: 0.00000113\n",
      "Epoch [2095/2500],\n",
      " Training Loss: 0.00000214\n",
      "Evaluation Loss: 0.00000102\n",
      "Epoch [2096/2500],\n",
      " Training Loss: 0.00000184\n",
      "Evaluation Loss: 0.00000120\n",
      "Epoch [2097/2500],\n",
      " Training Loss: 0.00000206\n",
      "Evaluation Loss: 0.00000148\n",
      "Epoch [2098/2500],\n",
      " Training Loss: 0.00000147\n",
      "Evaluation Loss: 0.00000179\n",
      "Epoch [2099/2500],\n",
      " Training Loss: 0.00000161\n",
      "Evaluation Loss: 0.00000221\n",
      "Epoch [2100/2500],\n",
      " Training Loss: 0.00000214\n",
      "Evaluation Loss: 0.00000367\n",
      "Epoch [2101/2500],\n",
      " Training Loss: 0.00000719\n",
      "Evaluation Loss: 0.00000479\n",
      "Epoch [2102/2500],\n",
      " Training Loss: 0.00000846\n",
      "Evaluation Loss: 0.00000185\n",
      "Epoch [2103/2500],\n",
      " Training Loss: 0.00000554\n",
      "Evaluation Loss: 0.00001378\n",
      "Epoch [2104/2500],\n",
      " Training Loss: 0.00000827\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [2105/2500],\n",
      " Training Loss: 0.00000501\n",
      "Evaluation Loss: 0.00000381\n",
      "Epoch [2106/2500],\n",
      " Training Loss: 0.00000744\n",
      "Evaluation Loss: 0.00000485\n",
      "Epoch [2107/2500],\n",
      " Training Loss: 0.00000435\n",
      "Evaluation Loss: 0.00000195\n",
      "Epoch [2108/2500],\n",
      " Training Loss: 0.00000406\n",
      "Evaluation Loss: 0.00000152\n",
      "Epoch [2109/2500],\n",
      " Training Loss: 0.00000377\n",
      "Evaluation Loss: 0.00000216\n",
      "Epoch [2110/2500],\n",
      " Training Loss: 0.00000735\n",
      "Evaluation Loss: 0.00000611\n",
      "Epoch [2111/2500],\n",
      " Training Loss: 0.00000916\n",
      "Evaluation Loss: 0.00000427\n",
      "Epoch [2112/2500],\n",
      " Training Loss: 0.00000410\n",
      "Evaluation Loss: 0.00000132\n",
      "Epoch [2113/2500],\n",
      " Training Loss: 0.00000358\n",
      "Evaluation Loss: 0.00000523\n",
      "Epoch [2114/2500],\n",
      " Training Loss: 0.00000487\n",
      "Evaluation Loss: 0.00000351\n",
      "Epoch [2115/2500],\n",
      " Training Loss: 0.00000507\n",
      "Evaluation Loss: 0.00000393\n",
      "Epoch [2116/2500],\n",
      " Training Loss: 0.00000484\n",
      "Evaluation Loss: 0.00000174\n",
      "Epoch [2117/2500],\n",
      " Training Loss: 0.00000409\n",
      "Evaluation Loss: 0.00000380\n",
      "Epoch [2118/2500],\n",
      " Training Loss: 0.00000634\n",
      "Evaluation Loss: 0.00000482\n",
      "Epoch [2119/2500],\n",
      " Training Loss: 0.00000457\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [2120/2500],\n",
      " Training Loss: 0.00000243\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [2121/2500],\n",
      " Training Loss: 0.00000310\n",
      "Evaluation Loss: 0.00000176\n",
      "Epoch [2122/2500],\n",
      " Training Loss: 0.00000190\n",
      "Evaluation Loss: 0.00000101\n",
      "Epoch [2123/2500],\n",
      " Training Loss: 0.00000140\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [2124/2500],\n",
      " Training Loss: 0.00000115\n",
      "Evaluation Loss: 0.00000084\n",
      "Epoch [2125/2500],\n",
      " Training Loss: 0.00000111\n",
      "Evaluation Loss: 0.00000063\n",
      "Epoch [2126/2500],\n",
      " Training Loss: 0.00000108\n",
      "Evaluation Loss: 0.00000152\n",
      "Epoch [2127/2500],\n",
      " Training Loss: 0.00000322\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [2128/2500],\n",
      " Training Loss: 0.00000172\n",
      "Evaluation Loss: 0.00000166\n",
      "Epoch [2129/2500],\n",
      " Training Loss: 0.00000297\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [2130/2500],\n",
      " Training Loss: 0.00000179\n",
      "Evaluation Loss: 0.00000116\n",
      "Epoch [2131/2500],\n",
      " Training Loss: 0.00000131\n",
      "Evaluation Loss: 0.00000133\n",
      "Epoch [2132/2500],\n",
      " Training Loss: 0.00000126\n",
      "Evaluation Loss: 0.00000263\n",
      "Epoch [2133/2500],\n",
      " Training Loss: 0.00000225\n",
      "Evaluation Loss: 0.00000198\n",
      "Epoch [2134/2500],\n",
      " Training Loss: 0.00000260\n",
      "Evaluation Loss: 0.00000589\n",
      "Epoch [2135/2500],\n",
      " Training Loss: 0.00000826\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [2136/2500],\n",
      " Training Loss: 0.00000771\n",
      "Evaluation Loss: 0.00000521\n",
      "Epoch [2137/2500],\n",
      " Training Loss: 0.00001233\n",
      "Evaluation Loss: 0.00001025\n",
      "Epoch [2138/2500],\n",
      " Training Loss: 0.00001320\n",
      "Evaluation Loss: 0.00001908\n",
      "Epoch [2139/2500],\n",
      " Training Loss: 0.00001599\n",
      "Evaluation Loss: 0.00000541\n",
      "Epoch [2140/2500],\n",
      " Training Loss: 0.00001198\n",
      "Evaluation Loss: 0.00001233\n",
      "Epoch [2141/2500],\n",
      " Training Loss: 0.00001217\n",
      "Evaluation Loss: 0.00000621\n",
      "Epoch [2142/2500],\n",
      " Training Loss: 0.00000772\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [2143/2500],\n",
      " Training Loss: 0.00000838\n",
      "Evaluation Loss: 0.00000525\n",
      "Epoch [2144/2500],\n",
      " Training Loss: 0.00000675\n",
      "Evaluation Loss: 0.00000197\n",
      "Epoch [2145/2500],\n",
      " Training Loss: 0.00000612\n",
      "Evaluation Loss: 0.00001141\n",
      "Epoch [2146/2500],\n",
      " Training Loss: 0.00000892\n",
      "Evaluation Loss: 0.00001051\n",
      "Epoch [2147/2500],\n",
      " Training Loss: 0.00003296\n",
      "Evaluation Loss: 0.00001582\n",
      "Epoch [2148/2500],\n",
      " Training Loss: 0.00003395\n",
      "Evaluation Loss: 0.00000387\n",
      "Epoch [2149/2500],\n",
      " Training Loss: 0.00001066\n",
      "Evaluation Loss: 0.00001028\n",
      "Epoch [2150/2500],\n",
      " Training Loss: 0.00001082\n",
      "Evaluation Loss: 0.00000303\n",
      "Epoch [2151/2500],\n",
      " Training Loss: 0.00000481\n",
      "Evaluation Loss: 0.00000178\n",
      "Epoch [2152/2500],\n",
      " Training Loss: 0.00000554\n",
      "Evaluation Loss: 0.00000333\n",
      "Epoch [2153/2500],\n",
      " Training Loss: 0.00000548\n",
      "Evaluation Loss: 0.00000420\n",
      "Epoch [2154/2500],\n",
      " Training Loss: 0.00000622\n",
      "Evaluation Loss: 0.00000109\n",
      "Epoch [2155/2500],\n",
      " Training Loss: 0.00000263\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [2156/2500],\n",
      " Training Loss: 0.00000277\n",
      "Evaluation Loss: 0.00000193\n",
      "Epoch [2157/2500],\n",
      " Training Loss: 0.00000397\n",
      "Evaluation Loss: 0.00000312\n",
      "Epoch [2158/2500],\n",
      " Training Loss: 0.00000286\n",
      "Evaluation Loss: 0.00000289\n",
      "Epoch [2159/2500],\n",
      " Training Loss: 0.00000336\n",
      "Evaluation Loss: 0.00000531\n",
      "Epoch [2160/2500],\n",
      " Training Loss: 0.00000198\n",
      "Evaluation Loss: 0.00000138\n",
      "Epoch [2161/2500],\n",
      " Training Loss: 0.00000095\n",
      "Evaluation Loss: 0.00000057\n",
      "Epoch [2162/2500],\n",
      " Training Loss: 0.00000087\n",
      "Evaluation Loss: 0.00000064\n",
      "Epoch [2163/2500],\n",
      " Training Loss: 0.00000095\n",
      "Evaluation Loss: 0.00000068\n",
      "Epoch [2164/2500],\n",
      " Training Loss: 0.00000095\n",
      "Evaluation Loss: 0.00000067\n",
      "Epoch [2165/2500],\n",
      " Training Loss: 0.00000085\n",
      "Evaluation Loss: 0.00000091\n",
      "Epoch [2166/2500],\n",
      " Training Loss: 0.00000114\n",
      "Evaluation Loss: 0.00000211\n",
      "Epoch [2167/2500],\n",
      " Training Loss: 0.00000172\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [2168/2500],\n",
      " Training Loss: 0.00000133\n",
      "Evaluation Loss: 0.00000148\n",
      "Epoch [2169/2500],\n",
      " Training Loss: 0.00000118\n",
      "Evaluation Loss: 0.00000085\n",
      "Epoch [2170/2500],\n",
      " Training Loss: 0.00000347\n",
      "Evaluation Loss: 0.00000602\n",
      "Epoch [2171/2500],\n",
      " Training Loss: 0.00000445\n",
      "Evaluation Loss: 0.00000529\n",
      "Epoch [2172/2500],\n",
      " Training Loss: 0.00001279\n",
      "Evaluation Loss: 0.00000486\n",
      "Epoch [2173/2500],\n",
      " Training Loss: 0.00001604\n",
      "Evaluation Loss: 0.00001211\n",
      "Epoch [2174/2500],\n",
      " Training Loss: 0.00001434\n",
      "Evaluation Loss: 0.00000623\n",
      "Epoch [2175/2500],\n",
      " Training Loss: 0.00001362\n",
      "Evaluation Loss: 0.00001310\n",
      "Epoch [2176/2500],\n",
      " Training Loss: 0.00001453\n",
      "Evaluation Loss: 0.00000556\n",
      "Epoch [2177/2500],\n",
      " Training Loss: 0.00001006\n",
      "Evaluation Loss: 0.00002564\n",
      "Epoch [2178/2500],\n",
      " Training Loss: 0.00001166\n",
      "Evaluation Loss: 0.00000225\n",
      "Epoch [2179/2500],\n",
      " Training Loss: 0.00000813\n",
      "Evaluation Loss: 0.00000429\n",
      "Epoch [2180/2500],\n",
      " Training Loss: 0.00001130\n",
      "Evaluation Loss: 0.00000519\n",
      "Epoch [2181/2500],\n",
      " Training Loss: 0.00000971\n",
      "Evaluation Loss: 0.00000973\n",
      "Epoch [2182/2500],\n",
      " Training Loss: 0.00001045\n",
      "Evaluation Loss: 0.00000712\n",
      "Epoch [2183/2500],\n",
      " Training Loss: 0.00000728\n",
      "Evaluation Loss: 0.00000287\n",
      "Epoch [2184/2500],\n",
      " Training Loss: 0.00000548\n",
      "Evaluation Loss: 0.00000462\n",
      "Epoch [2185/2500],\n",
      " Training Loss: 0.00000700\n",
      "Evaluation Loss: 0.00000293\n",
      "Epoch [2186/2500],\n",
      " Training Loss: 0.00000418\n",
      "Evaluation Loss: 0.00000186\n",
      "Epoch [2187/2500],\n",
      " Training Loss: 0.00000272\n",
      "Evaluation Loss: 0.00000363\n",
      "Epoch [2188/2500],\n",
      " Training Loss: 0.00000229\n",
      "Evaluation Loss: 0.00000156\n",
      "Epoch [2189/2500],\n",
      " Training Loss: 0.00000106\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [2190/2500],\n",
      " Training Loss: 0.00000107\n",
      "Evaluation Loss: 0.00000086\n",
      "Epoch [2191/2500],\n",
      " Training Loss: 0.00000156\n",
      "Evaluation Loss: 0.00000076\n",
      "Epoch [2192/2500],\n",
      " Training Loss: 0.00000082\n",
      "Evaluation Loss: 0.00000054\n",
      "Epoch [2193/2500],\n",
      " Training Loss: 0.00000098\n",
      "Evaluation Loss: 0.00000114\n",
      "Epoch [2194/2500],\n",
      " Training Loss: 0.00000099\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [2195/2500],\n",
      " Training Loss: 0.00000176\n",
      "Evaluation Loss: 0.00000369\n",
      "Epoch [2196/2500],\n",
      " Training Loss: 0.00000313\n",
      "Evaluation Loss: 0.00000180\n",
      "Epoch [2197/2500],\n",
      " Training Loss: 0.00000251\n",
      "Evaluation Loss: 0.00000136\n",
      "Epoch [2198/2500],\n",
      " Training Loss: 0.00000553\n",
      "Evaluation Loss: 0.00000249\n",
      "Epoch [2199/2500],\n",
      " Training Loss: 0.00000394\n",
      "Evaluation Loss: 0.00000117\n",
      "Epoch [2200/2500],\n",
      " Training Loss: 0.00000371\n",
      "Evaluation Loss: 0.00000444\n",
      "Epoch [2201/2500],\n",
      " Training Loss: 0.00000600\n",
      "Evaluation Loss: 0.00000439\n",
      "Epoch [2202/2500],\n",
      " Training Loss: 0.00000441\n",
      "Evaluation Loss: 0.00000402\n",
      "Epoch [2203/2500],\n",
      " Training Loss: 0.00000470\n",
      "Evaluation Loss: 0.00000466\n",
      "Epoch [2204/2500],\n",
      " Training Loss: 0.00000686\n",
      "Evaluation Loss: 0.00000323\n",
      "Epoch [2205/2500],\n",
      " Training Loss: 0.00000379\n",
      "Evaluation Loss: 0.00000320\n",
      "Epoch [2206/2500],\n",
      " Training Loss: 0.00000501\n",
      "Evaluation Loss: 0.00000527\n",
      "Epoch [2207/2500],\n",
      " Training Loss: 0.00006046\n",
      "Evaluation Loss: 0.00009977\n",
      "Epoch [2208/2500],\n",
      " Training Loss: 0.00008357\n",
      "Evaluation Loss: 0.00004039\n",
      "Epoch [2209/2500],\n",
      " Training Loss: 0.00005998\n",
      "Evaluation Loss: 0.00005420\n",
      "Epoch [2210/2500],\n",
      " Training Loss: 0.00004289\n",
      "Evaluation Loss: 0.00001132\n",
      "Epoch [2211/2500],\n",
      " Training Loss: 0.00000821\n",
      "Evaluation Loss: 0.00000204\n",
      "Epoch [2212/2500],\n",
      " Training Loss: 0.00000360\n",
      "Evaluation Loss: 0.00000146\n",
      "Epoch [2213/2500],\n",
      " Training Loss: 0.00000231\n",
      "Evaluation Loss: 0.00000273\n",
      "Epoch [2214/2500],\n",
      " Training Loss: 0.00000200\n",
      "Evaluation Loss: 0.00000109\n",
      "Epoch [2215/2500],\n",
      " Training Loss: 0.00000131\n",
      "Evaluation Loss: 0.00000073\n",
      "Epoch [2216/2500],\n",
      " Training Loss: 0.00000129\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [2217/2500],\n",
      " Training Loss: 0.00000103\n",
      "Evaluation Loss: 0.00000062\n",
      "Epoch [2218/2500],\n",
      " Training Loss: 0.00000094\n",
      "Evaluation Loss: 0.00000083\n",
      "Epoch [2219/2500],\n",
      " Training Loss: 0.00000083\n",
      "Evaluation Loss: 0.00000075\n",
      "Epoch [2220/2500],\n",
      " Training Loss: 0.00000094\n",
      "Evaluation Loss: 0.00000060\n",
      "Epoch [2221/2500],\n",
      " Training Loss: 0.00000065\n",
      "Evaluation Loss: 0.00000087\n",
      "Epoch [2222/2500],\n",
      " Training Loss: 0.00000072\n",
      "Evaluation Loss: 0.00000099\n",
      "Epoch [2223/2500],\n",
      " Training Loss: 0.00000118\n",
      "Evaluation Loss: 0.00000070\n",
      "Epoch [2224/2500],\n",
      " Training Loss: 0.00000102\n",
      "Evaluation Loss: 0.00000074\n",
      "Epoch [2225/2500],\n",
      " Training Loss: 0.00000094\n",
      "Evaluation Loss: 0.00000097\n",
      "Epoch [2226/2500],\n",
      " Training Loss: 0.00000088\n",
      "Evaluation Loss: 0.00000116\n",
      "Epoch [2227/2500],\n",
      " Training Loss: 0.00000141\n",
      "Evaluation Loss: 0.00000199\n",
      "Epoch [2228/2500],\n",
      " Training Loss: 0.00000133\n",
      "Evaluation Loss: 0.00000102\n",
      "Epoch [2229/2500],\n",
      " Training Loss: 0.00000092\n",
      "Evaluation Loss: 0.00000067\n",
      "Epoch [2230/2500],\n",
      " Training Loss: 0.00000115\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [2231/2500],\n",
      " Training Loss: 0.00000090\n",
      "Evaluation Loss: 0.00000073\n",
      "Epoch [2232/2500],\n",
      " Training Loss: 0.00000112\n",
      "Evaluation Loss: 0.00000079\n",
      "Epoch [2233/2500],\n",
      " Training Loss: 0.00000114\n",
      "Evaluation Loss: 0.00000057\n",
      "Epoch [2234/2500],\n",
      " Training Loss: 0.00000095\n",
      "Evaluation Loss: 0.00000097\n",
      "Epoch [2235/2500],\n",
      " Training Loss: 0.00000144\n",
      "Evaluation Loss: 0.00000075\n",
      "Epoch [2236/2500],\n",
      " Training Loss: 0.00000478\n",
      "Evaluation Loss: 0.00000261\n",
      "Epoch [2237/2500],\n",
      " Training Loss: 0.00000520\n",
      "Evaluation Loss: 0.00000420\n",
      "Epoch [2238/2500],\n",
      " Training Loss: 0.00000374\n",
      "Evaluation Loss: 0.00000146\n",
      "Epoch [2239/2500],\n",
      " Training Loss: 0.00000318\n",
      "Evaluation Loss: 0.00000219\n",
      "Epoch [2240/2500],\n",
      " Training Loss: 0.00000464\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [2241/2500],\n",
      " Training Loss: 0.00000873\n",
      "Evaluation Loss: 0.00000579\n",
      "Epoch [2242/2500],\n",
      " Training Loss: 0.00000482\n",
      "Evaluation Loss: 0.00000242\n",
      "Epoch [2243/2500],\n",
      " Training Loss: 0.00000599\n",
      "Evaluation Loss: 0.00001283\n",
      "Epoch [2244/2500],\n",
      " Training Loss: 0.00001458\n",
      "Evaluation Loss: 0.00000770\n",
      "Epoch [2245/2500],\n",
      " Training Loss: 0.00001280\n",
      "Evaluation Loss: 0.00000326\n",
      "Epoch [2246/2500],\n",
      " Training Loss: 0.00002986\n",
      "Evaluation Loss: 0.00001807\n",
      "Epoch [2247/2500],\n",
      " Training Loss: 0.00002700\n",
      "Evaluation Loss: 0.00001895\n",
      "Epoch [2248/2500],\n",
      " Training Loss: 0.00003422\n",
      "Evaluation Loss: 0.00001713\n",
      "Epoch [2249/2500],\n",
      " Training Loss: 0.00001698\n",
      "Evaluation Loss: 0.00000668\n",
      "Epoch [2250/2500],\n",
      " Training Loss: 0.00001041\n",
      "Evaluation Loss: 0.00000736\n",
      "Epoch [2251/2500],\n",
      " Training Loss: 0.00001368\n",
      "Evaluation Loss: 0.00000620\n",
      "Epoch [2252/2500],\n",
      " Training Loss: 0.00000889\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [2253/2500],\n",
      " Training Loss: 0.00000405\n",
      "Evaluation Loss: 0.00000141\n",
      "Epoch [2254/2500],\n",
      " Training Loss: 0.00000316\n",
      "Evaluation Loss: 0.00000565\n",
      "Epoch [2255/2500],\n",
      " Training Loss: 0.00000599\n",
      "Evaluation Loss: 0.00000586\n",
      "Epoch [2256/2500],\n",
      " Training Loss: 0.00000469\n",
      "Evaluation Loss: 0.00000099\n",
      "Epoch [2257/2500],\n",
      " Training Loss: 0.00000194\n",
      "Evaluation Loss: 0.00000125\n",
      "Epoch [2258/2500],\n",
      " Training Loss: 0.00000211\n",
      "Evaluation Loss: 0.00000202\n",
      "Epoch [2259/2500],\n",
      " Training Loss: 0.00000265\n",
      "Evaluation Loss: 0.00000178\n",
      "Epoch [2260/2500],\n",
      " Training Loss: 0.00000379\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [2261/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000400\n",
      "Epoch [2262/2500],\n",
      " Training Loss: 0.00000310\n",
      "Evaluation Loss: 0.00000236\n",
      "Epoch [2263/2500],\n",
      " Training Loss: 0.00000186\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [2264/2500],\n",
      " Training Loss: 0.00000178\n",
      "Evaluation Loss: 0.00000075\n",
      "Epoch [2265/2500],\n",
      " Training Loss: 0.00000126\n",
      "Evaluation Loss: 0.00000072\n",
      "Epoch [2266/2500],\n",
      " Training Loss: 0.00000145\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [2267/2500],\n",
      " Training Loss: 0.00000136\n",
      "Evaluation Loss: 0.00000137\n",
      "Epoch [2268/2500],\n",
      " Training Loss: 0.00000188\n",
      "Evaluation Loss: 0.00000109\n",
      "Epoch [2269/2500],\n",
      " Training Loss: 0.00000135\n",
      "Evaluation Loss: 0.00000128\n",
      "Epoch [2270/2500],\n",
      " Training Loss: 0.00000149\n",
      "Evaluation Loss: 0.00000068\n",
      "Epoch [2271/2500],\n",
      " Training Loss: 0.00000157\n",
      "Evaluation Loss: 0.00000327\n",
      "Epoch [2272/2500],\n",
      " Training Loss: 0.00000223\n",
      "Evaluation Loss: 0.00000217\n",
      "Epoch [2273/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000097\n",
      "Epoch [2274/2500],\n",
      " Training Loss: 0.00000321\n",
      "Evaluation Loss: 0.00000241\n",
      "Epoch [2275/2500],\n",
      " Training Loss: 0.00000502\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [2276/2500],\n",
      " Training Loss: 0.00000431\n",
      "Evaluation Loss: 0.00000183\n",
      "Epoch [2277/2500],\n",
      " Training Loss: 0.00000563\n",
      "Evaluation Loss: 0.00000519\n",
      "Epoch [2278/2500],\n",
      " Training Loss: 0.00000577\n",
      "Evaluation Loss: 0.00000188\n",
      "Epoch [2279/2500],\n",
      " Training Loss: 0.00001707\n",
      "Evaluation Loss: 0.00002390\n",
      "Epoch [2280/2500],\n",
      " Training Loss: 0.00001670\n",
      "Evaluation Loss: 0.00000777\n",
      "Epoch [2281/2500],\n",
      " Training Loss: 0.00000926\n",
      "Evaluation Loss: 0.00000642\n",
      "Epoch [2282/2500],\n",
      " Training Loss: 0.00001300\n",
      "Evaluation Loss: 0.00000599\n",
      "Epoch [2283/2500],\n",
      " Training Loss: 0.00000421\n",
      "Evaluation Loss: 0.00000147\n",
      "Epoch [2284/2500],\n",
      " Training Loss: 0.00000139\n",
      "Evaluation Loss: 0.00000067\n",
      "Epoch [2285/2500],\n",
      " Training Loss: 0.00000108\n",
      "Evaluation Loss: 0.00000070\n",
      "Epoch [2286/2500],\n",
      " Training Loss: 0.00000117\n",
      "Evaluation Loss: 0.00000093\n",
      "Epoch [2287/2500],\n",
      " Training Loss: 0.00000098\n",
      "Evaluation Loss: 0.00000080\n",
      "Epoch [2288/2500],\n",
      " Training Loss: 0.00000155\n",
      "Evaluation Loss: 0.00000115\n",
      "Epoch [2289/2500],\n",
      " Training Loss: 0.00000072\n",
      "Evaluation Loss: 0.00000055\n",
      "Epoch [2290/2500],\n",
      " Training Loss: 0.00000090\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [2291/2500],\n",
      " Training Loss: 0.00000080\n",
      "Evaluation Loss: 0.00000085\n",
      "Epoch [2292/2500],\n",
      " Training Loss: 0.00000093\n",
      "Evaluation Loss: 0.00000119\n",
      "Epoch [2293/2500],\n",
      " Training Loss: 0.00000094\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [2294/2500],\n",
      " Training Loss: 0.00000097\n",
      "Evaluation Loss: 0.00000060\n",
      "Epoch [2295/2500],\n",
      " Training Loss: 0.00000124\n",
      "Evaluation Loss: 0.00000064\n",
      "Epoch [2296/2500],\n",
      " Training Loss: 0.00000179\n",
      "Evaluation Loss: 0.00000191\n",
      "Epoch [2297/2500],\n",
      " Training Loss: 0.00000201\n",
      "Evaluation Loss: 0.00000547\n",
      "Epoch [2298/2500],\n",
      " Training Loss: 0.00000203\n",
      "Evaluation Loss: 0.00000261\n",
      "Epoch [2299/2500],\n",
      " Training Loss: 0.00000553\n",
      "Evaluation Loss: 0.00000970\n",
      "Epoch [2300/2500],\n",
      " Training Loss: 0.00000647\n",
      "Evaluation Loss: 0.00000494\n",
      "Epoch [2301/2500],\n",
      " Training Loss: 0.00000338\n",
      "Evaluation Loss: 0.00000387\n",
      "Epoch [2302/2500],\n",
      " Training Loss: 0.00000333\n",
      "Evaluation Loss: 0.00000229\n",
      "Epoch [2303/2500],\n",
      " Training Loss: 0.00000891\n",
      "Evaluation Loss: 0.00000239\n",
      "Epoch [2304/2500],\n",
      " Training Loss: 0.00000825\n",
      "Evaluation Loss: 0.00000457\n",
      "Epoch [2305/2500],\n",
      " Training Loss: 0.00000962\n",
      "Evaluation Loss: 0.00000723\n",
      "Epoch [2306/2500],\n",
      " Training Loss: 0.00000619\n",
      "Evaluation Loss: 0.00000179\n",
      "Epoch [2307/2500],\n",
      " Training Loss: 0.00000872\n",
      "Evaluation Loss: 0.00000440\n",
      "Epoch [2308/2500],\n",
      " Training Loss: 0.00001056\n",
      "Evaluation Loss: 0.00001095\n",
      "Epoch [2309/2500],\n",
      " Training Loss: 0.00001415\n",
      "Evaluation Loss: 0.00000140\n",
      "Epoch [2310/2500],\n",
      " Training Loss: 0.00000617\n",
      "Evaluation Loss: 0.00001178\n",
      "Epoch [2311/2500],\n",
      " Training Loss: 0.00001218\n",
      "Evaluation Loss: 0.00000712\n",
      "Epoch [2312/2500],\n",
      " Training Loss: 0.00000854\n",
      "Evaluation Loss: 0.00000405\n",
      "Epoch [2313/2500],\n",
      " Training Loss: 0.00000497\n",
      "Evaluation Loss: 0.00000159\n",
      "Epoch [2314/2500],\n",
      " Training Loss: 0.00000252\n",
      "Evaluation Loss: 0.00000266\n",
      "Epoch [2315/2500],\n",
      " Training Loss: 0.00000168\n",
      "Evaluation Loss: 0.00000094\n",
      "Epoch [2316/2500],\n",
      " Training Loss: 0.00000104\n",
      "Evaluation Loss: 0.00000069\n",
      "Epoch [2317/2500],\n",
      " Training Loss: 0.00000098\n",
      "Evaluation Loss: 0.00000095\n",
      "Epoch [2318/2500],\n",
      " Training Loss: 0.00000187\n",
      "Evaluation Loss: 0.00000102\n",
      "Epoch [2319/2500],\n",
      " Training Loss: 0.00000169\n",
      "Evaluation Loss: 0.00000156\n",
      "Epoch [2320/2500],\n",
      " Training Loss: 0.00000146\n",
      "Evaluation Loss: 0.00000146\n",
      "Epoch [2321/2500],\n",
      " Training Loss: 0.00000149\n",
      "Evaluation Loss: 0.00000221\n",
      "Epoch [2322/2500],\n",
      " Training Loss: 0.00000370\n",
      "Evaluation Loss: 0.00000274\n",
      "Epoch [2323/2500],\n",
      " Training Loss: 0.00000420\n",
      "Evaluation Loss: 0.00000845\n",
      "Epoch [2324/2500],\n",
      " Training Loss: 0.00000615\n",
      "Evaluation Loss: 0.00000749\n",
      "Epoch [2325/2500],\n",
      " Training Loss: 0.00000475\n",
      "Evaluation Loss: 0.00000405\n",
      "Epoch [2326/2500],\n",
      " Training Loss: 0.00000244\n",
      "Evaluation Loss: 0.00000187\n",
      "Epoch [2327/2500],\n",
      " Training Loss: 0.00000360\n",
      "Evaluation Loss: 0.00000500\n",
      "Epoch [2328/2500],\n",
      " Training Loss: 0.00000605\n",
      "Evaluation Loss: 0.00000152\n",
      "Epoch [2329/2500],\n",
      " Training Loss: 0.00000946\n",
      "Evaluation Loss: 0.00000613\n",
      "Epoch [2330/2500],\n",
      " Training Loss: 0.00001183\n",
      "Evaluation Loss: 0.00000683\n",
      "Epoch [2331/2500],\n",
      " Training Loss: 0.00000985\n",
      "Evaluation Loss: 0.00001061\n",
      "Epoch [2332/2500],\n",
      " Training Loss: 0.00000717\n",
      "Evaluation Loss: 0.00001088\n",
      "Epoch [2333/2500],\n",
      " Training Loss: 0.00001027\n",
      "Evaluation Loss: 0.00000359\n",
      "Epoch [2334/2500],\n",
      " Training Loss: 0.00000903\n",
      "Evaluation Loss: 0.00000531\n",
      "Epoch [2335/2500],\n",
      " Training Loss: 0.00000841\n",
      "Evaluation Loss: 0.00000277\n",
      "Epoch [2336/2500],\n",
      " Training Loss: 0.00000597\n",
      "Evaluation Loss: 0.00000302\n",
      "Epoch [2337/2500],\n",
      " Training Loss: 0.00000538\n",
      "Evaluation Loss: 0.00000154\n",
      "Epoch [2338/2500],\n",
      " Training Loss: 0.00000388\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [2339/2500],\n",
      " Training Loss: 0.00000463\n",
      "Evaluation Loss: 0.00000325\n",
      "Epoch [2340/2500],\n",
      " Training Loss: 0.00000766\n",
      "Evaluation Loss: 0.00000327\n",
      "Epoch [2341/2500],\n",
      " Training Loss: 0.00000843\n",
      "Evaluation Loss: 0.00000169\n",
      "Epoch [2342/2500],\n",
      " Training Loss: 0.00000875\n",
      "Evaluation Loss: 0.00000282\n",
      "Epoch [2343/2500],\n",
      " Training Loss: 0.00000528\n",
      "Evaluation Loss: 0.00000341\n",
      "Epoch [2344/2500],\n",
      " Training Loss: 0.00000564\n",
      "Evaluation Loss: 0.00000256\n",
      "Epoch [2345/2500],\n",
      " Training Loss: 0.00000357\n",
      "Evaluation Loss: 0.00000265\n",
      "Epoch [2346/2500],\n",
      " Training Loss: 0.00000369\n",
      "Evaluation Loss: 0.00000220\n",
      "Epoch [2347/2500],\n",
      " Training Loss: 0.00000283\n",
      "Evaluation Loss: 0.00000231\n",
      "Epoch [2348/2500],\n",
      " Training Loss: 0.00000161\n",
      "Evaluation Loss: 0.00000112\n",
      "Epoch [2349/2500],\n",
      " Training Loss: 0.00000116\n",
      "Evaluation Loss: 0.00000071\n",
      "Epoch [2350/2500],\n",
      " Training Loss: 0.00000111\n",
      "Evaluation Loss: 0.00000138\n",
      "Epoch [2351/2500],\n",
      " Training Loss: 0.00000141\n",
      "Evaluation Loss: 0.00000255\n",
      "Epoch [2352/2500],\n",
      " Training Loss: 0.00000310\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [2353/2500],\n",
      " Training Loss: 0.00000257\n",
      "Evaluation Loss: 0.00000510\n",
      "Epoch [2354/2500],\n",
      " Training Loss: 0.00000500\n",
      "Evaluation Loss: 0.00000280\n",
      "Epoch [2355/2500],\n",
      " Training Loss: 0.00000265\n",
      "Evaluation Loss: 0.00000193\n",
      "Epoch [2356/2500],\n",
      " Training Loss: 0.00000242\n",
      "Evaluation Loss: 0.00000482\n",
      "Epoch [2357/2500],\n",
      " Training Loss: 0.00000346\n",
      "Evaluation Loss: 0.00000218\n",
      "Epoch [2358/2500],\n",
      " Training Loss: 0.00000212\n",
      "Evaluation Loss: 0.00000114\n",
      "Epoch [2359/2500],\n",
      " Training Loss: 0.00000505\n",
      "Evaluation Loss: 0.00000589\n",
      "Epoch [2360/2500],\n",
      " Training Loss: 0.00002269\n",
      "Evaluation Loss: 0.00001183\n",
      "Epoch [2361/2500],\n",
      " Training Loss: 0.00005244\n",
      "Evaluation Loss: 0.00002942\n",
      "Epoch [2362/2500],\n",
      " Training Loss: 0.00003650\n",
      "Evaluation Loss: 0.00001355\n",
      "Epoch [2363/2500],\n",
      " Training Loss: 0.00002079\n",
      "Evaluation Loss: 0.00001206\n",
      "Epoch [2364/2500],\n",
      " Training Loss: 0.00001072\n",
      "Evaluation Loss: 0.00000503\n",
      "Epoch [2365/2500],\n",
      " Training Loss: 0.00000789\n",
      "Evaluation Loss: 0.00000526\n",
      "Epoch [2366/2500],\n",
      " Training Loss: 0.00000354\n",
      "Evaluation Loss: 0.00000134\n",
      "Epoch [2367/2500],\n",
      " Training Loss: 0.00000420\n",
      "Evaluation Loss: 0.00000913\n",
      "Epoch [2368/2500],\n",
      " Training Loss: 0.00000311\n",
      "Evaluation Loss: 0.00000252\n",
      "Epoch [2369/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000249\n",
      "Epoch [2370/2500],\n",
      " Training Loss: 0.00000232\n",
      "Evaluation Loss: 0.00000370\n",
      "Epoch [2371/2500],\n",
      " Training Loss: 0.00000349\n",
      "Evaluation Loss: 0.00000687\n",
      "Epoch [2372/2500],\n",
      " Training Loss: 0.00000716\n",
      "Evaluation Loss: 0.00000632\n",
      "Epoch [2373/2500],\n",
      " Training Loss: 0.00000889\n",
      "Evaluation Loss: 0.00000520\n",
      "Epoch [2374/2500],\n",
      " Training Loss: 0.00000507\n",
      "Evaluation Loss: 0.00000613\n",
      "Epoch [2375/2500],\n",
      " Training Loss: 0.00000345\n",
      "Evaluation Loss: 0.00000193\n",
      "Epoch [2376/2500],\n",
      " Training Loss: 0.00000209\n",
      "Evaluation Loss: 0.00000105\n",
      "Epoch [2377/2500],\n",
      " Training Loss: 0.00000132\n",
      "Evaluation Loss: 0.00000145\n",
      "Epoch [2378/2500],\n",
      " Training Loss: 0.00000150\n",
      "Evaluation Loss: 0.00000156\n",
      "Epoch [2379/2500],\n",
      " Training Loss: 0.00000210\n",
      "Evaluation Loss: 0.00000209\n",
      "Epoch [2380/2500],\n",
      " Training Loss: 0.00000220\n",
      "Evaluation Loss: 0.00000163\n",
      "Epoch [2381/2500],\n",
      " Training Loss: 0.00000234\n",
      "Evaluation Loss: 0.00000144\n",
      "Epoch [2382/2500],\n",
      " Training Loss: 0.00000401\n",
      "Evaluation Loss: 0.00000220\n",
      "Epoch [2383/2500],\n",
      " Training Loss: 0.00000356\n",
      "Evaluation Loss: 0.00000387\n",
      "Epoch [2384/2500],\n",
      " Training Loss: 0.00000407\n",
      "Evaluation Loss: 0.00000162\n",
      "Epoch [2385/2500],\n",
      " Training Loss: 0.00000505\n",
      "Evaluation Loss: 0.00000762\n",
      "Epoch [2386/2500],\n",
      " Training Loss: 0.00000948\n",
      "Evaluation Loss: 0.00001131\n",
      "Epoch [2387/2500],\n",
      " Training Loss: 0.00000758\n",
      "Evaluation Loss: 0.00000327\n",
      "Epoch [2388/2500],\n",
      " Training Loss: 0.00001339\n",
      "Evaluation Loss: 0.00001289\n",
      "Epoch [2389/2500],\n",
      " Training Loss: 0.00001253\n",
      "Evaluation Loss: 0.00000728\n",
      "Epoch [2390/2500],\n",
      " Training Loss: 0.00002750\n",
      "Evaluation Loss: 0.00001103\n",
      "Epoch [2391/2500],\n",
      " Training Loss: 0.00001219\n",
      "Evaluation Loss: 0.00000365\n",
      "Epoch [2392/2500],\n",
      " Training Loss: 0.00000700\n",
      "Evaluation Loss: 0.00000620\n",
      "Epoch [2393/2500],\n",
      " Training Loss: 0.00000821\n",
      "Evaluation Loss: 0.00000253\n",
      "Epoch [2394/2500],\n",
      " Training Loss: 0.00000301\n",
      "Evaluation Loss: 0.00000084\n",
      "Epoch [2395/2500],\n",
      " Training Loss: 0.00000230\n",
      "Evaluation Loss: 0.00000091\n",
      "Epoch [2396/2500],\n",
      " Training Loss: 0.00000279\n",
      "Evaluation Loss: 0.00000283\n",
      "Epoch [2397/2500],\n",
      " Training Loss: 0.00000508\n",
      "Evaluation Loss: 0.00000535\n",
      "Epoch [2398/2500],\n",
      " Training Loss: 0.00000453\n",
      "Evaluation Loss: 0.00000302\n",
      "Epoch [2399/2500],\n",
      " Training Loss: 0.00000357\n",
      "Evaluation Loss: 0.00000328\n",
      "Epoch [2400/2500],\n",
      " Training Loss: 0.00000174\n",
      "Evaluation Loss: 0.00000135\n",
      "Epoch [2401/2500],\n",
      " Training Loss: 0.00000125\n",
      "Evaluation Loss: 0.00000094\n",
      "Epoch [2402/2500],\n",
      " Training Loss: 0.00000103\n",
      "Evaluation Loss: 0.00000111\n",
      "Epoch [2403/2500],\n",
      " Training Loss: 0.00000107\n",
      "Evaluation Loss: 0.00000081\n",
      "Epoch [2404/2500],\n",
      " Training Loss: 0.00000092\n",
      "Evaluation Loss: 0.00000083\n",
      "Epoch [2405/2500],\n",
      " Training Loss: 0.00000104\n",
      "Evaluation Loss: 0.00000075\n",
      "Epoch [2406/2500],\n",
      " Training Loss: 0.00000113\n",
      "Evaluation Loss: 0.00000123\n",
      "Epoch [2407/2500],\n",
      " Training Loss: 0.00000098\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [2408/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000076\n",
      "Epoch [2409/2500],\n",
      " Training Loss: 0.00000169\n",
      "Evaluation Loss: 0.00000139\n",
      "Epoch [2410/2500],\n",
      " Training Loss: 0.00000179\n",
      "Evaluation Loss: 0.00000078\n",
      "Epoch [2411/2500],\n",
      " Training Loss: 0.00000161\n",
      "Evaluation Loss: 0.00000077\n",
      "Epoch [2412/2500],\n",
      " Training Loss: 0.00000443\n",
      "Evaluation Loss: 0.00000419\n",
      "Epoch [2413/2500],\n",
      " Training Loss: 0.00000337\n",
      "Evaluation Loss: 0.00000240\n",
      "Epoch [2414/2500],\n",
      " Training Loss: 0.00000239\n",
      "Evaluation Loss: 0.00000231\n",
      "Epoch [2415/2500],\n",
      " Training Loss: 0.00000271\n",
      "Evaluation Loss: 0.00000239\n",
      "Epoch [2416/2500],\n",
      " Training Loss: 0.00000600\n",
      "Evaluation Loss: 0.00000161\n",
      "Epoch [2417/2500],\n",
      " Training Loss: 0.00000704\n",
      "Evaluation Loss: 0.00000446\n",
      "Epoch [2418/2500],\n",
      " Training Loss: 0.00001477\n",
      "Evaluation Loss: 0.00001744\n",
      "Epoch [2419/2500],\n",
      " Training Loss: 0.00002648\n",
      "Evaluation Loss: 0.00003901\n",
      "Epoch [2420/2500],\n",
      " Training Loss: 0.00001882\n",
      "Evaluation Loss: 0.00001078\n",
      "Epoch [2421/2500],\n",
      " Training Loss: 0.00002411\n",
      "Evaluation Loss: 0.00001131\n",
      "Epoch [2422/2500],\n",
      " Training Loss: 0.00001857\n",
      "Evaluation Loss: 0.00000739\n",
      "Epoch [2423/2500],\n",
      " Training Loss: 0.00001268\n",
      "Evaluation Loss: 0.00000475\n",
      "Epoch [2424/2500],\n",
      " Training Loss: 0.00001513\n",
      "Evaluation Loss: 0.00002610\n",
      "Epoch [2425/2500],\n",
      " Training Loss: 0.00000662\n",
      "Evaluation Loss: 0.00000347\n",
      "Epoch [2426/2500],\n",
      " Training Loss: 0.00002376\n",
      "Evaluation Loss: 0.00004288\n",
      "Epoch [2427/2500],\n",
      " Training Loss: 0.00002530\n",
      "Evaluation Loss: 0.00000598\n",
      "Epoch [2428/2500],\n",
      " Training Loss: 0.00000376\n",
      "Evaluation Loss: 0.00000154\n",
      "Epoch [2429/2500],\n",
      " Training Loss: 0.00000469\n",
      "Evaluation Loss: 0.00000438\n",
      "Epoch [2430/2500],\n",
      " Training Loss: 0.00000255\n",
      "Evaluation Loss: 0.00000137\n",
      "Epoch [2431/2500],\n",
      " Training Loss: 0.00000210\n",
      "Evaluation Loss: 0.00000165\n",
      "Epoch [2432/2500],\n",
      " Training Loss: 0.00000177\n",
      "Evaluation Loss: 0.00000121\n",
      "Epoch [2433/2500],\n",
      " Training Loss: 0.00000167\n",
      "Evaluation Loss: 0.00000134\n",
      "Epoch [2434/2500],\n",
      " Training Loss: 0.00000129\n",
      "Evaluation Loss: 0.00000175\n",
      "Epoch [2435/2500],\n",
      " Training Loss: 0.00000170\n",
      "Evaluation Loss: 0.00000112\n",
      "Epoch [2436/2500],\n",
      " Training Loss: 0.00000122\n",
      "Evaluation Loss: 0.00000100\n",
      "Epoch [2437/2500],\n",
      " Training Loss: 0.00000086\n",
      "Evaluation Loss: 0.00000046\n",
      "Epoch [2438/2500],\n",
      " Training Loss: 0.00000083\n",
      "Evaluation Loss: 0.00000118\n",
      "Epoch [2439/2500],\n",
      " Training Loss: 0.00000120\n",
      "Evaluation Loss: 0.00000099\n",
      "Epoch [2440/2500],\n",
      " Training Loss: 0.00000137\n",
      "Evaluation Loss: 0.00000153\n",
      "Epoch [2441/2500],\n",
      " Training Loss: 0.00000159\n",
      "Evaluation Loss: 0.00000061\n",
      "Epoch [2442/2500],\n",
      " Training Loss: 0.00000122\n",
      "Evaluation Loss: 0.00000082\n",
      "Epoch [2443/2500],\n",
      " Training Loss: 0.00000097\n",
      "Evaluation Loss: 0.00000074\n",
      "Epoch [2444/2500],\n",
      " Training Loss: 0.00000065\n",
      "Evaluation Loss: 0.00000064\n",
      "Epoch [2445/2500],\n",
      " Training Loss: 0.00000071\n",
      "Evaluation Loss: 0.00000046\n",
      "Epoch [2446/2500],\n",
      " Training Loss: 0.00000070\n",
      "Evaluation Loss: 0.00000056\n",
      "Epoch [2447/2500],\n",
      " Training Loss: 0.00000074\n",
      "Evaluation Loss: 0.00000066\n",
      "Epoch [2448/2500],\n",
      " Training Loss: 0.00000089\n",
      "Evaluation Loss: 0.00000057\n",
      "Epoch [2449/2500],\n",
      " Training Loss: 0.00000091\n",
      "Evaluation Loss: 0.00000066\n",
      "Epoch [2450/2500],\n",
      " Training Loss: 0.00000076\n",
      "Evaluation Loss: 0.00000070\n",
      "Epoch [2451/2500],\n",
      " Training Loss: 0.00000094\n",
      "Evaluation Loss: 0.00000062\n",
      "Epoch [2452/2500],\n",
      " Training Loss: 0.00000083\n",
      "Evaluation Loss: 0.00000046\n",
      "Epoch [2453/2500],\n",
      " Training Loss: 0.00000072\n",
      "Evaluation Loss: 0.00000052\n",
      "Epoch [2454/2500],\n",
      " Training Loss: 0.00000077\n",
      "Evaluation Loss: 0.00000101\n",
      "Epoch [2455/2500],\n",
      " Training Loss: 0.00000076\n",
      "Evaluation Loss: 0.00000124\n",
      "Epoch [2456/2500],\n",
      " Training Loss: 0.00000087\n",
      "Evaluation Loss: 0.00000083\n",
      "Epoch [2457/2500],\n",
      " Training Loss: 0.00000087\n",
      "Evaluation Loss: 0.00000165\n",
      "Epoch [2458/2500],\n",
      " Training Loss: 0.00000208\n",
      "Evaluation Loss: 0.00000479\n",
      "Epoch [2459/2500],\n",
      " Training Loss: 0.00000652\n",
      "Evaluation Loss: 0.00000924\n",
      "Epoch [2460/2500],\n",
      " Training Loss: 0.00000729\n",
      "Evaluation Loss: 0.00000438\n",
      "Epoch [2461/2500],\n",
      " Training Loss: 0.00000360\n",
      "Evaluation Loss: 0.00000768\n",
      "Epoch [2462/2500],\n",
      " Training Loss: 0.00000763\n",
      "Evaluation Loss: 0.00000172\n",
      "Epoch [2463/2500],\n",
      " Training Loss: 0.00000779\n",
      "Evaluation Loss: 0.00000256\n",
      "Epoch [2464/2500],\n",
      " Training Loss: 0.00000835\n",
      "Evaluation Loss: 0.00000288\n",
      "Epoch [2465/2500],\n",
      " Training Loss: 0.00001825\n",
      "Evaluation Loss: 0.00000545\n",
      "Epoch [2466/2500],\n",
      " Training Loss: 0.00000585\n",
      "Evaluation Loss: 0.00000399\n",
      "Epoch [2467/2500],\n",
      " Training Loss: 0.00000546\n",
      "Evaluation Loss: 0.00000336\n",
      "Epoch [2468/2500],\n",
      " Training Loss: 0.00000723\n",
      "Evaluation Loss: 0.00000388\n",
      "Epoch [2469/2500],\n",
      " Training Loss: 0.00001351\n",
      "Evaluation Loss: 0.00000454\n",
      "Epoch [2470/2500],\n",
      " Training Loss: 0.00001501\n",
      "Evaluation Loss: 0.00001417\n",
      "Epoch [2471/2500],\n",
      " Training Loss: 0.00001233\n",
      "Evaluation Loss: 0.00000511\n",
      "Epoch [2472/2500],\n",
      " Training Loss: 0.00000687\n",
      "Evaluation Loss: 0.00001507\n",
      "Epoch [2473/2500],\n",
      " Training Loss: 0.00000452\n",
      "Evaluation Loss: 0.00000270\n",
      "Epoch [2474/2500],\n",
      " Training Loss: 0.00000687\n",
      "Evaluation Loss: 0.00000464\n",
      "Epoch [2475/2500],\n",
      " Training Loss: 0.00000643\n",
      "Evaluation Loss: 0.00000711\n",
      "Epoch [2476/2500],\n",
      " Training Loss: 0.00000519\n",
      "Evaluation Loss: 0.00000714\n",
      "Epoch [2477/2500],\n",
      " Training Loss: 0.00000530\n",
      "Evaluation Loss: 0.00000245\n",
      "Epoch [2478/2500],\n",
      " Training Loss: 0.00000177\n",
      "Evaluation Loss: 0.00000187\n",
      "Epoch [2479/2500],\n",
      " Training Loss: 0.00000177\n",
      "Evaluation Loss: 0.00000060\n",
      "Epoch [2480/2500],\n",
      " Training Loss: 0.00000206\n",
      "Evaluation Loss: 0.00000398\n",
      "Epoch [2481/2500],\n",
      " Training Loss: 0.00000507\n",
      "Evaluation Loss: 0.00000350\n",
      "Epoch [2482/2500],\n",
      " Training Loss: 0.00000230\n",
      "Evaluation Loss: 0.00000286\n",
      "Epoch [2483/2500],\n",
      " Training Loss: 0.00000509\n",
      "Evaluation Loss: 0.00000224\n",
      "Epoch [2484/2500],\n",
      " Training Loss: 0.00000221\n",
      "Evaluation Loss: 0.00000208\n",
      "Epoch [2485/2500],\n",
      " Training Loss: 0.00000342\n",
      "Evaluation Loss: 0.00000451\n",
      "Epoch [2486/2500],\n",
      " Training Loss: 0.00000918\n",
      "Evaluation Loss: 0.00001027\n",
      "Epoch [2487/2500],\n",
      " Training Loss: 0.00001302\n",
      "Evaluation Loss: 0.00000534\n",
      "Epoch [2488/2500],\n",
      " Training Loss: 0.00002241\n",
      "Evaluation Loss: 0.00000866\n",
      "Epoch [2489/2500],\n",
      " Training Loss: 0.00000928\n",
      "Evaluation Loss: 0.00000292\n",
      "Epoch [2490/2500],\n",
      " Training Loss: 0.00001239\n",
      "Evaluation Loss: 0.00000221\n",
      "Epoch [2491/2500],\n",
      " Training Loss: 0.00000756\n",
      "Evaluation Loss: 0.00000259\n",
      "Epoch [2492/2500],\n",
      " Training Loss: 0.00001596\n",
      "Evaluation Loss: 0.00002787\n",
      "Epoch [2493/2500],\n",
      " Training Loss: 0.00001988\n",
      "Evaluation Loss: 0.00000525\n",
      "Epoch [2494/2500],\n",
      " Training Loss: 0.00001962\n",
      "Evaluation Loss: 0.00002038\n",
      "Epoch [2495/2500],\n",
      " Training Loss: 0.00002179\n",
      "Evaluation Loss: 0.00000411\n",
      "Epoch [2496/2500],\n",
      " Training Loss: 0.00000744\n",
      "Evaluation Loss: 0.00000145\n",
      "Epoch [2497/2500],\n",
      " Training Loss: 0.00000232\n",
      "Evaluation Loss: 0.00000167\n",
      "Epoch [2498/2500],\n",
      " Training Loss: 0.00000197\n",
      "Evaluation Loss: 0.00000158\n",
      "Epoch [2499/2500],\n",
      " Training Loss: 0.00000122\n",
      "Evaluation Loss: 0.00000116\n",
      "Epoch [2500/2500],\n",
      " Training Loss: 0.00000145\n",
      "Evaluation Loss: 0.00000285\n",
      "[INFO] Finished training the network...\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "\n",
    "    print(\"[INFO] training the network...\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for graph_batch in train_dataloader:\n",
    "                \n",
    "            node_inputs = (graph_batch.x).to(device)\n",
    "            edge_index = graph_batch.edge_index\n",
    "            edge_attr = graph_batch.edge_attr\n",
    "            global_inputs = graph_batch.global_input\n",
    "            targets = (graph_batch.y).to(device)\n",
    "\n",
    "            node_inputs = node_inputs.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_attr = edge_attr.to(device)\n",
    "            global_inputs = global_inputs.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_composite.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(node_inputs, targets, edge_index, edge_attr, global_inputs, batch_size=BATCH_SIZE, num_nodes=10)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer_composite.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss/len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for graph_batch in val_dataloader:\n",
    "                        \n",
    "                node_inputs = (graph_batch.x).to(device)\n",
    "                edge_index = graph_batch.edge_index\n",
    "                edge_attr = graph_batch.edge_attr\n",
    "                global_inputs = graph_batch.global_input\n",
    "                targets = (graph_batch.y).to(device)\n",
    "\n",
    "                node_inputs = node_inputs.to(device)\n",
    "                edge_index = edge_index.to(device)\n",
    "                edge_attr = edge_attr.to(device)\n",
    "                global_inputs = global_inputs.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(node_inputs, targets, edge_index, edge_attr, global_inputs, batch_size=BATCH_SIZE, num_nodes=10)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "            avg_eval_loss = eval_loss / len(val_dataloader)\n",
    "            eval_losses.append(avg_eval_loss)\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}],\\n Training Loss: {running_loss/len(train_dataloader):.8f}')\n",
    "        print(f'Evaluation Loss: {eval_loss/len(val_dataloader):.8f}')\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"[INFO] Finished training the network...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x1800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=300)  # High-resolution plot\n",
    "\n",
    "plt.plot((train_losses), color='black', linewidth=1.5, label=\"Training loss\")\n",
    "plt.plot((eval_losses), color='red', linewidth=1.5, label=\"Validation loss\")\n",
    "plt.title(\"Training Loss and Validation Loss\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=10)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.ylim(-0.0001, 0.01)\n",
    "# plt.savefig(\"aircraft_model_loss.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TRAINED_MODEL:\n",
    "\n",
    "    import datetime\n",
    "    date = datetime.date.today()\n",
    "\n",
    "    # Save Python model\n",
    "\n",
    "    save_path =  project_path + '/trained_models/models/vehicle/{}_aircraft_mseLoss_arch2_lr{}_e{}_hidDim{}.pth'.format(date, LEARNING_RATE, EPOCHS, hidden_dim)\n",
    "    print(\"The model will be saved as the following:\\n {}\".format(save_path))\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # Save the input and output scalers\n",
    "\n",
    "    wing_input_scaler_path =  project_path + '/trained_models/scalers/vehicle/{}_wing_input_scaler_mseLoss_arch2_lr{}_e{}_hidDim{}.joblib'.format(date, LEARNING_RATE, EPOCHS, hidden_dim)\n",
    "    wing_output_scaler_path = project_path + '/trained_models/scalers/vehicle/{}_wing_output_scaler_mseLoss_arch2_lr{}_e{}_hidDim{}.joblib'.format(date, LEARNING_RATE, EPOCHS, hidden_dim)\n",
    "\n",
    "    canard_input_scaler_path =  project_path + '/trained_models/scalers/vehicle/{}_canard_input_scaler_mseLoss_arch2_lr{}_e{}_hidDim{}.joblib'.format(date, LEARNING_RATE, EPOCHS, hidden_dim)\n",
    "    canard_output_scaler_path = project_path + '/trained_models/scalers/vehicle/{}_canard_output_scaler_mseLoss_arch2_lr{}_e{}_hidDim{}.joblib'.format(date, LEARNING_RATE, EPOCHS, hidden_dim)\n",
    "\n",
    "    joblib.dump(input_scaler_wing_temp, wing_input_scaler_path)\n",
    "    joblib.dump(output_scaler_wing_temp, wing_output_scaler_path)\n",
    "\n",
    "    joblib.dump(input_scaler_canard_temp, canard_input_scaler_path)\n",
    "    joblib.dump(output_scaler_canard_temp, canard_output_scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_model_vs_ground_truth(time, model_outputs, ground_truths, aoa, v_inf, simulation_case, save_csv=False, csv_filename=\"NN_predictions.csv\", save_path=\"./plots/\"):\n",
    "    \"\"\"\n",
    "    Plot model predictions vs ground truth for aerodynamic coefficients with professional formatting and save the plots.\n",
    "    Optionally saves the NN predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        time (array-like): Time steps.\n",
    "        model_outputs (dict): Dictionary containing model predictions.\n",
    "        ground_truths (dict): Dictionary containing ground truth values.\n",
    "        aoa (float): Angle of attack in degrees.\n",
    "        v_inf (float): Free-stream velocity in m/s.\n",
    "        simulation_case (str): Unique identifier for the simulation case (used in filenames).\n",
    "        save_csv (bool): If True, saves predictions to a CSV file.\n",
    "        csv_filename (str): Name of the CSV file to save predictions.\n",
    "        save_path (str): Directory where plots should be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Define categories with specific labels\n",
    "    categories = {\n",
    "        f\"wing_{simulation_case}\": [(\"Cl_Wing\", \"Lift Coefficient, $C_L$\"), (\"Cd_Wing\", \"Drag Coefficient, $C_D$\")],\n",
    "        f\"canard_{simulation_case}\": [(\"Cl_Canard\", \"Lift Coefficient, $C_L$\"), (\"Cd_Canard\", \"Drag Coefficient, $C_D$\")],\n",
    "        f\"rotorsL_{simulation_case}\": [\n",
    "            (\"Ct_L1\", \"Thrust Coefficient, $C_T$\"), (\"Cq_L1\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_L2\", \"Thrust Coefficient, $C_T$\"), (\"Cq_L2\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_L3\", \"Thrust Coefficient, $C_T$\"), (\"Cq_L3\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_L4\", \"Thrust Coefficient, $C_T$\"), (\"Cq_L4\", \"Torque Coefficient, $C_Q$\")\n",
    "        ],\n",
    "        f\"rotorsR_{simulation_case}\": [\n",
    "            (\"Ct_R1\", \"Thrust Coefficient, $C_T$\"), (\"Cq_R1\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_R2\", \"Thrust Coefficient, $C_T$\"), (\"Cq_R2\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_R3\", \"Thrust Coefficient, $C_T$\"), (\"Cq_R3\", \"Torque Coefficient, $C_Q$\"),\n",
    "            (\"Ct_R4\", \"Thrust Coefficient, $C_T$\"), (\"Cq_R4\", \"Torque Coefficient, $C_Q$\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store CSV data\n",
    "    csv_data = {\"Time\": time}\n",
    "\n",
    "    # Iterate through categories and create separate figures with professional formatting\n",
    "    for filename, labels in categories.items():\n",
    "        num_plots = len(labels)\n",
    "        rows, cols = (num_plots // 2, 2) if num_plots > 2 else (1, num_plots)\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "        axes = axes.flatten() if num_plots > 1 else [axes]  # Ensure iterable axes\n",
    "\n",
    "        for i, (key, ylabel) in enumerate(labels):\n",
    "            ax = axes[i]\n",
    "            ax.plot(time, model_outputs[key], label=f'Predicted {ylabel}', color='black', linestyle='--', linewidth=2)\n",
    "            ax.plot(time, ground_truths[key], label=f'Actual {ylabel}', color='red', linestyle='-', linewidth=2)\n",
    "            ax.set_xlabel('Time [s]', fontsize=12)\n",
    "            ax.set_ylabel(ylabel, fontsize=12)\n",
    "            ax.set_title(f'{ylabel} Comparison', fontsize=14)\n",
    "            ax.legend(\n",
    "                loc='upper center', fontsize=10,\n",
    "                title=f'$AoA$ = ${aoa}^{{\\circ}}$, $V_{{\\infty}}$ = ${v_inf}$ m/s',\n",
    "                fancybox=True, borderpad=1, title_fontsize='12', ncol=2\n",
    "            )\n",
    "            ax.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "            # Set y-limits dynamically based on the data range\n",
    "            y_min = min(np.min(model_outputs[key]), np.min(ground_truths[key])) - 0.05 * np.abs(np.min(model_outputs[key]))\n",
    "            y_max = max(np.max(model_outputs[key]), np.max(ground_truths[key])) + 0.05 * np.abs(np.max(model_outputs[key]))\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "\n",
    "            # Store data for CSV\n",
    "            csv_data[f\"{key}_NN\"] = model_outputs[key]\n",
    "            csv_data[f\"{key}_Truth\"] = ground_truths[key]\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_path, f\"{filename}.pdf\")\n",
    "        plt.savefig(plot_filename, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Plot saved: {plot_filename}\")\n",
    "\n",
    "    # Save predictions to CSV if enabled\n",
    "    if save_csv:\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        csv_filepath = os.path.join(save_path, csv_filename)\n",
    "        df.to_csv(csv_filepath, index=False)\n",
    "        print(f\"Neural Network predictions saved to '{csv_filepath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Absolute Percentage Error (MAPE)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def relative_l2_norm(y_true, y_pred):\n",
    "    \"\"\"Compute Relative L2 Norm Error (ε)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    numerator = np.linalg.norm(y_pred[mask] - y_true[mask], ord=2)  # ||pred - true||_2\n",
    "    denominator = np.linalg.norm(y_true[mask], ord=2)  # ||true||_2\n",
    "    return (numerator / denominator) * 100\n",
    "\n",
    "epsilon_wing_cl_list, epsilon_wing_cd_list = [], []\n",
    "epsilon_canard_cl_list, epsilon_canard_cd_list = [], []\n",
    "epsilon_r1_ct_list, epsilon_r1_cq_list = [], []\n",
    "epsilon_r2_ct_list, epsilon_r2_cq_list = [], []\n",
    "epsilon_r3_ct_list, epsilon_r3_cq_list = [], []\n",
    "epsilon_r4_ct_list, epsilon_r4_cq_list = [], []\n",
    "epsilon_l1_ct_list, epsilon_l1_cq_list = [], []\n",
    "epsilon_l2_ct_list, epsilon_l2_cq_list = [], []\n",
    "epsilon_l3_ct_list, epsilon_l3_cq_list = [], []\n",
    "epsilon_l4_ct_list, epsilon_l4_cq_list = [], []\n",
    "\n",
    "epsilon_wing_list = []\n",
    "epsilon_canard_list = []\n",
    "epsilon_r1_list = []\n",
    "epsilon_r2_list = []\n",
    "epsilon_r3_list = []\n",
    "epsilon_r4_list = []\n",
    "epsilon_l1_list = []\n",
    "epsilon_l2_list = []\n",
    "epsilon_l3_list = []\n",
    "epsilon_l4_list = []\n",
    "\n",
    "r2_wing_cl_list, r2_wing_cd_list = [], []\n",
    "r2_canard_cl_list, r2_canard_cd_list = [], []\n",
    "r2_r1_ct_list, r2_r1_cq_list = [], []\n",
    "r2_r2_ct_list, r2_r2_cq_list = [], []\n",
    "r2_r3_ct_list, r2_r3_cq_list = [], []\n",
    "r2_r4_ct_list, r2_r4_cq_list = [], []\n",
    "r2_l1_ct_list, r2_l1_cq_list = [], []\n",
    "r2_l2_ct_list, r2_l2_cq_list = [], []\n",
    "r2_l3_ct_list, r2_l3_cq_list = [], []\n",
    "r2_l4_ct_list, r2_l4_cq_list = [], []\n",
    "\n",
    "r2_wing_list = []\n",
    "r2_canard_list = []\n",
    "r2_r1_list = []\n",
    "r2_r2_list = []\n",
    "r2_r3_list = []\n",
    "r2_r4_list = []\n",
    "r2_l1_list = []\n",
    "r2_l2_list = []\n",
    "r2_l3_list = []\n",
    "r2_l4_list = []\n",
    "\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "mape_wing_cl_list, mape_wing_cd_list = [], []\n",
    "mape_canard_cl_list, mape_canard_cd_list = [], []\n",
    "mape_r1_ct_list, mape_r1_cq_list = [], []\n",
    "mape_r2_ct_list, mape_r2_cq_list = [], []\n",
    "mape_r3_ct_list, mape_r3_cq_list = [], []\n",
    "mape_r4_ct_list, mape_r4_cq_list = [], []\n",
    "mape_l1_ct_list, mape_l1_cq_list = [], []\n",
    "mape_l2_ct_list, mape_l2_cq_list = [], []\n",
    "mape_l3_ct_list, mape_l3_cq_list = [], []\n",
    "mape_l4_ct_list, mape_l4_cq_list = [], []\n",
    "\n",
    "mape_wing_list = []\n",
    "mape_canard_list = []\n",
    "mape_r1_list = []\n",
    "mape_r2_list = []\n",
    "mape_r3_list = []\n",
    "mape_r4_list = []\n",
    "mape_l1_list = []\n",
    "mape_l2_list = []\n",
    "mape_l3_list = []\n",
    "mape_l4_list = []\n",
    "\n",
    "simulation_case_list = []\n",
    "\n",
    "\n",
    "root_test_base = '/mnt/e/eVTOL_model/eVTOL-VehicleModel/FLOWUnsteady_simulations/aircraft_data/testing_data/'\n",
    "batch_size_test = 1\n",
    "\n",
    "for simulation_case in os.listdir(root_test_base):\n",
    "\n",
    "    root_test_dir = root_test_base+simulation_case\n",
    "\n",
    "    # Canard dataset\n",
    "    dataset_canard_test = WingDataset(root_test_dir, \n",
    "                                    af_model_ESCNN_Cl=af_model_ESCNN_Cl, \n",
    "                                    af_model_ESCNN_Cd=af_model_ESCNN_Cd,\n",
    "                                    airfoil_cl=airfoil_cl, \n",
    "                                    airfoil_cd=airfoil_cd, \n",
    "                                    device=device,\n",
    "                                    wing_name = 'Canard',     # select 'wing_main' or 'Canard'  \n",
    "                                    subdir_condition=subdir_condition_wing)\n",
    "\n",
    "    inputs_canard_test, outputs_canard_test = dataset_canard_test[0:]\n",
    "\n",
    "    input_tensor_canard_test = inputs_canard_test\n",
    "    input_tensor_canard_test = inputs_canard_test.squeeze(1)  # Reshaping\n",
    "    # print(\"Input shape (Canard dataset):\", input_tensor_canard.shape)\n",
    "    output_tensor_canard_test = outputs_canard_test.squeeze(1)\n",
    "    # print(\"Output shape (Canard dataset):\",output_tensor_canard.shape)\n",
    "\n",
    "    dataset_wing_test = WingDataset(root_test_dir, \n",
    "                                    af_model_ESCNN_Cl=af_model_ESCNN_Cl, \n",
    "                                    af_model_ESCNN_Cd=af_model_ESCNN_Cd,\n",
    "                                    airfoil_cl=airfoil_cl, \n",
    "                                    airfoil_cd=airfoil_cd, \n",
    "                                    device=device,\n",
    "                                    wing_name = 'wing_main',     # select 'wing_main' or 'Canard'  \n",
    "                                    subdir_condition=subdir_condition_wing)\n",
    "    \n",
    "    inputs_wing_test, outputs_wing_test = dataset_wing_test[0:]\n",
    "\n",
    "    input_tensor_wing_test = inputs_wing_test\n",
    "    input_tensor_wing_test = inputs_wing_test.squeeze(1)  # Reshaping\n",
    "    # print(\"Input shape (Wing dataset):\", input_tensor_wing.shape)\n",
    "    output_tensor_wing_test = outputs_wing_test.squeeze(1)\n",
    "    # print(\"Output shape (Wing dataset):\",output_tensor_wing.shape)\n",
    "\n",
    "\n",
    "    # dataset - Rotor L1\n",
    "    dataset_rotor_L1_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'L1',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rL1_test, outputs_rL1_test = dataset_rotor_L1_test[0:]\n",
    "\n",
    "    input_tensor_rL1_test = inputs_rL1_test.squeeze(1)  # Reshaping\n",
    "    # print(\"Input shape (rotor - L1):\", input_tensor_rL1.shape) \n",
    "    output_tensor_rL1_test = outputs_rL1_test.squeeze(1)\n",
    "    # print(\"Output shape (rotor - L1):\",output_tensor_rL1.shape)\n",
    "\n",
    "    dataset_rotor_L2_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'L2',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rL2_test, outputs_rL2_test = dataset_rotor_L2_test[0:]\n",
    "\n",
    "    input_tensor_rL2_test = inputs_rL2_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rL2_test = outputs_rL2_test.squeeze(1)\n",
    "\n",
    "    dataset_rotor_L3_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'L3',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rL3_test, outputs_rL3_test = dataset_rotor_L3_test[0:]\n",
    "\n",
    "    input_tensor_rL3_test = inputs_rL3_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rL3_test = outputs_rL3_test.squeeze(1)\n",
    "\n",
    "    dataset_rotor_L4_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'L4',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rL4_test, outputs_rL4_test = dataset_rotor_L4_test[0:]\n",
    "\n",
    "    input_tensor_rL4_test = inputs_rL4_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rL4_test = outputs_rL4_test.squeeze(1)\n",
    "\n",
    "    # dataset - Rotor R1\n",
    "    dataset_rotor_R1_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'R1',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rR1_test, outputs_rR1_test = dataset_rotor_R1_test[0:]\n",
    "\n",
    "    input_tensor_rR1_test = inputs_rR1_test.squeeze(1)  # Reshaping\n",
    "    # print(\"Input shape (rotor - R1):\", input_tensor_rR1.shape) \n",
    "    output_tensor_rR1_test = outputs_rR1_test.squeeze(1)\n",
    "    # print(\"Output shape (rotor - R1):\",output_tensor_rR1.shape) \n",
    "\n",
    "    dataset_rotor_R2_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'R2',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rR2_test, outputs_rR2_test = dataset_rotor_R2_test[0:]\n",
    "\n",
    "    input_tensor_rR2_test = inputs_rR2_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rR2_test = outputs_rR2_test.squeeze(1)\n",
    "\n",
    "    dataset_rotor_R3_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'R3',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rR3_test, outputs_rR3_test = dataset_rotor_R3_test[0:]\n",
    "\n",
    "    input_tensor_rR3_test = inputs_rR3_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rR3_test = outputs_rR3_test.squeeze(1)\n",
    "\n",
    "    dataset_rotor_R4_test = PropellerDataset(root_test_dir,\n",
    "                            rotor_notation = 'R4',                       # Select: L1, L2, L3, L4, R1, R2, R3, R4\n",
    "                            subdir_condition=subdir_condition_rotor)\n",
    "    inputs_rR4_test, outputs_rR4_test = dataset_rotor_R4_test[0:]\n",
    "    \n",
    "    input_tensor_rR4_test = inputs_rR4_test.squeeze(1)  # Reshaping\n",
    "    output_tensor_rR4_test = outputs_rR4_test.squeeze(1)\n",
    "\n",
    "    dataset_aircraft_test = AircraftDataset(dataset_wing_test,\n",
    "                                            dataset_canard_test,\n",
    "                                            dataset_rotor_L1_test,\n",
    "                                            dataset_rotor_L2_test,\n",
    "                                            dataset_rotor_L3_test,\n",
    "                                            dataset_rotor_L4_test,\n",
    "                                            dataset_rotor_R1_test,\n",
    "                                            dataset_rotor_R2_test,\n",
    "                                            dataset_rotor_R3_test,\n",
    "                                            dataset_rotor_R4_test,\n",
    "                                            input_scaler_wing_temp,\n",
    "                                            output_scaler_wing_temp,\n",
    "                                            input_scaler_canard_temp,\n",
    "                                            output_scaler_canard_temp,\n",
    "                                            input_scaler_rotor1, output_scaler_rotor1,\n",
    "                                            input_scaler_rotor2, output_scaler_rotor2,\n",
    "                                            input_scaler_rotor3, output_scaler_rotor3,\n",
    "                                            input_scaler_rotor4, output_scaler_rotor4)\n",
    "\n",
    "\n",
    "    graph_dataset_test = GraphDataset(dataset_aircraft_test)\n",
    "\n",
    "    graph_dataset_test = graph_dataset_test[0]\n",
    "\n",
    "    node_inputs_test = (graph_dataset_test.x).to(device)\n",
    "    edge_index_test = (graph_dataset_test.edge_index).to(device)\n",
    "    edge_attr_test = (graph_dataset_test.edge_attr).to(device)\n",
    "    global_inputs_test = (graph_dataset_test.global_input).to(device)\n",
    "    targets_test = (graph_dataset_test.y).to(device)\n",
    "    # targets_test = (graph_dataset_test.y).reshape(10, -1, 2).to(device)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # outputs_predicted = scripted_model(node_inputs_test, targets_test, edge_index_test,\n",
    "        #                                      edge_attr_test, global_inputs_test,\n",
    "        #                                      batch_size_test, num_nodes=10)\n",
    "        \n",
    "        outputs_predicted = model(node_inputs_test, targets_test, edge_index_test, edge_attr_test, global_inputs_test, batch_size_test, num_nodes=10)\n",
    "\n",
    "    \n",
    "    \n",
    "    outputs_predicted = outputs_predicted.cpu().detach().numpy()  # Convert tensor to numpy array\n",
    "    # print(\"Predicted Outputs Shape:\", outputs_predicted.shape)\n",
    "    outputs_predicted = outputs_predicted.squeeze(0)\n",
    "\n",
    "    predicted_outputs_canard = outputs_predicted[:, 16:18]\n",
    "    predicted_outputs_wing = outputs_predicted[:, 18:20]\n",
    "    predicted_outputs_rotor_L1 = outputs_predicted[:, 0:2]\n",
    "    predicted_outputs_rotor_R1 = outputs_predicted[:, 8:10]\n",
    "    predicted_outputs_rotor_L2 = outputs_predicted[:, 2:4]\n",
    "    predicted_outputs_rotor_R2 = outputs_predicted[:, 10:12]\n",
    "    predicted_outputs_rotor_L3 = outputs_predicted[:, 4:6]\n",
    "    predicted_outputs_rotor_R3 = outputs_predicted[:, 12:14]\n",
    "    predicted_outputs_rotor_L4 = outputs_predicted[:, 6:8]\n",
    "    predicted_outputs_rotor_R4 = outputs_predicted[:, 14:16]\n",
    "    \n",
    "    # Inverse scaling\n",
    "    predicted_outputs_canard_og_scl = output_scaler_canard_temp.inverse_transform(predicted_outputs_canard)\n",
    "    predicted_outputs_wing_og_scl = output_scaler_wing_temp.inverse_transform(predicted_outputs_wing)\n",
    "    predicted_outputs_rotor_L1_og_scl = output_scaler_rotor1.inverse_transform(predicted_outputs_rotor_L1)\n",
    "    predicted_outputs_rotor_R1_og_scl = output_scaler_rotor1.inverse_transform(predicted_outputs_rotor_R1)\n",
    "    predicted_outputs_rotor_L2_og_scl = output_scaler_rotor2.inverse_transform(predicted_outputs_rotor_L2)\n",
    "    predicted_outputs_rotor_R2_og_scl = output_scaler_rotor2.inverse_transform(predicted_outputs_rotor_R2)\n",
    "    predicted_outputs_rotor_L3_og_scl = output_scaler_rotor3.inverse_transform(predicted_outputs_rotor_L3)\n",
    "    predicted_outputs_rotor_R3_og_scl = output_scaler_rotor3.inverse_transform(predicted_outputs_rotor_R3)\n",
    "    predicted_outputs_rotor_L4_og_scl = output_scaler_rotor4.inverse_transform(predicted_outputs_rotor_L4)\n",
    "    predicted_outputs_rotor_R4_og_scl = output_scaler_rotor4.inverse_transform(predicted_outputs_rotor_R4)\n",
    "\n",
    "    # time_steps = time_steps.cpu().detach().numpy()\n",
    "    time_steps = (global_inputs_test.squeeze(0))[:,0]\n",
    "    time_steps = time_steps.cpu().detach().numpy()\n",
    "\n",
    "    # Predictions from the Neural Network\n",
    "    cl_test_NN_wing = predicted_outputs_wing_og_scl[:, 0]\n",
    "    cd_test_NN_wing = predicted_outputs_wing_og_scl[:, 1]\n",
    "\n",
    "    cl_test_NN_canard = predicted_outputs_canard_og_scl[:, 0]\n",
    "    cd_test_NN_canard = predicted_outputs_canard_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_L1 = predicted_outputs_rotor_L1_og_scl[:, 0]\n",
    "    cq_test_NN_L1 = predicted_outputs_rotor_L1_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_R1 = predicted_outputs_rotor_R1_og_scl[:, 0]\n",
    "    cq_test_NN_R1 = predicted_outputs_rotor_R1_og_scl[:, 1]\n",
    "    \n",
    "    ct_test_NN_L2 = predicted_outputs_rotor_L2_og_scl[:, 0]\n",
    "    cq_test_NN_L2 = predicted_outputs_rotor_L2_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_R2 = predicted_outputs_rotor_R2_og_scl[:, 0]\n",
    "    cq_test_NN_R2 = predicted_outputs_rotor_R2_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_L3 = predicted_outputs_rotor_L3_og_scl[:, 0]\n",
    "    cq_test_NN_L3 = predicted_outputs_rotor_L3_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_R3 = predicted_outputs_rotor_R3_og_scl[:, 0]\n",
    "    cq_test_NN_R3 = predicted_outputs_rotor_R3_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_L4 = predicted_outputs_rotor_L4_og_scl[:, 0]\n",
    "    cq_test_NN_L4 = predicted_outputs_rotor_L4_og_scl[:, 1]\n",
    "\n",
    "    ct_test_NN_R4 = predicted_outputs_rotor_R4_og_scl[:, 0]\n",
    "    cq_test_NN_R4 = predicted_outputs_rotor_R4_og_scl[:, 1]\n",
    "\n",
    "    \n",
    "    # FLOWUnsteady dataset\n",
    "    cl_test_flowuns_canard = dataset_canard_test.get_variable('CL')\n",
    "    cd_test_flowuns_canard = dataset_canard_test.get_variable('CD')\n",
    "    cl_test_flowuns_wing = dataset_wing_test.get_variable('CL')\n",
    "    cd_test_flowuns_wing = dataset_wing_test.get_variable('CD')\n",
    "    ct_test_flowuns_L1 = dataset_rotor_L1_test.get_variable('CT')\n",
    "    cq_test_flowuns_L1 = dataset_rotor_L1_test.get_variable('CQ')\n",
    "    ct_test_flowuns_L2 = dataset_rotor_L2_test.get_variable('CT')\n",
    "    cq_test_flowuns_L2 = dataset_rotor_L2_test.get_variable('CQ')\n",
    "    ct_test_flowuns_L3 = dataset_rotor_L3_test.get_variable('CT')\n",
    "    cq_test_flowuns_L3 = dataset_rotor_L3_test.get_variable('CQ')\n",
    "    ct_test_flowuns_L4 = dataset_rotor_L4_test.get_variable('CT')\n",
    "    cq_test_flowuns_L4 = dataset_rotor_L4_test.get_variable('CQ')\n",
    "    ct_test_flowuns_R1 = dataset_rotor_R1_test.get_variable('CT')\n",
    "    cq_test_flowuns_R1 = dataset_rotor_R1_test.get_variable('CQ')\n",
    "    ct_test_flowuns_R2 = dataset_rotor_R2_test.get_variable('CT')\n",
    "    cq_test_flowuns_R2 = dataset_rotor_R2_test.get_variable('CQ')\n",
    "    ct_test_flowuns_R3 = dataset_rotor_R3_test.get_variable('CT')\n",
    "    cq_test_flowuns_R3 = dataset_rotor_R3_test.get_variable('CQ')\n",
    "    ct_test_flowuns_R4 = dataset_rotor_R4_test.get_variable('CT')\n",
    "    cq_test_flowuns_R4 = dataset_rotor_R4_test.get_variable('CQ')\n",
    "\n",
    "    ct_test_flowuns_L1 = align_timesteps(ct_test_flowuns_L1[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_L1 = align_timesteps(cq_test_flowuns_L1[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_L2 = align_timesteps(ct_test_flowuns_L2[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_L2 = align_timesteps(cq_test_flowuns_L2[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_L3 = align_timesteps(ct_test_flowuns_L3[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_L3 = align_timesteps(cq_test_flowuns_L3[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_L4 = align_timesteps(ct_test_flowuns_L4[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_L4 = align_timesteps(cq_test_flowuns_L4[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_R1 = align_timesteps(ct_test_flowuns_R1[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_R1 = align_timesteps(cq_test_flowuns_R1[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_R2 = align_timesteps(ct_test_flowuns_R2[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_R2 = align_timesteps(cq_test_flowuns_R2[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_R3 = align_timesteps(ct_test_flowuns_R3[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_R3 = align_timesteps(cq_test_flowuns_R3[0][np.newaxis, :], len(time_steps))\n",
    "    ct_test_flowuns_R4 = align_timesteps(ct_test_flowuns_R4[0][np.newaxis, :], len(time_steps))\n",
    "    cq_test_flowuns_R4 = align_timesteps(cq_test_flowuns_R4[0][np.newaxis, :], len(time_steps))\n",
    "    # shape = ct_test_flowuns_L1.shape\n",
    "\n",
    "    print(len(ct_test_flowuns_L1))\n",
    "\n",
    "    # ct_test_flowuns_L1 = align_timesteps(ct_test_flowuns_L1, len(time_steps))\n",
    "\n",
    "    model_outputs = {\n",
    "        \"Cl_Wing\": cl_test_NN_wing,\n",
    "        \"Cd_Wing\": cd_test_NN_wing,\n",
    "        \"Cl_Canard\": cl_test_NN_canard,\n",
    "        \"Cd_Canard\": cd_test_NN_canard,\n",
    "        \"Ct_L1\": ct_test_NN_L1,\n",
    "        \"Cq_L1\": cq_test_NN_L1,\n",
    "        \"Ct_L2\": ct_test_NN_L2,\n",
    "        \"Cq_L2\": cq_test_NN_L2,\n",
    "        \"Ct_L3\": ct_test_NN_L3,\n",
    "        \"Cq_L3\": cq_test_NN_L3,\n",
    "        \"Ct_L4\": ct_test_NN_L4,\n",
    "        \"Cq_L4\": cq_test_NN_L4,\n",
    "        \"Ct_R1\": ct_test_NN_R1,\n",
    "        \"Cq_R1\": cq_test_NN_R1,\n",
    "        \"Ct_R2\": ct_test_NN_R2,\n",
    "        \"Cq_R2\": cq_test_NN_R2,\n",
    "        \"Ct_R3\": ct_test_NN_R3,\n",
    "        \"Cq_R3\": cq_test_NN_R3,\n",
    "        \"Ct_R4\": ct_test_NN_R4,\n",
    "        \"Cq_R4\": cq_test_NN_R4,\n",
    "    }\n",
    "\n",
    "    ground_truths = {\n",
    "        \"Cl_Wing\": cl_test_flowuns_wing[0],\n",
    "        \"Cd_Wing\": cd_test_flowuns_wing[0],\n",
    "        \"Cl_Canard\": cl_test_flowuns_canard[0],\n",
    "        \"Cd_Canard\": cd_test_flowuns_canard[0],\n",
    "        \"Ct_L1\": ct_test_flowuns_L1[0],\n",
    "        \"Cq_L1\": cq_test_flowuns_L1[0],\n",
    "        \"Ct_L2\": ct_test_flowuns_L2[0],\n",
    "        \"Cq_L2\": cq_test_flowuns_L2[0],\n",
    "        \"Ct_L3\": ct_test_flowuns_L3[0],\n",
    "        \"Cq_L3\": cq_test_flowuns_L3[0], \n",
    "        \"Ct_L4\": ct_test_flowuns_L4[0],\n",
    "        \"Cq_L4\": cq_test_flowuns_L4[0],\n",
    "        \"Ct_R1\": ct_test_flowuns_R1[0],\n",
    "        \"Cq_R1\": cq_test_flowuns_R1[0],\n",
    "        \"Ct_R2\": ct_test_flowuns_R2[0],\n",
    "        \"Cq_R2\": cq_test_flowuns_R2[0],\n",
    "        \"Ct_R3\": ct_test_flowuns_R3[0],\n",
    "        \"Cq_R3\": cq_test_flowuns_R3[0],\n",
    "        \"Ct_R4\": ct_test_flowuns_R4[0],\n",
    "        \"Cq_R4\": cq_test_flowuns_R4[0],\n",
    "    }\n",
    "\n",
    "    # List of labels for plotting\n",
    "    labels = list(model_outputs.keys())  # Ensures ordering matches\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape_wing_cl = mape(ground_truths[\"Cl_Wing\"], model_outputs[\"Cl_Wing\"])\n",
    "    mape_wing_cd = mape(ground_truths[\"Cd_Wing\"], model_outputs[\"Cd_Wing\"])\n",
    "    mape_canard_cl = mape(ground_truths[\"Cl_Canard\"], model_outputs[\"Cl_Canard\"])\n",
    "    mape_canard_cd = mape(ground_truths[\"Cd_Canard\"], model_outputs[\"Cd_Canard\"])\n",
    "    mape_r1_ct = mape(ground_truths[\"Ct_R1\"], model_outputs[\"Ct_R1\"])\n",
    "    mape_r1_cq = mape(ground_truths[\"Cq_R1\"], model_outputs[\"Cq_R1\"])\n",
    "    mape_r2_ct = mape(ground_truths[\"Ct_R2\"], model_outputs[\"Ct_R2\"])\n",
    "    mape_r2_cq = mape(ground_truths[\"Cq_R2\"], model_outputs[\"Cq_R2\"])\n",
    "    mape_r3_ct = mape(ground_truths[\"Ct_R3\"], model_outputs[\"Ct_R3\"])\n",
    "    mape_r3_cq = mape(ground_truths[\"Cq_R3\"], model_outputs[\"Cq_R3\"])\n",
    "    mape_r4_ct = mape(ground_truths[\"Ct_R4\"], model_outputs[\"Ct_R4\"])\n",
    "    mape_r4_cq = mape(ground_truths[\"Cq_R4\"], model_outputs[\"Cq_R4\"])\n",
    "    mape_l1_ct = mape(ground_truths[\"Ct_L1\"], model_outputs[\"Ct_L1\"])\n",
    "    mape_l1_cq = mape(ground_truths[\"Cq_L1\"], model_outputs[\"Cq_L1\"])   \n",
    "    mape_l2_ct = mape(ground_truths[\"Ct_L2\"], model_outputs[\"Ct_L2\"])\n",
    "    mape_l2_cq = mape(ground_truths[\"Cq_L2\"], model_outputs[\"Cq_L2\"])\n",
    "    mape_l3_ct = mape(ground_truths[\"Ct_L3\"], model_outputs[\"Ct_L3\"])\n",
    "    mape_l3_cq = mape(ground_truths[\"Cq_L3\"], model_outputs[\"Cq_L3\"])\n",
    "    mape_l4_ct = mape(ground_truths[\"Ct_L4\"], model_outputs[\"Ct_L4\"])\n",
    "    mape_l4_cq = mape(ground_truths[\"Cq_L4\"], model_outputs[\"Cq_L4\"])\n",
    "\n",
    "    mape_wing_cl_list.append(mape_wing_cl)\n",
    "    mape_wing_cd_list.append(mape_wing_cd)\n",
    "    mape_canard_cl_list.append(mape_canard_cl)\n",
    "    mape_canard_cd_list.append(mape_canard_cd)\n",
    "    mape_r1_ct_list.append(mape_r1_ct)\n",
    "    mape_r1_cq_list.append(mape_r1_cq)\n",
    "    mape_r2_ct_list.append(mape_r2_ct)\n",
    "    mape_r2_cq_list.append(mape_r2_cq)\n",
    "    mape_r3_ct_list.append(mape_r3_ct)\n",
    "    mape_r3_cq_list.append(mape_r3_cq)\n",
    "    mape_r4_ct_list.append(mape_r4_ct)\n",
    "    mape_r4_cq_list.append(mape_r4_cq)\n",
    "    mape_l1_ct_list.append(mape_l1_ct)\n",
    "    mape_l1_cq_list.append(mape_l1_cq)\n",
    "    mape_l2_ct_list.append(mape_l2_ct)\n",
    "    mape_l2_cq_list.append(mape_l2_cq)\n",
    "    mape_l3_ct_list.append(mape_l3_ct)\n",
    "    mape_l3_cq_list.append(mape_l3_cq)\n",
    "    mape_l4_ct_list.append(mape_l4_ct)\n",
    "    mape_l4_cq_list.append(mape_l4_cq)\n",
    "\n",
    "    mape_wing = (mape_wing_cl + mape_wing_cd) / 2\n",
    "    mape_canard = (mape_canard_cl + mape_canard_cd) / 2\n",
    "    mape_r1 = (mape_r1_ct + mape_r1_cq) / 2\n",
    "    mape_r2 = (mape_r2_ct + mape_r2_cq) / 2\n",
    "    mape_r3 = (mape_r3_ct + mape_r3_cq) / 2\n",
    "    mape_r4 = (mape_r4_ct + mape_r4_cq) / 2\n",
    "    mape_l1 = (mape_l1_ct + mape_l1_cq) / 2\n",
    "    mape_l2 = (mape_l2_ct + mape_l2_cq) / 2\n",
    "    mape_l3 = (mape_l3_ct + mape_l3_cq) / 2\n",
    "    mape_l4 = (mape_l4_ct + mape_l4_cq) / 2 \n",
    "\n",
    "    mape_wing_list.append(mape_wing)\n",
    "    mape_canard_list.append(mape_canard)\n",
    "    mape_r1_list.append(mape_r1)\n",
    "    mape_r2_list.append(mape_r2)\n",
    "    mape_r3_list.append(mape_r3)\n",
    "    mape_r4_list.append(mape_r4)\n",
    "    mape_l1_list.append(mape_l1)\n",
    "    mape_l2_list.append(mape_l2)\n",
    "    mape_l3_list.append(mape_l3)\n",
    "    mape_l4_list.append(mape_l4)\n",
    "\n",
    "\n",
    "    # Calculate R2\n",
    "    r2_wing_cl = r2_score(ground_truths[\"Cl_Wing\"], model_outputs[\"Cl_Wing\"])\n",
    "    r2_wing_cd = r2_score(ground_truths[\"Cd_Wing\"], model_outputs[\"Cd_Wing\"])\n",
    "    r2_canard_cl = r2_score(ground_truths[\"Cl_Canard\"], model_outputs[\"Cl_Canard\"])\n",
    "    r2_canard_cd = r2_score(ground_truths[\"Cd_Canard\"], model_outputs[\"Cd_Canard\"])\n",
    "    r2_r1_ct = r2_score(ground_truths[\"Ct_R1\"], model_outputs[\"Ct_R1\"])\n",
    "    r2_r1_cq = r2_score(ground_truths[\"Cq_R1\"], model_outputs[\"Cq_R1\"])\n",
    "    r2_r2_ct = r2_score(ground_truths[\"Ct_R2\"], model_outputs[\"Ct_R2\"])\n",
    "    r2_r2_cq = r2_score(ground_truths[\"Cq_R2\"], model_outputs[\"Cq_R2\"])\n",
    "    r2_r3_ct = r2_score(ground_truths[\"Ct_R3\"], model_outputs[\"Ct_R3\"])\n",
    "    r2_r3_cq = r2_score(ground_truths[\"Cq_R3\"], model_outputs[\"Cq_R3\"])\n",
    "    r2_r4_ct = r2_score(ground_truths[\"Ct_R4\"], model_outputs[\"Ct_R4\"])\n",
    "    r2_r4_cq = r2_score(ground_truths[\"Cq_R4\"], model_outputs[\"Cq_R4\"])\n",
    "    r2_l1_ct = r2_score(ground_truths[\"Ct_L1\"], model_outputs[\"Ct_L1\"])\n",
    "    r2_l1_cq = r2_score(ground_truths[\"Cq_L1\"], model_outputs[\"Cq_L1\"])\n",
    "    r2_l2_ct = r2_score(ground_truths[\"Ct_L2\"], model_outputs[\"Ct_L2\"])\n",
    "    r2_l2_cq = r2_score(ground_truths[\"Cq_L2\"], model_outputs[\"Cq_L2\"])\n",
    "    r2_l3_ct = r2_score(ground_truths[\"Ct_L3\"], model_outputs[\"Ct_L3\"])\n",
    "    r2_l3_cq = r2_score(ground_truths[\"Cq_L3\"], model_outputs[\"Cq_L3\"])\n",
    "    r2_l4_ct = r2_score(ground_truths[\"Ct_L4\"], model_outputs[\"Ct_L4\"])\n",
    "    r2_l4_cq = r2_score(ground_truths[\"Cq_L4\"], model_outputs[\"Cq_L4\"])\n",
    "\n",
    "    r2_wing_cl_list.append(r2_wing_cl)\n",
    "    r2_wing_cd_list.append(r2_wing_cd)\n",
    "    r2_canard_cl_list.append(r2_canard_cl)\n",
    "    r2_canard_cd_list.append(r2_canard_cd)\n",
    "    r2_r1_ct_list.append(r2_r1_ct)\n",
    "    r2_r1_cq_list.append(r2_r1_cq)\n",
    "    r2_r2_ct_list.append(r2_r2_ct)\n",
    "    r2_r2_cq_list.append(r2_r2_cq)\n",
    "    r2_r3_ct_list.append(r2_r3_ct)\n",
    "    r2_r3_cq_list.append(r2_r3_cq)\n",
    "    r2_r4_ct_list.append(r2_r4_ct)\n",
    "    r2_r4_cq_list.append(r2_r4_cq)\n",
    "    r2_l1_ct_list.append(r2_l1_ct)\n",
    "    r2_l1_cq_list.append(r2_l1_cq)\n",
    "    r2_l2_ct_list.append(r2_l2_ct)\n",
    "    r2_l2_cq_list.append(r2_l2_cq)\n",
    "    r2_l3_ct_list.append(r2_l3_ct)\n",
    "    r2_l3_cq_list.append(r2_l3_cq)\n",
    "    r2_l4_ct_list.append(r2_l4_ct)\n",
    "    r2_l4_cq_list.append(r2_l4_cq)\n",
    "\n",
    "    r2_wing = (r2_wing_cl + r2_wing_cd) / 2\n",
    "    r2_canard = (r2_canard_cl + r2_canard_cd) / 2\n",
    "    r2_r1 = (r2_r1_ct + r2_r1_cq) / 2\n",
    "    r2_r2 = (r2_r2_ct + r2_r2_cq) / 2\n",
    "    r2_r3 = (r2_r3_ct + r2_r3_cq) / 2\n",
    "    r2_r4 = (r2_r4_ct + r2_r4_cq) / 2\n",
    "    r2_l1 = (r2_l1_ct + r2_l1_cq) / 2\n",
    "    r2_l2 = (r2_l2_ct + r2_l2_cq) / 2\n",
    "    r2_l3 = (r2_l3_ct + r2_l3_cq) / 2\n",
    "    r2_l4 = (r2_l4_ct + r2_l4_cq) / 2\n",
    "\n",
    "    r2_wing_list.append(r2_wing)\n",
    "    r2_canard_list.append(r2_canard)\n",
    "    r2_r1_list.append(r2_r1)\n",
    "    r2_r2_list.append(r2_r2)\n",
    "    r2_r3_list.append(r2_r3)\n",
    "    r2_r4_list.append(r2_r4)\n",
    "    r2_l1_list.append(r2_l1)\n",
    "    r2_l2_list.append(r2_l2)\n",
    "    r2_l3_list.append(r2_l3)\n",
    "    r2_l4_list.append(r2_l4)\n",
    "\n",
    "    # Calculate relative l2 error\n",
    "    epsilon_wing_cl = relative_l2_norm(ground_truths[\"Cl_Wing\"], model_outputs[\"Cl_Wing\"])\n",
    "    epsilon_wing_cd = relative_l2_norm(ground_truths[\"Cd_Wing\"], model_outputs[\"Cd_Wing\"])\n",
    "    epsilon_canard_cl = relative_l2_norm(ground_truths[\"Cl_Canard\"], model_outputs[\"Cl_Canard\"])\n",
    "    epsilon_canard_cd = relative_l2_norm(ground_truths[\"Cd_Canard\"], model_outputs[\"Cd_Canard\"])\n",
    "    epsilon_r1_ct = relative_l2_norm(ground_truths[\"Ct_R1\"], model_outputs[\"Ct_R1\"])\n",
    "    epsilon_r1_cq = relative_l2_norm(ground_truths[\"Cq_R1\"], model_outputs[\"Cq_R1\"])\n",
    "    epsilon_r2_ct = relative_l2_norm(ground_truths[\"Ct_R2\"], model_outputs[\"Ct_R2\"])\n",
    "    epsilon_r2_cq = relative_l2_norm(ground_truths[\"Cq_R2\"], model_outputs[\"Cq_R2\"])\n",
    "    epsilon_r3_ct = relative_l2_norm(ground_truths[\"Ct_R3\"], model_outputs[\"Ct_R3\"])\n",
    "    epsilon_r3_cq = relative_l2_norm(ground_truths[\"Cq_R3\"], model_outputs[\"Cq_R3\"])\n",
    "    epsilon_r4_ct = relative_l2_norm(ground_truths[\"Ct_R4\"], model_outputs[\"Ct_R4\"])\n",
    "    epsilon_r4_cq = relative_l2_norm(ground_truths[\"Cq_R4\"], model_outputs[\"Cq_R4\"])\n",
    "    epsilon_l1_ct = relative_l2_norm(ground_truths[\"Ct_L1\"], model_outputs[\"Ct_L1\"])\n",
    "    epsilon_l1_cq = relative_l2_norm(ground_truths[\"Cq_L1\"], model_outputs[\"Cq_L1\"])\n",
    "    epsilon_l2_ct = relative_l2_norm(ground_truths[\"Ct_L2\"], model_outputs[\"Ct_L2\"])\n",
    "    epsilon_l2_cq = relative_l2_norm(ground_truths[\"Cq_L2\"], model_outputs[\"Cq_L2\"])\n",
    "    epsilon_l3_ct = relative_l2_norm(ground_truths[\"Ct_L3\"], model_outputs[\"Ct_L3\"])\n",
    "    epsilon_l3_cq = relative_l2_norm(ground_truths[\"Cq_L3\"], model_outputs[\"Cq_L3\"])\n",
    "    epsilon_l4_ct = relative_l2_norm(ground_truths[\"Ct_L4\"], model_outputs[\"Ct_L4\"])\n",
    "    epsilon_l4_cq = relative_l2_norm(ground_truths[\"Cq_L4\"], model_outputs[\"Cq_L4\"])\n",
    "\n",
    "    epsilon_wing_cl_list.append(epsilon_wing_cl)\n",
    "    epsilon_wing_cd_list.append(epsilon_wing_cd)\n",
    "    epsilon_canard_cl_list.append(epsilon_canard_cl)\n",
    "    epsilon_canard_cd_list.append(epsilon_canard_cd)\n",
    "    epsilon_r1_ct_list.append(epsilon_r1_ct)\n",
    "    epsilon_r1_cq_list.append(epsilon_r1_cq)\n",
    "    epsilon_r2_ct_list.append(epsilon_r2_ct)\n",
    "    epsilon_r2_cq_list.append(epsilon_r2_cq)\n",
    "    epsilon_r3_ct_list.append(epsilon_r3_ct)\n",
    "    epsilon_r3_cq_list.append(epsilon_r3_cq)\n",
    "    epsilon_r4_ct_list.append(epsilon_r4_ct)\n",
    "    epsilon_r4_cq_list.append(epsilon_r4_cq)\n",
    "    epsilon_l1_ct_list.append(epsilon_l1_ct)\n",
    "    epsilon_l1_cq_list.append(epsilon_l1_cq)\n",
    "    epsilon_l2_ct_list.append(epsilon_l2_ct)\n",
    "    epsilon_l2_cq_list.append(epsilon_l2_cq)\n",
    "    epsilon_l3_ct_list.append(epsilon_l3_ct)\n",
    "    epsilon_l3_cq_list.append(epsilon_l3_cq)\n",
    "    epsilon_l4_ct_list.append(epsilon_l4_ct)\n",
    "    epsilon_l4_cq_list.append(epsilon_l4_cq)\n",
    "\n",
    "    epsilon_wing = (epsilon_wing_cl + epsilon_wing_cd) / 2\n",
    "    epsilon_canard = (epsilon_canard_cl + epsilon_canard_cd) / 2\n",
    "    epsilon_r1 = (epsilon_r1_ct + epsilon_r1_cq) / 2\n",
    "    epsilon_r2 = (epsilon_r2_ct + epsilon_r2_cq) / 2\n",
    "    epsilon_r3 = (epsilon_r3_ct + epsilon_r3_cq) / 2\n",
    "    epsilon_r4 = (epsilon_r4_ct + epsilon_r4_cq) / 2\n",
    "    epsilon_l1 = (epsilon_l1_ct + epsilon_l1_cq) / 2\n",
    "    epsilon_l2 = (epsilon_l2_ct + epsilon_l2_cq) / 2\n",
    "    epsilon_l3 = (epsilon_l3_ct + epsilon_l3_cq) / 2\n",
    "    epsilon_l4 = (epsilon_l4_ct + epsilon_l4_cq) / 2\n",
    "\n",
    "    epsilon_wing_list.append(epsilon_wing)\n",
    "    epsilon_canard_list.append(epsilon_canard)\n",
    "    epsilon_r1_list.append(epsilon_r1)\n",
    "    epsilon_r2_list.append(epsilon_r2)\n",
    "    epsilon_r3_list.append(epsilon_r3)\n",
    "    epsilon_r4_list.append(epsilon_r4)\n",
    "    epsilon_l1_list.append(epsilon_l1)\n",
    "    epsilon_l2_list.append(epsilon_l2)\n",
    "    epsilon_l3_list.append(epsilon_l3)\n",
    "    epsilon_l4_list.append(epsilon_l4)\n",
    "\n",
    "    print(\"Simulation Case:\", simulation_case)  \n",
    "    simulation_case_list = simulation_case.split('_')\n",
    "    vinf = simulation_case_list[2].split('v')[1]\n",
    "    rpm = simulation_case_list[3].split('r')[1]\n",
    "    alpha = simulation_case_list[4].split('a')[1]\n",
    "\n",
    "    simulation_case_list.append(simulation_case)\n",
    "\n",
    "    # save_path = root_test_dir + simulation_case + '_NN_predictions.csv'\n",
    "\n",
    "    print(\"MAPE Wing CL:\", mape_wing_cl, \"MAPE Wing CD:\", mape_wing_cd, \"epsilon Wing CL:\", epsilon_wing_cl, \"epsilon Wing CD:\", epsilon_wing_cd, \"R2 Wing CL:\", r2_wing_cl, \"R2 Wing CD:\", r2_wing_cd)\n",
    "    print(\"MAPE Canard CL:\", mape_canard_cl, \"MAPE Canard CD:\", mape_canard_cd, \"epsilon Canard CL:\", epsilon_canard_cl, \"epsilon Canard CD:\", epsilon_canard_cd, \"R2 Canard CL:\", r2_canard_cl, \"R2 Canard CD:\", r2_canard_cd)\n",
    "    print(\"MAPE R1 CT:\", mape_r1_ct, \"MAPE R1 CQ:\", mape_r1_cq, \"epsilon R1 CT:\", epsilon_r1_ct, \"epsilon R1 CQ:\", epsilon_r1_cq, \"R2 R1 CT:\", r2_r1_ct, \"R2 R1 CQ:\", r2_r1_cq)\n",
    "    print(\"MAPE R2 CT:\", mape_r2_ct, \"MAPE R2 CQ:\", mape_r2_cq, \"epsilon R2 CT:\", epsilon_r2_ct, \"epsilon R2 CQ:\", epsilon_r2_cq, \"R2 R2 CT:\", r2_r2_ct, \"R2 R2 CQ:\", r2_r2_cq)\n",
    "    print(\"MAPE R3 CT:\", mape_r3_ct, \"MAPE R3 CQ:\", mape_r3_cq, \"epsilon R3 CT:\", epsilon_r3_ct, \"epsilon R3 CQ:\", epsilon_r3_cq, \"R2 R3 CT:\", r2_r3_ct, \"R2 R3 CQ:\", r2_r3_cq)\n",
    "    print(\"MAPE R4 CT:\", mape_r4_ct, \"MAPE R4 CQ:\", mape_r4_cq, \"epsilon R4 CT:\", epsilon_r4_ct, \"epsilon R4 CQ:\", epsilon_r4_cq, \"R2 R4 CT:\", r2_r4_ct, \"R2 R4 CQ:\", r2_r4_cq)\n",
    "    print(\"MAPE L1 CT:\", mape_l1_ct, \"MAPE L1 CQ:\", mape_l1_cq, \"epsilon L1 CT:\", epsilon_l1_ct, \"epsilon L1 CQ:\", epsilon_l1_cq, \"R2 L1 CT:\", r2_l1_ct, \"R2 L1 CQ:\", r2_l1_cq)\n",
    "    print(\"MAPE L2 CT:\", mape_l2_ct, \"MAPE L2 CQ:\", mape_l2_cq, \"epsilon L2 CT:\", epsilon_l2_ct, \"epsilon L2 CQ:\", epsilon_l2_cq, \"R2 L2 CT:\", r2_l2_ct, \"R2 L2 CQ:\", r2_l2_cq)\n",
    "    print(\"MAPE L3 CT:\", mape_l3_ct, \"MAPE L3 CQ:\", mape_l3_cq, \"epsilon L3 CT:\", epsilon_l3_ct, \"epsilon L3 CQ:\", epsilon_l3_cq, \"R2 L3 CT:\", r2_l3_ct, \"R2 L3 CQ:\", r2_l3_cq)\n",
    "    print(\"MAPE L4 CT:\", mape_l4_ct, \"MAPE L4 CQ:\", mape_l4_cq, \"epsilon L4 CT:\", epsilon_l4_ct, \"epsilon L4 CQ:\", epsilon_l4_cq, \"R2 L4 CT:\", r2_l4_ct, \"R2 L4 CQ:\", r2_l4_cq)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    save_path = result_out_path+\"/new_results/\"\n",
    "    # os.makedirs(save_path, exist_ok=True)\n",
    "    if PLOT_RESULT:\n",
    "        plot_and_save_model_vs_ground_truth(time_steps, model_outputs, ground_truths, alpha, vinf, simulation_case, save_csv=False, csv_filename='predictions.csv', save_path=save_path)\n",
    "        # Plot Thrust Coefficients\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Ground Truth (FLOWUnsteady)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(time_steps, ct_test_flowuns_L1[0], label='Actual $C_T$ L1', color='red', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_flowuns_L2[0], label='Actual $C_T$ L2', color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_flowuns_L3[0], label='Actual $C_T$ L3', color='green', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_flowuns_L4[0], label='Actual $C_T$ L4', color='purple', linestyle='-', linewidth=2)\n",
    "        plt.xlabel('Time [s]', fontsize=12)\n",
    "        plt.ylabel('Thrust Coefficient, $C_T$', fontsize=12)\n",
    "        plt.title(f'Thrust Coefficient Comparison - FLOWUnsteady', fontsize=14)\n",
    "        plt.legend(loc='upper center', fontsize=12, fancybox=True, ncol=2, title='$AoA$ = ${}^{{\\circ}}$, $V_{{\\infty}}$ = ${} m/s$, ${{\\omega}}$ = ${} RPM$'.format(alpha, vinf, rpm), title_fontsize='14')\n",
    "        plt.ylim(ct_test_flowuns_L4[0].min() - 0.003, ct_test_flowuns_L1[0].max() + 0.003)\n",
    "        # plt.ylim(ct_test_flowuns_L4[0].min() - 0.002, ct_test_flowuns_L1[0].max() + 0.002)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "        # Predicted (Neural Network)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(time_steps, ct_test_NN_L1[0:], label='Predicted $C_T$ L1', color='red', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_NN_L2[0:], label='Predicted $C_T$ L2', color='blue', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_NN_L3[0:], label='Predicted $C_T$ L3', color='green', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, ct_test_NN_L4[0:], label='Predicted $C_T$ L4', color='purple', linestyle='--', linewidth=2)\n",
    "        plt.xlabel('Time [s]', fontsize=12)\n",
    "        plt.ylabel('Thrust Coefficient, $C_T$', fontsize=12)\n",
    "        plt.title(f'Thrust Coefficient Comparison - NN Model', fontsize=14)\n",
    "        plt.legend(loc='upper center', fontsize=12, fancybox=True, ncol=2, title='$AoA$ = ${}^{{\\circ}}$, $V_{{\\infty}}$ = ${} m/s$, ${{\\omega}}$ = ${} RPM$'.format(alpha, vinf, rpm), title_fontsize='14')\n",
    "        plt.ylim(ct_test_flowuns_L4[0].min() - 0.003, ct_test_flowuns_L1[0].max() + 0.003)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if SAVE_PLOT:\n",
    "            thrust_plot_filename = os.path.join(save_path, f\"thrust_coefficients_{simulation_case}.pdf\")\n",
    "            plt.savefig(thrust_plot_filename, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Torque Coefficients\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Ground Truth (FLOWUnsteady)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(time_steps, cq_test_flowuns_L1[0], label='Actual $C_Q$ L1', color='red', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_flowuns_L2[0], label='Actual $C_Q$ L2', color='blue', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_flowuns_L3[0], label='Actual $C_Q$ L3', color='green', linestyle='-', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_flowuns_L4[0], label='Actual $C_Q$ L4', color='purple', linestyle='-', linewidth=2)\n",
    "        plt.xlabel('Time [s]', fontsize=12)\n",
    "        plt.ylabel('Torque Coefficient, $C_Q$', fontsize=12)\n",
    "        plt.title(f'Torque Coefficient Comparison - FLOWUnsteady', fontsize=14)\n",
    "        plt.legend(loc='upper center', fontsize=12, fancybox=True, ncol=2, title='$AoA$ = ${}^{{\\circ}}$, $V_{{\\infty}}$ = ${} m/s$, ${{\\omega}}$ = ${} RPM$'.format(alpha, vinf, rpm), title_fontsize='14')\n",
    "        plt.ylim(cq_test_flowuns_L4[0].min() - 0.0003, cq_test_flowuns_L1[0].max() + 0.0003)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "        # Predicted (Neural Network)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(time_steps, cq_test_NN_L1[0:], label='Predicted $C_Q$ L1', color='red', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_NN_L2[0:], label='Predicted $C_Q$ L2', color='blue', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_NN_L3[0:], label='Predicted $C_Q$ L3', color='green', linestyle='--', linewidth=2)\n",
    "        plt.plot(time_steps, cq_test_NN_L4[0:], label='Predicted $C_Q$ L4', color='purple', linestyle='--', linewidth=2)\n",
    "        plt.xlabel('Time [s]', fontsize=12)\n",
    "        plt.ylabel('Torque Coefficient, $C_Q$', fontsize=12)\n",
    "        plt.title(f'Torque Coefficient Comparison - NN Model', fontsize=14)\n",
    "        plt.legend(loc='upper center', fontsize=12, fancybox=True, ncol=2, title='$AoA$ = ${}^{{\\circ}}$, $V_{{\\infty}}$ = ${} m/s$, ${{\\omega}}$ = ${} RPM$'.format(alpha, vinf, rpm), title_fontsize='14')\n",
    "        plt.ylim(cq_test_flowuns_L4[0].min() - 0.0003, cq_test_flowuns_L1[0].max() + 0.0003)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if SAVE_PLOT:\n",
    "            torque_plot_filename = os.path.join(save_path, f\"torque_coefficients_{simulation_case}.pdf\")\n",
    "            plt.savefig(torque_plot_filename, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Average MAPE\n",
    "if SAVE_RESULT:\n",
    "    mape_wing_cl_avg = np.mean(mape_wing_cl_list)\n",
    "    mape_wing_cd_avg = np.mean(mape_wing_cd_list)   \n",
    "    mape_canard_cl_avg = np.mean(mape_canard_cl_list)\n",
    "    mape_canard_cd_avg = np.mean(mape_canard_cd_list)\n",
    "    mape_r1_ct_avg = np.mean(mape_r1_ct_list)\n",
    "    mape_r1_cq_avg = np.mean(mape_r1_cq_list)\n",
    "    mape_r2_ct_avg = np.mean(mape_r2_ct_list)\n",
    "    mape_r2_cq_avg = np.mean(mape_r2_cq_list)\n",
    "    mape_r3_ct_avg = np.mean(mape_r3_ct_list)\n",
    "    mape_r3_cq_avg = np.mean(mape_r3_cq_list)\n",
    "    mape_r4_ct_avg = np.mean(mape_r4_ct_list)\n",
    "    mape_r4_cq_avg = np.mean(mape_r4_cq_list)\n",
    "    mape_l1_ct_avg = np.mean(mape_l1_ct_list)\n",
    "    mape_l1_cq_avg = np.mean(mape_l1_cq_list)\n",
    "    mape_l2_ct_avg = np.mean(mape_l2_ct_list)\n",
    "    mape_l2_cq_avg = np.mean(mape_l2_cq_list)\n",
    "    mape_l3_ct_avg = np.mean(mape_l3_ct_list)\n",
    "    mape_l3_cq_avg = np.mean(mape_l3_cq_list)\n",
    "    mape_l4_ct_avg = np.mean(mape_l4_ct_list)\n",
    "    mape_l4_cq_avg = np.mean(mape_l4_cq_list)\n",
    "\n",
    "    mape_wing_avg = np.mean(mape_wing_list)\n",
    "    mape_canard_avg = np.mean(mape_canard_list)\n",
    "    mape_r1_avg = np.mean(mape_r1_list)\n",
    "    mape_r2_avg = np.mean(mape_r2_list)\n",
    "    mape_r3_avg = np.mean(mape_r3_list)\n",
    "    mape_r4_avg = np.mean(mape_r4_list)\n",
    "    mape_l1_avg = np.mean(mape_l1_list)\n",
    "    mape_l2_avg = np.mean(mape_l2_list)\n",
    "    mape_l3_avg = np.mean(mape_l3_list)\n",
    "    mape_l4_avg = np.mean(mape_l4_list)\n",
    "\n",
    "    print(\"################################################################################################################\")\n",
    "\n",
    "    print(\"Average MAPE Wing CL:\", mape_wing_cl_avg, \"Average MAPE Wing CD:\", mape_wing_cd_avg, \"Average MAPE Wing:\", mape_wing_avg)\n",
    "    print(\"Average MAPE Canard CL:\", mape_canard_cl_avg, \"Average MAPE Canard CD:\", mape_canard_cd_avg, \"Average MAPE Canard:\", mape_canard_avg)\n",
    "    print(\"Average MAPE Rotor L1 CT:\", mape_r1_ct_avg, \"Average MAPE Rotor L1 CQ:\", mape_r1_cq_avg, \"Average MAPE Rotor L1:\", mape_r1_avg)\n",
    "    print(\"Average MAPE Rotor L2 CT:\", mape_r2_ct_avg, \"Average MAPE Rotor L2 CQ:\", mape_r2_cq_avg, \"Average MAPE Rotor L2:\", mape_r2_avg)\n",
    "    print(\"Average MAPE Rotor L3 CT:\", mape_r3_ct_avg, \"Average MAPE Rotor L3 CQ:\", mape_r3_cq_avg, \"Average MAPE Rotor L3:\", mape_r3_avg)\n",
    "    print(\"Average MAPE Rotor L4 CT:\", mape_r4_ct_avg, \"Average MAPE Rotor L4 CQ:\", mape_r4_cq_avg, \"Average MAPE Rotor L4:\", mape_r4_avg)\n",
    "    print(\"Average MAPE Rotor R1 CT:\", mape_l1_ct_avg, \"Average MAPE Rotor R1 CQ:\", mape_l1_cq_avg, \"Average MAPE Rotor R1:\", mape_l1_avg)\n",
    "    print(\"Average MAPE Rotor R2 CT:\", mape_l2_ct_avg, \"Average MAPE Rotor R2 CQ:\", mape_l2_cq_avg, \"Average MAPE Rotor R2:\", mape_l2_avg)\n",
    "    print(\"Average MAPE Rotor R3 CT:\", mape_l3_ct_avg, \"Average MAPE Rotor R3 CQ:\", mape_l3_cq_avg, \"Average MAPE Rotor R3:\", mape_l3_avg)\n",
    "    print(\"Average MAPE Rotor R4 CT:\", mape_l4_ct_avg, \"Average MAPE Rotor R4 CQ:\", mape_l4_cq_avg, \"Average MAPE Rotor R4:\", mape_l4_avg)\n",
    "\n",
    "    epsilon_wing_cl_avg = np.mean(epsilon_wing_cl_list)\n",
    "    epsilon_wing_cd_avg = np.mean(epsilon_wing_cd_list)\n",
    "    epsilon_canard_cl_avg = np.mean(epsilon_canard_cl_list)\n",
    "    epsilon_canard_cd_avg = np.mean(epsilon_canard_cd_list)\n",
    "    epsilon_r1_ct_avg = np.mean(epsilon_r1_ct_list)\n",
    "    epsilon_r1_cq_avg = np.mean(epsilon_r1_cq_list)\n",
    "    epsilon_r2_ct_avg = np.mean(epsilon_r2_ct_list)\n",
    "    epsilon_r2_cq_avg = np.mean(epsilon_r2_cq_list)\n",
    "    epsilon_r3_ct_avg = np.mean(epsilon_r3_ct_list)\n",
    "    epsilon_r3_cq_avg = np.mean(epsilon_r3_cq_list)\n",
    "    epsilon_r4_ct_avg = np.mean(epsilon_r4_ct_list)\n",
    "    epsilon_r4_cq_avg = np.mean(epsilon_r4_cq_list)\n",
    "    epsilon_l1_ct_avg = np.mean(epsilon_l1_ct_list)\n",
    "    epsilon_l1_cq_avg = np.mean(epsilon_l1_cq_list)\n",
    "    epsilon_l2_ct_avg = np.mean(epsilon_l2_ct_list)\n",
    "    epsilon_l2_cq_avg = np.mean(epsilon_l2_cq_list)\n",
    "    epsilon_l3_ct_avg = np.mean(epsilon_l3_ct_list)\n",
    "    epsilon_l3_cq_avg = np.mean(epsilon_l3_cq_list)\n",
    "    epsilon_l4_ct_avg = np.mean(epsilon_l4_ct_list)\n",
    "    epsilon_l4_cq_avg = np.mean(epsilon_l4_cq_list)\n",
    "\n",
    "    epsilon_wing_avg = np.mean(epsilon_wing_list)\n",
    "    epsilon_canard_avg = np.mean(epsilon_canard_list)\n",
    "    epsilon_r1_avg = np.mean(epsilon_r1_list)\n",
    "    epsilon_r2_avg = np.mean(epsilon_r2_list)\n",
    "    epsilon_r3_avg = np.mean(epsilon_r3_list)\n",
    "    epsilon_r4_avg = np.mean(epsilon_r4_list)\n",
    "    epsilon_l1_avg = np.mean(epsilon_l1_list)\n",
    "    epsilon_l2_avg = np.mean(epsilon_l2_list)\n",
    "    epsilon_l3_avg = np.mean(epsilon_l3_list)\n",
    "    epsilon_l4_avg = np.mean(epsilon_l4_list)\n",
    "\n",
    "    print(\"################################################################################################################\")\n",
    "\n",
    "    print(\"Average epsilon Wing CL:\", epsilon_wing_cl_avg, \"Average epsilon Wing CD:\", epsilon_wing_cd_avg, \"Average epsilon Wing:\", epsilon_wing_avg)\n",
    "    print(\"Average epsilon Canard CL:\", epsilon_canard_cl_avg, \"Average epsilon Canard CD:\", epsilon_canard_cd_avg, \"Average epsilon Canard:\", epsilon_canard_avg)\n",
    "    print(\"Average epsilon Rotor L1 CT:\", epsilon_r1_ct_avg, \"Average epsilon Rotor L1 CQ:\", epsilon_r1_cq_avg, \"Average epsilon Rotor L1:\", epsilon_r1_avg)\n",
    "    print(\"Average epsilon Rotor L2 CT:\", epsilon_r2_ct_avg, \"Average epsilon Rotor L2 CQ:\", epsilon_r2_cq_avg, \"Average epsilon Rotor L2:\", epsilon_r2_avg)\n",
    "    print(\"Average epsilon Rotor L3 CT:\", epsilon_r3_ct_avg, \"Average epsilon Rotor L3 CQ:\", epsilon_r3_cq_avg, \"Average epsilon Rotor L3:\", epsilon_r3_avg)\n",
    "    print(\"Average epsilon Rotor L4 CT:\", epsilon_r4_ct_avg, \"Average epsilon Rotor L4 CQ:\", epsilon_r4_cq_avg, \"Average epsilon Rotor L4:\", epsilon_r4_avg)\n",
    "    print(\"Average epsilon Rotor R1 CT:\", epsilon_l1_ct_avg, \"Average epsilon Rotor R1 CQ:\", epsilon_l1_cq_avg, \"Average epsilon Rotor R1:\", epsilon_l1_avg)\n",
    "    print(\"Average epsilon Rotor R2 CT:\", epsilon_l2_ct_avg, \"Average epsilon Rotor R2 CQ:\", epsilon_l2_cq_avg, \"Average epsilon Rotor R2:\", epsilon_l2_avg)\n",
    "    print(\"Average epsilon Rotor R3 CT:\", epsilon_l3_ct_avg, \"Average epsilon Rotor R3 CQ:\", epsilon_l3_cq_avg, \"Average epsilon Rotor R3:\", epsilon_l3_avg)\n",
    "    print(\"Average epsilon Rotor R4 CT:\", epsilon_l4_ct_avg, \"Average epsilon Rotor R4 CQ:\", epsilon_l4_cq_avg, \"Average epsilon Rotor R4:\", epsilon_l4_avg)\n",
    "\n",
    "    r2_wing_cl_avg = np.mean(r2_wing_cl_list)\n",
    "    r2_wing_cd_avg = np.mean(r2_wing_cd_list)\n",
    "    r2_canard_cl_avg = np.mean(r2_canard_cl_list)\n",
    "    r2_canard_cd_avg = np.mean(r2_canard_cd_list)\n",
    "    r2_r1_ct_avg = np.mean(r2_r1_ct_list)\n",
    "    r2_r1_cq_avg = np.mean(r2_r1_cq_list)\n",
    "    r2_r2_ct_avg = np.mean(r2_r2_ct_list)\n",
    "    r2_r2_cq_avg = np.mean(r2_r2_cq_list)\n",
    "    r2_r3_ct_avg = np.mean(r2_r3_ct_list)\n",
    "    r2_r3_cq_avg = np.mean(r2_r3_cq_list)\n",
    "    r2_r4_ct_avg = np.mean(r2_r4_ct_list)\n",
    "    r2_r4_cq_avg = np.mean(r2_r4_cq_list)\n",
    "    r2_l1_ct_avg = np.mean(r2_l1_ct_list)   \n",
    "    r2_l1_cq_avg = np.mean(r2_l1_cq_list)\n",
    "    r2_l2_ct_avg = np.mean(r2_l2_ct_list)\n",
    "    r2_l2_cq_avg = np.mean(r2_l2_cq_list)\n",
    "    r2_l3_ct_avg = np.mean(r2_l3_ct_list)\n",
    "    r2_l3_cq_avg = np.mean(r2_l3_cq_list)\n",
    "    r2_l4_ct_avg = np.mean(r2_l4_ct_list)\n",
    "    r2_l4_cq_avg = np.mean(r2_l4_cq_list)\n",
    "\n",
    "    r2_wing_avg = np.mean(r2_wing_list)\n",
    "    r2_canard_avg = np.mean(r2_canard_list)\n",
    "    r2_r1_avg = np.mean(r2_r1_list)\n",
    "    r2_r2_avg = np.mean(r2_r2_list)\n",
    "    r2_r3_avg = np.mean(r2_r3_list)\n",
    "    r2_r4_avg = np.mean(r2_r4_list)\n",
    "    r2_l1_avg = np.mean(r2_l1_list)\n",
    "    r2_l2_avg = np.mean(r2_l2_list)\n",
    "    r2_l3_avg = np.mean(r2_l3_list)\n",
    "    r2_l4_avg = np.mean(r2_l4_list)\n",
    "\n",
    "    print(\"################################################################################################################\")\n",
    "\n",
    "    print(\"Average R2 Wing CL:\", r2_wing_cl_avg, \"Average R2 Wing CD:\", r2_wing_cd_avg, \"Average R2 Wing:\", r2_wing_avg)\n",
    "    print(\"Average R2 Canard CL:\", r2_canard_cl_avg, \"Average R2 Canard CD:\", r2_canard_cd_avg, \"Average R2 Canard:\", r2_canard_avg)\n",
    "    print(\"Average R2 Rotor L1 CT:\", r2_r1_ct_avg, \"Average R2 Rotor L1 CQ:\", r2_r1_cq_avg, \"Average R2 Rotor L1:\", r2_r1_avg)\n",
    "    print(\"Average R2 Rotor L2 CT:\", r2_r2_ct_avg, \"Average R2 Rotor L2 CQ:\", r2_r2_cq_avg, \"Average R2 Rotor L2:\", r2_r2_avg)\n",
    "    print(\"Average R2 Rotor L3 CT:\", r2_r3_ct_avg, \"Average R2 Rotor L3 CQ:\", r2_r3_cq_avg, \"Average R2 Rotor L3:\", r2_r3_avg)\n",
    "    print(\"Average R2 Rotor L4 CT:\", r2_r4_ct_avg, \"Average R2 Rotor L4 CQ:\", r2_r4_cq_avg, \"Average R2 Rotor L4:\", r2_r4_avg)\n",
    "    print(\"Average R2 Rotor R1 CT:\", r2_l1_ct_avg, \"Average R2 Rotor R1 CQ:\", r2_l1_cq_avg, \"Average R2 Rotor R1:\", r2_l1_avg)\n",
    "    print(\"Average R2 Rotor R2 CT:\", r2_l2_ct_avg, \"Average R2 Rotor R2 CQ:\", r2_l2_cq_avg, \"Average R2 Rotor R2:\", r2_l2_avg)\n",
    "    print(\"Average R2 Rotor R3 CT:\", r2_l3_ct_avg, \"Average R2 Rotor R3 CQ:\", r2_l3_cq_avg, \"Average R2 Rotor R3:\", r2_l3_avg)\n",
    "    print(\"Average R2 Rotor R4 CT:\", r2_l4_ct_avg, \"Average R2 Rotor R4 CQ:\", r2_l4_cq_avg, \"Average R2 Rotor R4:\", r2_l4_avg)\n",
    "\n",
    "\n",
    "    # 1. Save average MAPE, Epsilon, and R² values\n",
    "    results_avg = {\n",
    "        \"Category\": [\n",
    "            \"Wing CL\", \"Wing CD\", \"Wing\", \n",
    "            \"Canard CL\", \"Canard CD\", \"Canard\",\n",
    "            \"Rotor L1 CT\", \"Rotor L1 CQ\", \"Rotor L1\",\n",
    "            \"Rotor L2 CT\", \"Rotor L2 CQ\", \"Rotor L2\",\n",
    "            \"Rotor L3 CT\", \"Rotor L3 CQ\", \"Rotor L3\",\n",
    "            \"Rotor L4 CT\", \"Rotor L4 CQ\", \"Rotor L4\",\n",
    "            \"Rotor R1 CT\", \"Rotor R1 CQ\", \"Rotor R1\",\n",
    "            \"Rotor R2 CT\", \"Rotor R2 CQ\", \"Rotor R2\",\n",
    "            \"Rotor R3 CT\", \"Rotor R3 CQ\", \"Rotor R3\",\n",
    "            \"Rotor R4 CT\", \"Rotor R4 CQ\", \"Rotor R4\"\n",
    "        ],\n",
    "        \"MAPE\": [\n",
    "            mape_wing_cl_avg, mape_wing_cd_avg, mape_wing_avg,\n",
    "            mape_canard_cl_avg, mape_canard_cd_avg, mape_canard_avg,\n",
    "            mape_r1_ct_avg, mape_r1_cq_avg, mape_r1_avg,\n",
    "            mape_r2_ct_avg, mape_r2_cq_avg, mape_r2_avg,\n",
    "            mape_r3_ct_avg, mape_r3_cq_avg, mape_r3_avg,\n",
    "            mape_r4_ct_avg, mape_r4_cq_avg, mape_r4_avg,\n",
    "            mape_l1_ct_avg, mape_l1_cq_avg, mape_l1_avg,\n",
    "            mape_l2_ct_avg, mape_l2_cq_avg, mape_l2_avg,\n",
    "            mape_l3_ct_avg, mape_l3_cq_avg, mape_l3_avg,\n",
    "            mape_l4_ct_avg, mape_l4_cq_avg, mape_l4_avg\n",
    "        ],\n",
    "        \"Epsilon\": [\n",
    "            epsilon_wing_cl_avg, epsilon_wing_cd_avg, epsilon_wing_avg,\n",
    "            epsilon_canard_cl_avg, epsilon_canard_cd_avg, epsilon_canard_avg,\n",
    "            epsilon_r1_ct_avg, epsilon_r1_cq_avg, epsilon_r1_avg,\n",
    "            epsilon_r2_ct_avg, epsilon_r2_cq_avg, epsilon_r2_avg,\n",
    "            epsilon_r3_ct_avg, epsilon_r3_cq_avg, epsilon_r3_avg,\n",
    "            epsilon_r4_ct_avg, epsilon_r4_cq_avg, epsilon_r4_avg,\n",
    "            epsilon_l1_ct_avg, epsilon_l1_cq_avg, epsilon_l1_avg,\n",
    "            epsilon_l2_ct_avg, epsilon_l2_cq_avg, epsilon_l2_avg,\n",
    "            epsilon_l3_ct_avg, epsilon_l3_cq_avg, epsilon_l3_avg,\n",
    "            epsilon_l4_ct_avg, epsilon_l4_cq_avg, epsilon_l4_avg\n",
    "        ],\n",
    "        \"R2\": [\n",
    "            r2_wing_cl_avg, r2_wing_cd_avg, r2_wing_avg,\n",
    "            r2_canard_cl_avg, r2_canard_cd_avg, r2_canard_avg,\n",
    "            r2_r1_ct_avg, r2_r1_cq_avg, r2_r1_avg,\n",
    "            r2_r2_ct_avg, r2_r2_cq_avg, r2_r2_avg,\n",
    "            r2_r3_ct_avg, r2_r3_cq_avg, r2_r3_avg,\n",
    "            r2_r4_ct_avg, r2_r4_cq_avg, r2_r4_avg,\n",
    "            r2_l1_ct_avg, r2_l1_cq_avg, r2_l1_avg,\n",
    "            r2_l2_ct_avg, r2_l2_cq_avg, r2_l2_avg,\n",
    "            r2_l3_ct_avg, r2_l3_cq_avg, r2_l3_avg,\n",
    "            r2_l4_ct_avg, r2_l4_cq_avg, r2_l4_avg\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_avg = pd.DataFrame(results_avg)\n",
    "    df_avg.to_csv(result_out_path + \"nn_model_results_hcgnn.csv\", index=False)\n",
    "    print(\"Average results saved to nn_model_results.csv\")\n",
    "\n",
    "\n",
    "    # 2. Save case-wise performance values (MAPE, Epsilon, R² for each test case)\n",
    "    num_cases = len(mape_wing_cl_list)  # Assuming all lists have the same length\n",
    "\n",
    "    # Create a dictionary to store all case-wise values\n",
    "    results_cases = {\n",
    "        \"Case\": np.arange(1, num_cases + 1),\n",
    "        \"MAPE Wing CL\": mape_wing_cl_list,\n",
    "        \"MAPE Wing CD\": mape_wing_cd_list,\n",
    "        \"MAPE Canard CL\": mape_canard_cl_list,\n",
    "        \"MAPE Canard CD\": mape_canard_cd_list,\n",
    "        \"MAPE Rotor L1 CT\": mape_r1_ct_list,\n",
    "        \"MAPE Rotor L1 CQ\": mape_r1_cq_list,\n",
    "        \"MAPE Rotor L2 CT\": mape_r2_ct_list,\n",
    "        \"MAPE Rotor L2 CQ\": mape_r2_cq_list,\n",
    "        \"MAPE Rotor L3 CT\": mape_r3_ct_list,\n",
    "        \"MAPE Rotor L3 CQ\": mape_r3_cq_list,\n",
    "        \"MAPE Rotor L4 CT\": mape_r4_ct_list,\n",
    "        \"MAPE Rotor L4 CQ\": mape_r4_cq_list,\n",
    "        \"MAPE Rotor R1 CT\": mape_l1_ct_list,\n",
    "        \"MAPE Rotor R1 CQ\": mape_l1_cq_list,\n",
    "        \"MAPE Rotor R2 CT\": mape_l2_ct_list,\n",
    "        \"MAPE Rotor R2 CQ\": mape_l2_cq_list,\n",
    "        \"MAPE Rotor R3 CT\": mape_l3_ct_list,\n",
    "        \"MAPE Rotor R3 CQ\": mape_l3_cq_list,\n",
    "        \"MAPE Rotor R4 CT\": mape_l4_ct_list,\n",
    "        \"MAPE Rotor R4 CQ\": mape_l4_cq_list,\n",
    "        \"Epsilon Wing CL\": epsilon_wing_cl_list,\n",
    "        \"Epsilon Wing CD\": epsilon_wing_cd_list,\n",
    "        \"Epsilon Canard CL\": epsilon_canard_cl_list,\n",
    "        \"Epsilon Canard CD\": epsilon_canard_cd_list,\n",
    "        \"Epsilon Rotor L1 CT\": epsilon_r1_ct_list,\n",
    "        \"Epsilon Rotor L1 CQ\": epsilon_r1_cq_list,\n",
    "        \"Epsilon Rotor L2 CT\": epsilon_r2_ct_list,\n",
    "        \"Epsilon Rotor L2 CQ\": epsilon_r2_cq_list,\n",
    "        \"Epsilon Rotor L3 CT\": epsilon_r3_ct_list,\n",
    "        \"Epsilon Rotor L3 CQ\": epsilon_r3_cq_list,\n",
    "        \"Epsilon Rotor L4 CT\": epsilon_r4_ct_list,\n",
    "        \"Epsilon Rotor L4 CQ\": epsilon_r4_cq_list,\n",
    "        \"Epsilon Rotor R1 CT\": epsilon_l1_ct_list,\n",
    "        \"Epsilon Rotor R1 CQ\": epsilon_l1_cq_list,\n",
    "        \"Epsilon Rotor R2 CT\": epsilon_l2_ct_list,\n",
    "        \"Epsilon Rotor R2 CQ\": epsilon_l2_cq_list,\n",
    "        \"Epsilon Rotor R3 CT\": epsilon_l3_ct_list,\n",
    "        \"Epsilon Rotor R3 CQ\": epsilon_l3_cq_list,\n",
    "        \"Epsilon Rotor R4 CT\": epsilon_l4_ct_list,\n",
    "        \"Epsilon Rotor R4 CQ\": epsilon_l4_cq_list,\n",
    "        \"R2 Wing CL\": r2_wing_cl_list,\n",
    "        \"R2 Wing CD\": r2_wing_cd_list,\n",
    "        \"R2 Canard CL\": r2_canard_cl_list,\n",
    "        \"R2 Canard CD\": r2_canard_cd_list,\n",
    "        \"R2 Rotor L1 CT\": r2_r1_ct_list,\n",
    "        \"R2 Rotor L1 CQ\": r2_r1_cq_list,\n",
    "        \"R2 Rotor L2 CT\": r2_r2_ct_list,\n",
    "        \"R2 Rotor L2 CQ\": r2_r2_cq_list,\n",
    "        \"R2 Rotor L3 CT\": r2_r3_ct_list,\n",
    "        \"R2 Rotor L3 CQ\": r2_r3_cq_list,\n",
    "        \"R2 Rotor L4 CT\": r2_r4_ct_list,\n",
    "        \"R2 Rotor L4 CQ\": r2_r4_cq_list\n",
    "    }\n",
    "\n",
    "    df_cases = pd.DataFrame(results_cases)\n",
    "    df_cases.to_csv(result_out_path + \"nn_model_performance_cases_hcgnn_new.csv\", index=False)\n",
    "    print(\"Case-wise performance results saved to nn_model_performance_cases.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
