{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotor Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This Jupyter Notebook documents the workflow for building and evaluating a machine learning model to predict the thrust and torque coefficients of a rotor. The notebook includes the following steps:\n",
    "\n",
    "1. **Import Libraries**: Import necessary libraries and modules for data processing, model building, and evaluation.\n",
    "2. **Define Dataset Class**: Create a custom dataset class to load and preprocess the rotor data from CSV files.\n",
    "3. **Extract and Organize Data**: Load the data, apply necessary transformations, and visualize the dataset.\n",
    "4. **Define Neural Network Model**: Define an LSTM-based neural network model to predict the thrust and torque coefficients.\n",
    "5. **Train the Model**: Train the model using the training dataset and evaluate it on the validation dataset.\n",
    "6. **Plot Losses**: Plot the training and validation losses to monitor the model's performance.\n",
    "7. **Save the Model**: Save the trained model and scalers for future use.\n",
    "8. **Test the Model**: Evaluate the model on a separate testing dataset and compute performance metrics such as MAPE, R² score, and relative L2 norm error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TRAINED_MODEL = False\n",
    "PLOT_RESULTS = True\n",
    "\n",
    "project_path = '/mnt/e/eVTOL_model/eVTOL-VehicleModel/'\n",
    "data_path = project_path + 'data/rotor_data/'\n",
    "model_path = project_path + 'trained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "# Condition function to filter subdirectories\n",
    "def subdir_condition(subdir_name):\n",
    "    \"\"\"\n",
    "    Condition: Only process subdirectories whose names start with 'propeller-example_dji'.\n",
    "    Modify this function to apply a specific filtering logic.\n",
    "    \"\"\"\n",
    "    return subdir_name.startswith('rotor_dataset')  # Change this condition as needed\n",
    "\n",
    "\n",
    "class PropellerDataset(Dataset):\n",
    "    # def __init__(self, root_dir, alpha, J, theta, yaw, tilt, subdir_condition=None):\n",
    "    def __init__(self, root_dir, subdir_condition=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory with subdirectories containing CSV files.\n",
    "            alpha (float): Angle of attack.\n",
    "            J (float): Advance ratio.\n",
    "            theta (float): Pitch.\n",
    "            yaw (float): Yaw.\n",
    "            tilt (float): Tilt.\n",
    "            subdir_condition (callable, optional): A function or condition to filter subdirectories by name.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.time_data = []  # Store time data separately\n",
    "        self.omega_data = []  # Store omega (RPM) separately\n",
    "        self.ct_data = []\n",
    "        self.cq_data = []\n",
    "        self.fft_ct_r = []\n",
    "        self.fft_ct_i = []\n",
    "        self.fft_cq_r = []\n",
    "        self.fft_cq_i = []\n",
    "        self.subdir_condition = subdir_condition\n",
    "\n",
    "        # Traverse the root directory to gather data\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Helper function to read CSV files from each subdirectory and extract relevant columns.\n",
    "        \"\"\"\n",
    "        # Iterate through each subdirectory in the root directory\n",
    "        for subdir, _, files in os.walk(self.root_dir):\n",
    "            subdir_name = os.path.basename(subdir)\n",
    "            \n",
    "            # Apply subdirectory name condition\n",
    "            if self.subdir_condition and not self.subdir_condition(subdir_name):\n",
    "                continue\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith(\"_convergence.csv\"):\n",
    "                    # Load the CSV file\n",
    "                    csv_path = os.path.join(subdir, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Extract necessary columns for input features\n",
    "                    time = df['T'].values  # Time\n",
    "                    omega = df['RPM_1'].values  # RPM\n",
    "                    J = df['J'].values\n",
    "                    AOA = df['AOA'].values\n",
    "                    v_inf = df['v_inf'].values\n",
    "                    pitch = df['Pitch (blade)'].values\n",
    "                    tilt = df['Tilt'].values\n",
    "                    yaw = df['Yaw'].values\n",
    "                    ref_angle = df['ref age (deg)'].values\n",
    "\n",
    "                    # Store time and omega in separate lists for easy access\n",
    "                    self.time_data.append(time)\n",
    "                    self.omega_data.append(omega)\n",
    "                    \n",
    "                    # Extract CT and CQ as output variables\n",
    "                    ct = df['CT_1'].values  # Thrust coefficient (CT)\n",
    "                    cq = df['CQ_1'].values  # Torque coefficient (CQ)\n",
    "\n",
    "                    fft_ct = fft(ct)\n",
    "                    fft_ct_real = np.real(fft_ct)\n",
    "                    fft_ct_imag = np.imag(fft_ct)\n",
    "\n",
    "                    fft_cq = fft(cq)\n",
    "                    fft_cq_real = np.real(fft_cq)\n",
    "                    fft_cq_imag = np.imag(fft_cq)\n",
    "\n",
    "                    \n",
    "                    # Store ct and cq in separate lists for easy access\n",
    "                    self.ct_data.append(ct)\n",
    "                    self.cq_data.append(cq)\n",
    "                    self.fft_ct_r.append(fft_ct_real)\n",
    "                    self.fft_ct_i.append(fft_ct_imag)\n",
    "                    self.fft_cq_r.append(fft_cq_real)\n",
    "                    self.fft_cq_i.append(fft_cq_imag)\n",
    "\n",
    "                    # For each simulation, the input sequence is structured as (n_timesteps, n_features)\n",
    "                    sequence_inputs = []\n",
    "                    sequence_outputs = []\n",
    "                    for i in range(len(time)):\n",
    "                        # Each time step has time, omega, and predefined variables: alpha, J, theta, yaw, tilt\n",
    "                        input_data = [\n",
    "                            time[i],  omega[i], AOA[i], v_inf[i], (10*np.sin(omega[i]*time[i])), \n",
    "                            (10*np.cos(omega[i]*time[i])), J[i], pitch[i], tilt[i], yaw[i]\n",
    "                        ]\n",
    "                        output_data = [ct[i], cq[i]]\n",
    "                        \n",
    "                        \n",
    "                        sequence_inputs.append(input_data)\n",
    "                        sequence_outputs.append(output_data)\n",
    "\n",
    "                    sequence_inputs = np.array([sequence_inputs], dtype=float)\n",
    "                    sequence_outputs = np.array([sequence_outputs], dtype=float)\n",
    "\n",
    "                    # Append input sequence (n_timesteps, num_features) and output (CT, CQ)\n",
    "                    self.data.append(sequence_inputs)\n",
    "                    self.targets.append(sequence_outputs)  # Append the whole CT and CQ sequences\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single sequence and its targets.\n",
    "        \"\"\"\n",
    "        print(f\"Requested index: {idx}\")\n",
    "        print(f\"Dataset length: {len(self.data)}\")  # Check if idx exceeds this\n",
    "        print(f\"Targets length: {len(self.targets)}\")\n",
    "        \n",
    "        inputs = self.data[idx]  # Input sequence: (n_timesteps, n_features)\n",
    "        targets = self.targets[idx]  # Output: (n_timesteps, 2)\n",
    "        \n",
    "        # return inputs, targets\n",
    "        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def get_variable(self, variable_name):\n",
    "        \"\"\"\n",
    "        Returns a list of arrays for the specified variable.\n",
    "        Args:\n",
    "            'time' - timesteps\n",
    "            'omega' - rotational velocity [RPM]\n",
    "            'CT' - Thrust coefficient\n",
    "            'CQ' - Torque coefficient\n",
    "            'fft_ct_r' - Thrust ccoefficient FFT - real part\n",
    "            'fft_ct_i' - Thrust ccoefficient FFT - imag part\n",
    "            'fft_cq_r' - Torque ccoefficient FFT - real part\n",
    "            'fft_cq_i' - Torque ccoefficient FFT - imag part\n",
    "\n",
    "        \"\"\"\n",
    "        if variable_name == 'time':\n",
    "            return self.time_data  # Return all time steps for each simulation\n",
    "        elif variable_name == 'omega':\n",
    "            return self.omega_data  # Return all omega (RPM) values for each simulation\n",
    "        elif variable_name == 'CT':\n",
    "            return self.ct_data \n",
    "        elif variable_name == 'CQ':\n",
    "            return self.cq_data\n",
    "        elif variable_name == 'fft_ct':\n",
    "            return self.fft_ct_r, self.fft_ct_i \n",
    "        elif variable_name == 'fft_cq':\n",
    "            return self.fft_cq_r, self.fft_cq_i  \n",
    "        else:\n",
    "            raise ValueError(f\"Variable {variable_name} not supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom dataset class\n",
    "# Creates training and validation datasets\n",
    "class SimulationDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.float32), torch.tensor(self.outputs[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tabular_dataset(inputs, targets, visualize:bool):\n",
    "    \n",
    "    if visualize:\n",
    "        # Additional part to visualize the data\n",
    "        df_ips = [None]*len(inputs)\n",
    "        df_ops = [None]*len(targets)\n",
    "\n",
    "        for num_batch in range(len(inputs)):\n",
    "            df_ips[num_batch] = pd.DataFrame(inputs[num_batch], columns=[\"timestep\", \"sin_comp\", \"cos_comp\", \"Omega\", \"alpha\", \"J\", \"theta\", \"tilt\", \"yaw\"])\n",
    "            df_ops[num_batch] = pd.DataFrame(targets[num_batch], columns=[\"fft_ct_real\", \"fft_ct_imag\", \"fft_cq_real\", \"fft_cq_imag\"])\n",
    "\n",
    "        print(\"Input dataset\\n\", df_ips)\n",
    "        print(\"Target dataset\\n\", df_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory where simulation subdirectories are stored\n",
    "root_dir = data_path + \"training_data/\"\n",
    "\n",
    "visualize_dataset = True         # Visualize loaded dataset\n",
    "visualize_norm_dataset = False   # Visualize normalized dataset\n",
    "visualize_nn = True              # Visulaize NN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset with a subdirectory condition\n",
    "\n",
    "dataset = PropellerDataset(root_dir, subdir_condition=subdir_condition)\n",
    "inputs, outputs = dataset[0:]\n",
    "\n",
    "\n",
    "input_tensor = inputs.squeeze(1)  # Reshaping\n",
    "print(\"Input shape:\", input_tensor.shape)   # Should print: torch.Size([Num_sumulations, tsteps_per_simulations, 10])\n",
    "\n",
    "output_tensor = outputs.squeeze(1)\n",
    "print(\"Output shape:\",output_tensor.shape)  # Should print: torch.Size([Num_sumulations, tsteps_per_simulations, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Definition\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.1):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 10             # Number of input features\n",
    "hidden_size = 50            # Number of LSTM units (hidden state size)\n",
    "output_size = 2             # Number of output features (e.g., 2 for thrust and torque coefficients)\n",
    "num_layers = 4              # Number of LSTM layers\n",
    "learning_rate = 5e-4         # Learning rate\n",
    "num_epochs = 2500           # Number of training epochs\n",
    "batch_size = 8              # Batch size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers for input and output\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Reshape data for scaling (flatten along the time dimension)\n",
    "inputs_reshaped = input_tensor.reshape(-1, input_size)\n",
    "outputs_reshaped = output_tensor.reshape(-1, output_size)\n",
    "\n",
    "# Fit and transform inputs and outputs\n",
    "inputs_normalized = input_scaler.fit_transform(inputs_reshaped).reshape(input_tensor.shape)\n",
    "outputs_normalized = output_scaler.fit_transform(outputs_reshaped).reshape(output_tensor.shape)\n",
    "\n",
    "print(\"Normalized input shape:\", inputs_normalized.shape)\n",
    "print(\"Normalized output shape:\", outputs_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset randomly\n",
    "inputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n",
    "    inputs_normalized, outputs_normalized, test_size=0.35, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimulationDataset(inputs_train, outputs_train)\n",
    "val_dataset = SimulationDataset(inputs_val, outputs_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "trainDataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
    "valDataLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pred, target):\n",
    "    return torch.mean((pred - target) ** 2) + 0.3 * torch.mean(torch.abs(pred - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMNet(input_size, hidden_size, output_size, num_layers).to(device)    # LSTM model\n",
    "# criterion = nn.MSELoss()                                                      # Loss function                                     \n",
    "criterion = custom_loss   \n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in trainDataLoader:\n",
    "        inputs, targets = inputs.squeeze(1).to(device), targets.squeeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clipping\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(trainDataLoader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # Evaluation Loop\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valDataLoader:\n",
    "            inputs, targets = inputs.squeeze(1).to(device), targets.squeeze(1).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = eval_loss / len(valDataLoader)\n",
    "        eval_losses.append(avg_eval_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}],\\n Training Loss: {running_loss/len(trainDataLoader):.8f}')\n",
    "    print(f'Evaluation Loss: {eval_loss/len(valDataLoader):.8f}')\n",
    "\n",
    "print(\"\\nFinished Training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=300)  # High-resolution plot\n",
    "\n",
    "plt.plot(train_losses, color='black', linewidth=1.5, label=\"Training loss\")\n",
    "plt.plot(eval_losses, color='red', linewidth=1.5, label=\"Validation loss\")\n",
    "plt.title(\"Training Loss and Validation Loss\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=10)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.savefig(\"rotor_model_loss.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.savefig('loss_curve.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model if needed\n",
    "if SAVE_TRAINED_MODEL:\n",
    "\n",
    "    model_save_path =  model_path + '/models/rotor/{}_scaled_H26FpropModel_MSELoss_lr{}_e{}_nL{}_numNN{}.pth'.format(date, learning_rate, num_epochs, num_layers, hidden_size)\n",
    "    print(\"The model will be saved as the following:\\n {}\".format(model_save_path))\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # save the scalers\n",
    "    scaler_save_path = model_path + '/scalers/rotor/'\n",
    "    joblib.dump(input_scaler, scaler_save_path+'/{}_scaled_H26F_MSELoss_ipScaler_lr{}_e{}_nL{}_numNN{}.pkl'.format(date, learning_rate, num_epochs, num_layers, hidden_size))\n",
    "    joblib.dump(output_scaler, scaler_save_path+'/{}_scaled_H26F_MSELoss_opScaler_lr{}_e{}_nL{}_numNN{}.pkl'.format(date, learning_rate, num_epochs, num_layers, hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Root directory where simulation subdirectories are stored\n",
    "root_dir_test_base = data_path + \"testing_data/\"\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Absolute Percentage Error (MAPE)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def relative_l2_norm(y_true, y_pred):\n",
    "    \"\"\"Compute Relative L2 Norm Error (ε)\"\"\"\n",
    "    mask = y_true != 0  # Avoid division by zero\n",
    "    numerator = np.linalg.norm(y_pred[mask] - y_true[mask], ord=2)  # ||pred - true||_2\n",
    "    denominator = np.linalg.norm(y_true[mask], ord=2)  # ||true||_2\n",
    "    return (numerator / denominator) * 100\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "mape_ct_list, mape_cq_list = [], []\n",
    "r2_ct_list, r2_cq_list = [], []\n",
    "relative_l2_norm_ct_list, relative_l2_norm_cq_list = [], []\n",
    "\n",
    "for simulation_case in os.listdir(root_dir_test_base):\n",
    "    # Root directory where simulation subdirectories are stored\n",
    "    root_dir_test_sim = root_dir_test_base+simulation_case\n",
    "  \n",
    "    # Create the dataset with a subdirectory condition\n",
    "    dataset_test = PropellerDataset(root_dir_test_sim, subdir_condition=subdir_condition)\n",
    "    inputs_test, outputs_test = dataset_test[0:]\n",
    "\n",
    "    # Assuming your input tensor is named `input_tensor`\n",
    "    input_tensor_test = inputs_test.squeeze(1)  # Remove the singleton dimension at index 1\n",
    "    # print(\"Input shape:\", input_tensor_test.shape)  # Should print: torch.Size([6, 145, 7])\n",
    "\n",
    "    output_tensor_test = outputs_test.squeeze(1)\n",
    "    # print(\"Output shape:\",output_tensor_test.shape)\n",
    "\n",
    "    inputs_test_reshaped = input_tensor_test.reshape(-1, input_size)\n",
    "\n",
    "    test_inputs_normalized = input_scaler.transform(inputs_test_reshaped.reshape(-1, input_size)).reshape(input_tensor_test.shape)\n",
    "\n",
    "    test_inputs_tensor = torch.tensor(test_inputs_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predicted_outputs = model(test_inputs_tensor)\n",
    "\n",
    "    # Convert the predictions back to numpy and inverse scale the outputs\n",
    "    predicted_outputs = predicted_outputs.cpu().detach().numpy()  # Convert tensor to numpy array\n",
    "    predicted_outputs_original_scale = output_scaler.inverse_transform(predicted_outputs.reshape(-1, output_size))\n",
    "\n",
    "    # Reshape the predictions to match the original sequence structure if needed\n",
    "    predicted_outputs_original_scale = predicted_outputs_original_scale.reshape(input_tensor_test.shape[0], input_tensor_test.shape[1], output_size)\n",
    "    predicted_outputs_original_scale = predicted_outputs_original_scale[0]\n",
    "\n",
    "    # print(predicted_outputs_original_scale.shape)\n",
    "\n",
    "    # Model predicted values\n",
    "    ct_test_NN = predicted_outputs_original_scale[:,0]\n",
    "    cq_test_NN = predicted_outputs_original_scale[:,1]\n",
    "\n",
    "    # Load timesteps, CT and CQ from FLOWUnsteady simualtions\n",
    "    time_steps = dataset_test.get_variable('time')\n",
    "\n",
    "    ct_test_flowuns = dataset_test.get_variable('CT')\n",
    "    cq_test_flowuns = dataset_test.get_variable('CQ')\n",
    "\n",
    "    fft_ct_fu_real, fft_ct_fu_imag = dataset_test.get_variable('fft_ct')\n",
    "    fft_cq_fu_real, fft_cq_fu_imag = dataset_test.get_variable('fft_cq')\n",
    "\n",
    "    # Compute MAPE and R² Score\n",
    "    mape_ct = mape(ct_test_flowuns[0], ct_test_NN)\n",
    "    mape_cq = mape(cq_test_flowuns[0], cq_test_NN)\n",
    "    r2_ct = r2_score(ct_test_flowuns[0], ct_test_NN)\n",
    "    r2_cq = r2_score(cq_test_flowuns[0], cq_test_NN)\n",
    "    relative_l2_norm_ct = relative_l2_norm(ct_test_flowuns[0], ct_test_NN)\n",
    "    relative_l2_norm_cq = relative_l2_norm(cq_test_flowuns[0], cq_test_NN)\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    mape_ct_list.append(mape_ct)\n",
    "    mape_cq_list.append(mape_cq)\n",
    "    r2_ct_list.append(r2_ct)\n",
    "    r2_cq_list.append(r2_cq)\n",
    "    relative_l2_norm_ct_list.append(relative_l2_norm_ct)\n",
    "    relative_l2_norm_cq_list.append(relative_l2_norm_cq)\n",
    "\n",
    "    # Print metrics for this simulation case\n",
    "    print(f\"Simulation Case: {simulation_case}\")\n",
    "    print(f\"  MAPE Ct: {mape_ct:.2f}%, MAPE Cq: {mape_cq:.2f}%\")\n",
    "    print(f\"  R² Ct: {r2_ct:.4f}, R² Cq: {r2_cq:.4f}\")\n",
    "    print(f\"  ε Ct: {relative_l2_norm_ct:.2f}%, ε Cq: {relative_l2_norm_cq:.2f}%\")\n",
    "\n",
    "    # Plot the results\n",
    "    if PLOT_RESULTS:\n",
    "        # plt.figure()\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(ct_test_NN, label = 'ct_NN')\n",
    "        plt.plot((ct_test_flowuns[0]), label = 'ct_flowuns')\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('Thrust coefficient, $C_T$')\n",
    "        plt.title(simulation_case)\n",
    "        plt.legend()\n",
    "\n",
    "        # plt.figure()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(cq_test_NN, label = 'cq_NN')\n",
    "        plt.plot(cq_test_flowuns[0], label = 'cq_flowuns')\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('Torque coefficient, $C_Q$')\n",
    "        plt.title(simulation_case)\n",
    "        plt.legend()\n",
    "\n",
    "# Compute average metrics across all test cases\n",
    "print(\"\\nOverall Metrics Across All Test Cases:\")\n",
    "print(f\"  Avg MAPE Ct: {np.mean(mape_ct_list):.2f}%, Avg MAPE Cq: {np.mean(mape_cq_list):.2f}%\")\n",
    "print(f\"  Avg R² Ct: {np.mean(r2_ct_list):.4f}, Avg R² Cq: {np.mean(r2_cq_list):.4f}\")\n",
    "print(f\"  Avg ε Ct: {np.mean(relative_l2_norm_ct_list):.2f}%, Avg ε Cq: {np.mean(relative_l2_norm_cq_list):.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
